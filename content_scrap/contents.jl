{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "NYU researchers invent new real-time data analysis system for humanitarian agencies", "content": ["ow can we efficiently help those in need? A difficult reality facing humanitarian agencies is that they cannot immediately address all of the world\u2019s crises, particularly when constrained by limited financial and human resources. This is why they prioritize which emergencies they respond to. But the prioritization process, which involves collecting, organizing, and analyzing vast volumes of secondary data produced by public institutions, NGOs, and news media about each crisis around the globe, is highly time consuming and relies solely on manual human labor.", ", however, aims to automate these tasks for humanitarian workers so that they can make decisions about aid delivery and disaster response more quickly. Invented by NYU doctoral student ", " and a research team comprising of ", ", the Executive Director of the ", ", and experts from ", ", the new system contains four main components: a focused web crawler, a metadata extractor, a content classifier, and a feedback mechanism.", "Crawlers generally strive to cover as many pages as possible, but \u201ca focused crawler,\u201d as the researchers explain, \u201cis a web crawler that is optimized to seek web pages that are relevant to predefined topics.\u201d And, because emergency situations tend to change rapidly, the researchers also designed a real-time re-crawling strategy in the system. Using a binary classifier, the crawler then categorizes whether particular webpages are relevant or irrelevant to the user\u2019s search topic, and then passes the webpages onto the metadata extractor.", "The extractor concentrates on mining the textual data of those webpages. After singling out the title, content, publication date, and mentioned countries from the relevant webpages that the crawler passed on, a content classifier analyzes and labels the webpages according to what type of crisis they are describing.", "Because the system\u2019s efficiency hinges on the accuracy of the content classifier, the researchers built a vital feedback cycle into the system, which collects user feedback so that the classifier can improve over time. \u201cThis especially increases the robustness of the page classifier,\u201d the researchers explain, \u201cas well as the adaptivity of the crawler.\u201d", "The researchers recently implemented a fully operational prototype of their system for humanitarian experts at the Assessments Capacities Project (ACAPS), an organization that supports crisis responders by providing needs assessments and analysis.", "While more work still needs to be done to tailor the system for domain-specific needs, the researchers hope that it will not only be widely implemented at humanitarian agencies in the future, but also incorporate social media data into its processes as well.", "Written by"], "postingTime": "2018-01-24T15:19:52.202Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Guitar-Set, a New Dataset for Music Information Retrieval", "content": ["ock-n-roll. Classical. Country. Blues. Punk. Pop. The guitar can do it all. Its substantial range attracts multitudinous musicians and listeners, and it provides engaging material for music research. But for researchers in the music information retrieval (MIR) community, the guitar\u2019s versatility can also complicate analytic processes.", "To facilitate guitar-related music research, CDS affiliated faculty member ", ", Associate Professor of Music and Music Education, is developing a new dataset, called ", " Many existing methods for automated analysis of guitar recordings depend on high-quality labeled data-sets, but labelling these data-sets takes significant time and money.", "Guitar-Set, collaboratively created with researchers from the NYU Music and Audio Research Lab and the Queen Mary University of London Centre for Digital Music, helps ease the difficulty and expense of labeling musical data-sets by automating as much of the process as possible. The method involves recording guitar performances with both a microphone and a hexaphonic pickup, a device attached to the guitar which magnetically records each string. The researchers recorded 16-bar performances from acoustic and electric guitars.", "Based on the hexaphonic recordings, Guitar-Set includes note-level annotations of string and fret position to recorded metadata about tempo, key, style, beat and downbeat, and chords. Guitar-Set\u2019s objective is to create these note-level annotations from individually recorded strings, but the researchers found that a full automation of this process is challenging. They encountered a high rate of false positives on the hexaphonic recording due to picked up vibrations that were not audible on the microphone-recorded track. While this requires a manual comparison between the two types of recordings, Bello and collaborators were able to mitigate some of the false positives by first running the hexaphonic data through a bleed removal algorithm.", "Despite the challenges, Guitar-Set will be a useful resource for researchers in the MIR community. Bello and collaborators also anticipate that the new method will aid other tasks such as source separation, understanding sympathetic resources in the acoustic guitar, and understanding guitar right-hand activity.", "Written by"], "postingTime": "2018-01-26T15:39:31.691Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Neural Networks and Toddlers: How Learning Biases Can Improve Word Learning", "content": ["eople use prior knowledge to contextualize and make inferences about new concepts, a method of learning that relies on inductive biases. These learning habits begin to develop in childhood as children learn how to learn, and they enable the human ability to learn new concepts from just a few examples. While artificial learning systems like neural networks require hundreds or thousands of examples to learn how to recognize similar objects, they can develop inductive biases much like humans.", "with a research partner, investigates whether neural networks develop a particular type of inductive bias called a shape bias. A shape bias is the inclination to relate a new object to a previously seen object of the same name according to shape rather than by texture or color. (While the word ", " can carry a negative connotation, shape biases have been shown to be effective facilitators of word learning in early childhood.)", "Lake modeled his study based on a cognitive study about shape bias conducted with toddlers. The toddler cognitive study involved a 1st-order and 2nd-order generalization test. In the 1st-order test, toddlers categorized unfamiliar objects by shape according to the shape of a familiar object; in the 2nd-order test, toddlers categorized wholly unfamiliar sets of objects by shape.", "Using this framework of 1st and 2nd-order generalization tests, Lake constructed three experiments. In the first experiment, a multilayer perceptron was trained to name synthetic stimuli that represented objects, and the results from the 2nd-order test showed that the network in this experiment passed the threshold for shape bias. In the second experiment, a convolutional neural network (CNN) was trained with synthetic object stimuli encoded as raw images. It was able to learn a shape bias from as few as six examples of eight categories.", "After determining with the first two experiments that neural networks can quickly develop shape biases, Lake explored in the third experiment whether shape bias can accelerate the speed with which CNNs can learn new words. The results showed a correlation between shape biases and improved word learning in CNNs which mirrors the effect shape biases have on improved word learning in toddlers.", "For the future, Lake and his research partner state, \u201cOne implication of this finding is that it may be possible to train largescale image recognition models more efficiently after initializing these models with shape bias training. In future work, we hope to investigate this hypothesis with ImageNet-scale deep neural networks, using an initialization framework designed with the intuitions garnered here.\u201d", "Written by"], "postingTime": "2018-02-26T16:00:04.535Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Science versus Religion\u2014they may not be as opposed as you think", "content": ["merican policy debates about issues like abortion, euthanasia, genetic engineering, and the teaching of evolutionary theory arose from a cultural conflict between science and religion in the 1980\u2019s. While this may suggest that science and religion are inherently opposed systems, a new study by CDS Affiliated Faculty Member ", ", Professor of Sociology, suggests that the reality is more nuanced.", "First, it incorporates spiritualism as a third dimension to the debate. Second, it uses two types of data analysis to classify attitudes.", "DiMaggio and collaborators applied their two methods of data analysis to the 1988 General Social Survey (GSS) which included 1018 respondents. The first method of data analysis they used, ", ", identifies subsets of respondents who are in agreement. Members within each subset share positions\u2014for example, members of one subset might agree that science should be trusted more, while members of another subset might put more trust in religion. LCA identified five subsets among the 1988 GSS respondents: Pro-science skeptics (29% of respondents), Anti-clerics (10%), Religious traditionalists (24%), Institutionalists (23%), and Spiritualists (14%).", "The second method of data analysis used by DiMaggio and collaborators, ", ", identifies subsets of respondents who agree about relationships between items. Members of one subset identified by RCA might, for example, share the attitude that science and religion are inherently opposed, while members of another RCA subset might believe that science and religion do not impinge on each other and can coexist. Members of RCA subsets do not necessarily agree about the issues, but they agree about how items are related. RCA identified three subsets from the 1988 GSS: Science vs. Spiritualism (39%), Religion vs. Science (40%), and Domain Decoupling (a subset that did not construe an opposition between science/religion/spiritualism) (21%).", "LCA and RCA together reveal that science and religion are not entirely opposed within American public opinion. While some groups see the two domains as opposed, others perceive an opposition between institutionalized modes of thinking (both science and religion) and modes of thinking separate from cultural institutions (spiritualism).", "The inclusion of spiritualism as a dimension in DiMaggio\u2019s research was critical to exposing this separate dichotomy, but the researchers caution that the survey included responses about spiritual experiences rather than attitudes. They also caution about extending generalizations from their research to the present, given their reliance on data from 1988. Still, their work offers a framework for future research about cultural attitudes, and highlights a potential pathway for alliances between conservative Christians and Americans who have more faith in science.", "Written by"], "postingTime": "2018-02-02T18:40:53.595Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Mobile Health Apps: To Download or Not To Download?", "content": ["or individuals with chronic conditions, adhering to a disease management plan can prolong and enrich life while minimizing healthcare costs. But properly managing a chronic condition can be stressful and demanding. One way chronically-ill people can ease this stress is by using mobile health apps which can facilitate disease management by promoting healthy lifestyle behaviors, tracking exercise, improving nutrition, assisting with weight loss, reminding people to take medication, and more.", "Over 3500 health apps are currently available for individuals with chronic conditions, with most designed for users with diabetes or depression. Yet, as mobile health apps have proliferated, little research has tracked how people with chronic conditions actually use them. ", "The researchers sourced their data from a national sample of mobile phone users collected in 2015 for a previous study about mobile phones and health.", "According to the demographic qualifications for the new study, the sample yielded 1604 respondents to a survey which included 36 questions about health and mobile health app use.", "For their data analysis, the researchers examined health app download and usage by chronic condition, self-reported health, physical activity, and according to demographic factors. 37% of respondents reported very good health and 13% reported excellent health. The most common chronic conditions reported by the remaining participants were hypertension, obesity, diabetes, depression, and high cholesterol.", "An analysis of the data showed that having self-reported ", " or ", " health was a strong predictor of health app download, and people with one day or more of reported physical activity per week were far more likely to use health apps than those who reported no physical activity. Among those with chronic conditions, people with hypertension, depression, and high cholesterol were least likely to download health apps, and only about one third of individuals with chronic conditions agreed that health apps have the potential to dramatically improve their health.", "For future intervention, the researchers propose improving how the benefits of health apps are communicated and marketed to people with chronic conditions. The researchers also suggest future studies to determine perceptions about the value of health apps among healthcare providers. As new data and analyses continue to emerge about attitudes toward health apps, individuals with chronic conditions stand to benefit from altering perceptions and increasing usage of health apps.", "Written by"], "postingTime": "2018-02-01T16:14:15.915Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Demystifying deep learning - Center for Data Science - Medium", "content": [" reason why recognition systems in fields like natural language processing and computer vision have improved so dramatically is due to the increasing application of deep learning networks. The approach has been such a success that deep learning has not only become a buzzword in the tech community but also in mainstream news.", "But as we race towards building bigger, better, and faster machines, it\u2019s important to think about ", " deep learning is so successful. What are deep learning\u2019s properties? What makes one deep architecture better than another? And, based on the observations that we\u2019ve collected during deep learning\u2019s young lifespan (after all, the powerful tool only emerged in the late-1980s thanks to the likes of ", " and ", "), can we begin to form a rigorous theory about how and why the approach works?", "To get to the heart of the matter, we have to return to the powerful language that underpins all aspects of science and technology: mathematics. Enter ", " ", " with ", " (Johns Hopkins), ", " (Tel-Aviv), and ", " (UCLA), which aims to provide some mathematical explanations for some of deep learning\u2019s properties.", "Though scientists have made empirical observations about, say, the relationship between the size of a neural network and its accuracy rate during neural network training, the researchers explain that \u201cthere is currently no rigorous theory that provides a precise mathematical explanation for these experimentally observed phenomena.\u201d", "To begin addressing this gap, the researchers collect, consolidate, and explore the mathematics that underpins deep learning\u2019s success \u2014 from aspects like global optimality to geometric stability. ", "Written by"], "postingTime": "2018-01-25T15:25:21.661Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Call me, maybe: a new algorithm detects call activity using smartphone sensors", "content": [" make an overseas phone call 50 years ago, you either had to buy a phone card or cough up wads of cash to foot an expensive phone bill.", "But, today, all you have to do is use one of the many internet-based phone apps like Skype, WhatsApp, or WeChat. Not only are these apps free, but they also have strong privacy protocols in place. Unfortunately, however, this is precisely why criminals and other ill-intentioned individuals have begun exploiting such apps to communicate with each other. When suspicious or forbidden calls are placed through these apps, then, security officials cannot effectively identify whom is calling whom, or when such calls are taking place.", " that graduate students ", " (lead author) and ", ", together with ", " (professor of Computer Science and CDS affiliated faculty member), have invented is poised to become a powerful tool for assisting the security sector, and more.", "Their system can detect when someone is making a call using an internet-based phone app with 91.25% accuracy, and it works by analyzing the data collected by built-in ", " and ", "sensors in our smartphones.", "\u201cThe proximity sensor,\u201d the researchers explain, \u201cmeasures the closeness between an object and the phone\u2019s screen in centimeters.\u201d The orientation sensor measures \u201cthe phone\u2019s rotation angles in degrees around the ", "axis, ", "axis, and ", "-axis.\u201d The orientation sensor can be used to detect whether the caller is sitting/standing, walking, or lying down. A proximity or orientation sensor alone cannot detect a phone call taking place, but the researchers found that combining the sensors leads to successful call detection.", "After collecting the range of orientation and proximity measurements both when a call is taking place and when it is not, the research team\u2019s algorithm can perform call detection by tracking subtle changes in the prevailing call state classification (e.g. is the person sitting/standing, lying down, or walking) and fusing that with sensed proximity.", "Their system was trained using three classifiers \u2014 Na\u00efve Bayes, SVM, and Logistic Regression \u2014 although the researchers point out that \u201cthere are many more classifiers that could be used such as decision tree, random tree, and neural networks. It is possible that other classifiers could produce better performance.\u201d", "Potential applications for their system, the researchers added, include \u201cassisting human activity recognition systems, monitoring health conditions, and many more areas.\u201d", "Written by"], "postingTime": "2018-01-23T15:18:12.198Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "I got 99 problems \u2014 and the law is one: how can we measure legislative efficacy?", "content": [" 2014, ", "published an article summarizing the career of retired US legislator Robert E. Andrews under a damning headline: \u201cAndrews proposed 646 bills, passed 0: worst record of past 20 years.\u201d", "Ouch. The statistic appears to suggest that Andrews\u2019s career was a flop \u2014 but as CDS\u2019s Data Science Fellow ", " explained at a recent Moore Sloan Research Lunch Seminar, new conclusions arise once we consider how the legislative system functions as a whole, and use a more nuanced approach to analyzing the data about successful and unsuccessful legal bills.", "Presently, researchers use a metric named the Legal Effectiveness Score (LES) to analyze the efficacy of bills and legislators. The LES scoring system measures how far a particular bill advances within the complex multi-step legislative process.", "\u201cBut,\u201d as Casas reminded us, \u201ca bill is a vehicle for policy ideas and not necessarily a policy idea itself.\u201d What LES does not account for is that the main ideas of several bills are usually extracted and then inserted into other larger bills that ", "eventually become the law. Moreover, the text, meaning, and intention of the ideas often remain intact when incorporated into larger bills.", "With this in mind, analyzing the evolution of these \u2018hitchhiker\u2019 bills, as Casas and co-authors ", "and ", "called them, instead of simply counting how many bills passed and failed, would be a more accurate way of measuring legislative efficiency. The question is, how can this be done?", "After compiling a dataset of 104,005 versions of non-enrolled bills and 4,073 enrolled bills from the 103rd to 113th Congress between the years of 1993 to 2014, Casas and colleagues tracked the insertion of non-enrolled bills into laws using an ensemble of NLP algorithms (that boast a 95% accuracy rate!). Essentially, these algorithms first pre-process the text of bills, reducing them to their core expression, and then evaluate the extent to which the full meaning of each non-enrolled bill has been inserted into a bill that became law in that same Congress.", "Their investigation yielded some revealing conclusions. For example, not only do more senate bills become law as hitchhikers on house laws (1,118) than when enacted on their own (1,037), but that they often become law when included in a bill that concerns a different topic. The key role that hitchhiker bills play in forming larger bills suggest, Casas concluded, that the legislative system is more decentralized and less partisan than we think. When taking these hitchhikers into account,we see that legislation is shaped by more viewpoints, interests, and people.", "An open question, however, is whether we have the ", " people in Congress. Will data science one day have the power to identify the ill-intentioned from the heroes? Well, it\u2019s not a reality yet \u2014 but one can certainly dream.", "Written by"], "postingTime": "2018-02-09T15:18:44.751Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "A Better Way to Predict Ambulance Calls from Two CDS Moore-Sloan Data Science Fellows", "content": [" 1854, John Snow made an early stride in the field of spatial epidemiology by mapping London cholera cases to determine the source of the outbreak. Modern spatial epidemiology has, of course, become more effective, especially by using data to predict medical incidents. Most current techniques related to non-infectious disease, however, focus on predicting the incidence of a particular disease without incorporating analysis of demographic or environmental factors.", ", which has been peer-reviewed and accepted to the ", "Broadly, their work aims to \u201cestimate the volume of ambulance calls at the level of individual Lower Super Output Areas (LSOA) in the North West of England,\u201d where LSOA\u2019s are small local areas with a median population of 1,520 people.", "To achieve their aim, they pose an important question: \u201c", "\u201d", "Answering this question required the researchers to examine geographic and demographic data, which they sourced from the UK government, online media sources, and the mobile web. In particular, three key data sources for their analysis were England\u2019s North West Ambulance Service, Foursquare user check-ins, and the UK government\u2019s Index of Multiple Deprivation (IMD) which identifies areas affected by lower quality of life.", "The researchers also improved their predictive capabilities by creating a new variable \u2014 daytime population \u2014 to address daily fluctuation in the number of people in one area. By estimating daytime population (workplace population plus residents younger than 16 and older than 74), the researchers were able to better predict ambulance calls than if they had simply used residential population statistics.", "Through correlation-driven data analysis, the researchers sought to determine whether certain types of ambulance calls are associated with different demographic and socio-economic conditions. They found that daytime population correlates with unconscious/fainting incidents, seizures, and falls; IMD correlates with breathing problems, chest pain, and psychiatric/suicide incidents; and dense urban centers correlate with unconscious/fainting incidents.", "With an understanding of these correlations, the researchers were able to generate a model for predicting both the number and type of ambulance calls in a local area. Though they are encouraged by their results, they recognize the limitations of data from social media due to potential biases. For future research, Noulas and Gon\u00e7alves suggest using \u201creal time digital datasets from location-based services to model medical incident activity not only across geographies, but also over time.\u201d They also hope that this type of research can soon begin to inform relevant public health policies.", "Written by"], "postingTime": "2018-02-28T15:36:18.435Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Love thy neighbor? Measuring immigrant integration in world cities using Twitter data", "content": ["lthough many cities today are multicultural hotspots, immigrant integration is still an on-going challenge \u2014 primarily because successful integration depends on several aspects like obtaining an education, finding employment, honing the key languages of the new country, and more. How can we measure the current state of immigrant integration?", "Researchers typically use metrics like spatial segregation to assess how integrated \u2014 or isolated \u2014 immigrant groups are, relative to the wider community. But the rise of social media means that researchers can also analyze the spatial segregation of languages through data from platforms like Twitter.", "Not only does Twitter data \u201chave the particularity of extending beyond national borders,\u201d explained Gon\u00e7alves ", ", but it can also \u201cquantify the spatial integration of immigrant communities\u201d by analyzing the spatio-temporal patterns of different languages in a given geographical location.", "With this in mind, the researchers collected over 350 million tweets posted by 14.5 million users between the years of 2010 to 2015 to examine immigrant integration in 53 cities.", "After extracting the UserID, geographical coordinates, date, time, and text of every tweet, they used some clever filtering techniques to confirm that each user actually lives in the place where they are tweeting (e.g. they\u2019re not just a visitor). Some of these filtering techniques involved calculating number of consecutive months of activity of each user, and the minimum number of hours spent by each user in the geographical area where their tweets are coming from.", "Then, the researchers used CLD2 (Chromium Compact Language Detector) to identify the language of each user\u2019s tweets. In addition to accounting for mutually intelligible languages and dialectical varieties, the researchers also labeled the official language of each city that they were examining as the \u201cLocal\u201d language.", "\u201cAfter defining the Local languages in each city,\u201d the researchers said, \u201cwe assign[ed] to each user its most frequent language. In case of bilingual/multilingual users, we set as user\u2019s language the one which differs from English or Local unless there are only two languages in [their] dictionary.\u201d", "The researchers also decided to remove English from their analysis because it\u2019s the world\u2019s ", " \u201cMoreover,\u201d the researchers added, \u201cthe role of English is dominant mainly in the worst links in terms of integration.\u201d", "After discarding English, their investigation yielded some fascinating results. \u201cArabic rises as the most common spatially segregated community,\u201d the researchers explained, \u201cfollowed by French-speaking communities that are spatially concentrated in other European countries such as Germany and Turkey.\u201d", "On a more positive note, they point out that London is in the lead of hosting diverse communities, followed by San Francisco, Tokyo, Los Angeles, Manchester, and New York.", "Of course, however, the researchers caution that Twitter data is only a partially representative sample of the population because the platform itself contains several biases, from the overrepresentation of young people, to the possibility that certain communities \u2014 like Chinese immigrants \u2014 may not use Twitter because it is inaccessible in their country of origin (China). Still, as Gon\u00e7alves and his researchers remind us, \u201cthe important question here is not whether we can find all the [immigrant] communities, but whether we are able to say something meaningful about those detected.\u201d", "Written by"], "postingTime": "2018-02-12T15:05:30.708Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "A Call to Action for Empiricism in Data Quality Research", "content": ["ow should the quality of data be measured? How does the quality of data affect the quality of data-driven insights? What is the value of data?", "In a new study, ", " Based on their study, Freire and her collaborators establish a framework for empirical methods of data evaluation throughout the entire data processing pipeline.", "The researchers\u2019 new framework involves two types of metrics (", " and ", ") and two scopes of methods (", " and ", "). The framework also includes a data continuum from real data to synthetic data.", " focus on the properties of data \u2014 they can be clearly defined and are application-independent such as completeness, lack of duplicates, or format consistency. This set of metrics would apply to information extraction and cleaning; and integration, aggregation, and representation.", ", however, are application-dependent, such as examining how well the data supports a specific business objective or determining the value of the data. These metrics apply to the modelling, analysis, and interpretation phases in the data processing pipeline.", " can be used in a variety of application contexts, while ", " are specific for a particular data type or application domain. This dichotomy, along with the dichotomy of extrinsic and intrinsic methods, classifies four ways of measuring data quality: Generic methods for intrinsic metrics, generic methods for extrinsic metrics, tailored methods for intrinsic metrics, and tailored methods for extrinsic metrics.", "This new classification from Freire and her collaborators establishes a path forward for future studies about data quality, and researchers can use this classification \u201cto reflect on the role of empiricism in their research, and identify gaps in their work towards a more comprehensive and impactful research agenda.\u201d", "Along with this new classification, the researchers call for three immediate actions that can be taken to promote empiricism in data quality research: the ", " of data, metadata, code, application scenarios, and benchmarks; new ", " for experimental design to separate between error creation and measurement; and, finally, the ", " of data quality research, including amendments or extensions to their classification.", "Written by"], "postingTime": "2018-02-27T15:07:04.648Z"}
{"nameOfPublication": "Learning New Stuff", "nameOfAuthor": "Per Harald Borgen", "articleTile": "A free course to teach you CSS Variables - Learning New Stuff - Medium", "content": ["Dear Learning New Stuff follower,", "The last few weeks I\u2019ve been taking a deep dive into CSS Variables, an awesome new technology of modern browsers. It brings the power of variables to CSS, which results in less repetition, better readability and more flexibility.", "This deep dive has resulted in a ", " on the subject.", "Each screencast is quick and to the point, as I\u2019d hate to waste your time.", "In some of the casts I\u2019ll also encourage you to pause the screencast and edit the code in order to solve a challenge (as this type of interactivity is possible with Scrimba). So this course will give you both theory and practice.", "Hope you like the course, and happy coding :)", "Cheers,", "Per", "Written by"], "postingTime": "2018-02-23T15:56:58.552Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 1. Exploratory Data Analysis with Pandas", "content": ["With this article, we, ", ", launch an open Machine Learning course. This is not aimed at developing another ", " introductory course on machine learning or data analysis (so this is not a substitute for fundamental education or online/offline courses/specializations and books). The purpose of this series of articles is to quickly refresh your knowledge and help you find topics for further advancement. Our approach is similar to that of the authors of ", ", which starts off with a review of mathematics and basics of machine learning \u2014 short, concise, and with many references to other resources.", " YouTube ", " with videolectures", "The course is designed to perfectly balance theory and practice; therefore, each topic is followed by an ", "with a deadline in a week. You can also take part in several Kaggle Inclass ", " held during the course.", "All materials are available as a ", " and in a ", ".", "The course is going to be actively discussed in the OpenDataScience Slack team. Please fill in ", " form to be invited. The next session of the course will start on ", ", 2018. Invitations will be sent in September.", "1. About the course", "2. Assignments", "3. Demonstration of main Pandas methods", "4. First attempt on predicting telecom churn", "5. Assignment #1", "6. Useful resources", "One of the most vivid advantages of our course is active community. If you join the OpenDataScience Slack team, you\u2019ll find the authors of articles and assignments right there in the same channel (#eng_mlcourse_open) eager to help you. This can help very much when you make your first steps in any discipline. Fill in ", " form to be invited. The form will ask you several questions about your background and skills, including a few easy math questions.", "We chat informally, like humor and emoji. Not every MOOC can boast to have such an alive community.", "The prerequisites are the following: basic concepts from calculus, linear algebra, probability theory and statistics, and Python programming skills. If you need to catch up, a good resource will be ", " from the \u201cDeep Learning\u201d book and various math and Python online courses (for Python, CodeAcademy will do). More info is available on the corresponding ", ".", "As for now, you\u2019ll only need ", " (built with Python 3.6) to reproduce the code in the course. Later in the course you\u2019ll have to install other libraries like Xgboost and Vowpal Wabbit.", "You can also resort to the ", " with all necessary software already installed. More info is available on the corresponding ", ".", "Well... There are dozens of cool tutorials on Pandas and visual data analysis. If you are familiar with these topics, just wait for the 3rd article in the series, where we get into machine learning.", " is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like ", ", ", ", or ", ". Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with ", " and ", ", ", " provides a wide range of opportunities for visual analysis of tabular data.", "The main data structures in ", " are implemented with ", " and ", " classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of ", "instances. ", " are great for representing real data: rows correspond to instances (objects, observations, etc.), and columns correspond to features for each of the instances.", "We\u2019ll demonstrate the main methods in action by analyzing a ", " on the churn rate of telecom operator clients. Let\u2019s read the data (using ", "), and take a look at the first 5 lines using the ", " method:", "Recall that each row corresponds to one client, the ", " of our research, and columns are ", " of the object.", "From the output, we can see that the table contains 3333 rows and 20 columns. Now let\u2019s try printing out the column names using ", ":", "We can use the ", " method to output some general information about the dataframe:", ", ", ", ", " and ", " are the data types of our features. We see that one feature is logical (", "), 3 features are of type ", ", and 16 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 3333 observations, the same number of rows we saw before with ", ".", "We can ", " with the ", " method. Let\u2019s apply this method to the ", " feature to convert it into ", ":", "The ", " method shows basic statistical characteristics of each numerical feature (", " and ", " types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles.", "In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the ", " parameter.", "For categorical (type ", ") and boolean (type ", ") features we can use the ", " method. Let\u2019s have a look at the distribution of ", ":", "2850 users out of 3333 are loyal; their ", " value is ", ". To calculate the proportion, pass ", " to the ", " function", "A DataFrame can be sorted by the value of one of the variables (i.e columns). For example, we can sort by Total day charge (use ", " to sort in descending order):", "Alternatively, we can also sort by multiple columns:", "DataFrame can be indexed in different ways.", "To get a single column, you can use a ", " construction. Let's use this to answer a question about that column alone: ", "14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt.", " with one column is also very convenient. The syntax is ", ", where ", " is some logical condition that is checked for each element of the ", " column. The result of such indexing is the DataFrame consisting only of rows that satisfy the ", " condition on the ", " column.", "Let\u2019s use it to answer the question:", "DataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The ", " method is used for ", ", while ", " is used for ", ".", "In the first case, we would say ", ", and, in the second case, we would say ", ".", "If we need the first or last line of the data frame, we use the ", " or ", " syntax.", "The ", " method can also be used to apply a function to each line. To do this, specify ", ". Lambda functions are very convenient in such scenarios. For example, if we need to select all states starting with W, we can do it like this:", "The ", " method can be used to ", " by passing a dictionary of the form ", " as its argument:", "Same thing can be done with the ", " method:", "In general, grouping data in Pandas goes as follows:", "Here is an example where we group the data according to the values of the ", " variable and display statistics of three columns in each group:", "Let\u2019s do the same thing, but slightly differently by passing a list of functions to ", ":", "Suppose we want to see how the observations in our sample are distributed in the context of two variables \u2014 ", " and ", ". To do so, we can build a ", " using the ", " method:", "We can see that most of the users are loyal and do not use additional services (International Plan/Voice mail).", "This will resemble ", " to those familiar with Excel. And, of course, pivot tables are implemented in Pandas: the ", " method takes the following parameters:", "Let\u2019s take a look at the average numbers of day, evening and night calls by area code:", "Like many other things in Pandas, adding columns to a DataFrame is doable in several ways.", "For example, if we want to calculate the total number of calls for all users, let\u2019s create the ", " Series and paste it into the DataFrame:", "It is possible to add a column more easily without creating an intermediate Series instance:", "To delete columns or rows, use the ", " method, passing the required indexes and the ", " parameter (", " if you delete columns, and nothing or ", " if you delete rows). The ", " argument tells whether to change the original DataFrame. With ", ", the ", " method doesn't change the existing DataFrame and returns a new one with dropped rows or columns. With ", ", it alters the DataFrame.", "Let\u2019s see how churn rate is related to the ", " variable. We\u2019ll do this using a ", " contingency table and also through visual analysis with ", " (however, visual analysis will be covered more thoroughly in the next article).", "We see that, with ", ", the churn rate is much higher, which is an interesting observation! Perhaps large and poorly controlled expenses with international calls are very conflict-prone and lead to dissatisfaction among the telecom operator\u2019s customers.", "Next, let\u2019s look at another important feature \u2014 ", ". Let\u2019s also make a summary table and a picture.", "Perhaps, it is not so obvious from the summary table, but the picture clearly states that the churn rate strongly increases starting from 4 calls to the service center.", "Let\u2019s now add a binary attribute to our DataFrame \u2014 ", ". And once again, let's see how it relates to the churn.", "Let\u2019s construct another contingency table that relates ", " with both ", " and freshly created ", ".", "Therefore, predicting that a customer will churn (", "=1) in the case when the number of calls to the service center is greater than 3 and the ", " is added (and predicting ", "=0 otherwise), we might expect an accuracy of 85.8% (we are mistaken only 464 + 9 times). This number, 85.8%, that we got with very simple reasoning serves as a good starting point (", ") for the further machine learning models that we will build.", "As we move on in this course, recall that, before the advent of machine learning, the data analysis process looked something like this. Let\u2019s recap what we\u2019ve covered:", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-10-07T12:47:36.075Z"}
{"nameOfPublication": "Center for Data Science", "nameOfAuthor": "NYU Center for Data Science", "articleTile": "Memory lanes: using neural circuit architectures for predicting recognition behavior", "content": ["What sparks recognition? Neuroscientists speculate that it\u2019s usually either \u201cdue to recollection (\u2018here comes my old school buddy\u2019),\u201d ", " \u201cor due to a vague sense of familiarity (\u2018I\u2019m sure I\u2019ve met this person before, but I have no idea when and why\u2019).\u201d", "There are presently two theories in neuroscience that are used to analyze neural data about recognition memory \u2014 dual module theories (DM) and single module theories (SM). DM theories believe that recollection and familiarity are independent aspects, while SM theories believe that recollection and familiarity work together to spark recognition.", "\u201cDespite over 30 years of memory recognition research,\u201d the researchers explain, \u201cno consensus exists about which class of models provides a more satisfying account of the data.\u201d Savin and her co-author, ", ", however, have developed a new neural circuit architecture that combines both SM and DM models. The neural network provides a more wholesome analysis of neural data, and can \u201cefficiently operate in the face of trace strength-ambiguity.\u201d", "Written by"], "postingTime": "2018-01-31T15:23:22.896Z"}
{"nameOfPublication": "Learning New Stuff", "nameOfAuthor": "Per Harald Borgen", "articleTile": "Here\u2019s my free full-length Flexbox course - Learning New Stuff - Medium", "content": ["Dear Learning New Stuff follower! As you might know, I co-founded ", " about a year ago. Our mission is to make it easier to teach and learn code through interactive screencasts.", "To help people understand the platform better, we\u2019ve just launched ", ", which takes you from beginner to advanced in 12 interactive screencasts.", "I\u2019ve written ", " which explains the entire course, so that you can decide whether it\u2019s a good fit for you. Simply click the image below and you\u2019ll be taken to the article.", "I really hope you\u2019ll enjoy the course. If you do, please give ", " a clap, so that more people will discover it :)", "Cheers", "Per", "Written by"], "postingTime": "2018-01-25T08:55:43.312Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Dmitriy Sergeev", "articleTile": "Open Machine Learning Course. Topic 9. Part 1. Time series analysis in Python", "content": ["Hi there!", "We continue our open machine learning course with a new article on time series.", "Let\u2019s take a look at how to work with time series in Python, what methods and models we can use for prediction; what\u2019s double and triple exponential smoothing; what to do if stationarity is not you favorite game; how to build SARIMA and stay alive; how to make predictions using xgboost. All of this will be applied to (harsh) real world example.", "In my day to day job I encounter time series-connected tasks almost every day. The most frequent question is \u2014 what will happen with our metrics in the next day/week/month/etc. \u2014 how many players will install the app, how much time will they spend online, how many actions users will do, and so forth. We can approach prediction task using different methods, depending on the required quality of the prediction, length of the forecasted period, and, of course, time we have to choose features and tune parameters to achieve desired results.", "Small ", " of time series:", "Therefore data is organized around relatively deterministic timestamps, and therefore, compared to random samples, may contain additional information that we will try to extract.", "Let\u2019s import some libraries. First and foremost we will need ", " library that has tons of statistical modeling functions, including time series. For R afficionados (that had to move to python) statsmodels will definitely look familiar as it supports model definitions like \u2018Wage ~ Age + Education\u2019.", "As an example let\u2019s use some real mobile game data on hourly ads watched by players and daily in-game currency spent:", "Before actually forecasting, let\u2019s understand how to measure the quality of predictions and have a look at the most common and widely used metrics", "Excellent, now we know how to measure the quality of the forecasts, what metrics can we use and how to translate the results to the boss. Little thing is left \u2014 building the model.", "Let\u2019s start with a naive hypothesis \u2014 \u201ctomorrow will be the same as today\u201d, but instead of a model like y\u0302(t)=y(t\u22121) (which is actually a great baseline for any time series prediction problems and sometimes it\u2019s impossible to beat it with any model) we\u2019ll assume that the future value of the variable depends on the average ", " of its previous values and therefore we\u2019ll use ", ".", "Unfortunately we can\u2019t make this prediction long-term \u2014 to get one for the next step we need the previous value to be actually observed. But moving average has another use case \u2014 smoothing of the original time series to indicate trends. Pandas has an implementation available ", ". The wider the window - the smoother will be the trend. In the case of the very noisy data, which can be very often encountered in finance, this procedure can help to detect common patterns.", "Smoothing by last 4 hours", "Smoothing by last 12 hours", "Smoothing by 24 hours \u2014 we get daily trend", "As you can see, applying daily smoothing on hour data allowed us to clearly see the dynamics of ads watched. During the weekends the values are higher (weekends \u2014 time to play) and weekdays are generally lower.", "We can also plot confidence intervals for our smoothed values", "And now let\u2019s create a simple anomaly detection system with the help of the moving average. Unfortunately, in this particular series everything is more or less normal, so we\u2019ll intentionally make one of the values abnormal in the dataframe ", "Let\u2019s see, if this simple method can catch the anomaly", "Neat! What about the second series (with weekly smoothing)?", "Oh no! Here is the downside of our simple approach \u2014 it did not catch monthly seasonality in our data and marked almost all 30-day peaks as an anomaly. If you don\u2019t want to have that many false alarms \u2014 it\u2019s best to consider more complex models.", " is a simple modification of the moving average, inside of which observations have different weights summing up to one, usually more recent observations have greater weight.", "And now let\u2019s take a look at what happens if instead of weighting the last nn values of the time series we start weighting all available observations while exponentially decreasing weights as we move further back in historical data. There\u2019s a formula of the simple ", " that will help us in that:", "Here the model value is a weighted average between the current true value and the previous model values. The \u03b1 weight is called a smoothing factor. It defines how quickly we will \u201cforget\u201d the last available true observation. The less \u03b1 is the more influence previous model values have, and the smoother the series is.", "Exponentiality is hiding in the recursivity of the function \u2014 we multiply each time (1\u2212\u03b1) by the previous model value which, in its turn, also containes (1\u2212\u03b1) and so forth until the very beginning.", "Until now all we could get from our methods in the best case was just a single future point prediction (and also some nice smoothing), that\u2019s cool but not enough, so let\u2019s extend exponential smoothing so that we can predict two future points (of course, we also get some smoothing).", "Series decomposition should help us \u2014 we obtain two components: intercept (also, level) \u2113 and trend (also, slope) b. We\u2019ve learnt to predict intercept (or expected series value) using previous methods, and now we will apply the same exponential smoothing to the trend, believing naively or perhaps not that the future direction of the time series changes depends on the previous weighted changes.", "As a result we get a set of functions. The first one describes intercept, as before it depends on the current value of the series, and the second term is now split into previous values of the level and of the trend. The second function describes trend \u2014 it depends on the level changes at the current step and on the previous value of the trend. In this case \u03b2 coefficient is a weight in the exponential smoothing. The final prediction is the sum of the model values of the intercept and trend.", "Now we have to tune two parameters \u2014 \u03b1 and \u03b2. The former is responsible for the series smoothing around trend, and the latter for the smoothing of the trend itself. The bigger the values, the more weight the latest observations will have and the less smoothed the model series will be. Combinations of the parameters may produce really weird results, especially if set manually. We\u2019ll look into choosing parameters automatically in a bit, immediately after triple exponential smoothing.", "Hooray! We\u2019ve successfully reached our next variant of exponential smoothing, this time triple.", "The idea of this method is that we add another, third component \u2014 seasonality. This means we should\u2019t use the method if our time series do not have seasonality, which is not the case in our example. Seasonal component in the model will explain repeated variations around intercept and trend, and it will be described by the length of the season, in other words by the period after which variations repeat. For each observation in the season there\u2019s a separate component, for example, if the length of the season is 7 (weekly seasonality), we will have 7 seasonal components, one for each day of the week.", "Now we get a new system:", "Intercept now depends on the current value of the series minus corresponding seasonal component, trend stays unchanged, and the seasonal component depends on the current value of the series minus intercept and on the previous value of the component. Please take into account that the component is smoothed through all the available seasons, for example, if we have a Monday component then it will only be averaged with other Mondays. You can read more on how averaging works and how initial approximation of the trend and seasonal components is done ", ". Now that we have seasonal component we can predict not one and not even two but arbitrary mm future steps which is very encouraging.", "Below is the code for a triple exponential smoothing model, also known by the last names of its creators \u2014 Charles Holt and his student Peter Winters. Additionally Brutlag method was included into the model to build confidence intervals:", "where T is the length of the season, d is the predicted deviation, and the other parameters were taken from the triple exponential smoothing. You can read more about the method and its applicability to anomalies detection in time series ", ".", "Before we start building model let\u2019s talk first about how to estimate model parameters automatically.", "There\u2019s nothing unusual here, as always we have to choose a loss function suitable for the task, that will tell us how close the model approximates data. Then using cross-validation we will evaluate our chosen loss function for given model parameters, calculate gradient, adjust model parameters and so forth, bravely descending to the global minimum of error.", "The question is how to do cross-validation on time series, because, you know, time series do have time structure and one just can\u2019t randomly mix values in a fold without preserving this structure, otherwise all time dependencies between observations will be lost. That\u2019s why we will have to use a bit more tricky approach to optimization of the model parameters, I don\u2019t know if there\u2019s an official name to it but on ", ", where one can find all the answers but the Answer to the Ultimate Question of Life, the Universe, and Everything, \u201ccross-validation on a rolling basis\u201d was proposed as a name.", "The idea is rather simple \u2014 we train our model on a small segment of the time series, from the beginning until some ", ", make predictions for the next ", "steps and calculate an error. Then we expand our training sample until ", " value and make predictions from ", " until ", ", and we continue moving our test segment of the time series until we hit the last available observation. As a result we have as many folds as many ", " will fit between the initial training sample and the last observation.", "Now, knowing how to set cross-validation, we will find optimal parameters for the Holt-Winters model, recall that we have daily seasonality in ads, hence the ", " parameter", "In the Holt-Winters model, as well as in the other models of exponential smoothing, there\u2019s a constraint on how big smoothing parameters could be, each of them is in the range from 0 to 1, therefore to minimize loss function we have to choose an algorithm that supports constraints on model parameters, in our case \u2014 Truncated Newton conjugate gradient.", "Chart rendering code", "Judging by the chart, our model was able to successfully approximate the initial time series, catching daily seasonality, overall downwards trend and even some anomalies. If you take a look at the modeled deviation, you can clearly see that the model reacts quite sharply to the changes in the structure of the series but then quickly returns deviation to the normal values, \u201cforgetting\u201d the past. This feature of the model allows us to quickly build anomaly detection systems even for quite noisy series without spending too much time and money on preparing data and training the model.", "We\u2019ll apply the same algorithm for the second series which, as we know, has trend and 30-day seasonality", "Looks quite adequate, model has caught both upwards trend and seasonal spikes and overall fits our values nicely", "Before we start modeling we should mention such an important property of time series as ", ".", "If the process is stationary that means it doesn\u2019t change its statistical properties over time, namely mean and variance do not change over time (constancy of variance is also called ", "), also covariance function does not depend on the time (should only depend on the distance between observations). You can see this visually on the pictures from the post of ", ":", "So why stationarity is so important? Because it\u2019s easy to make predictions on the stationary series as we assume that the future statistical properties will not be different from the currently observed. Most of the time series models in one way or the other model and predict those properties (mean or variance, for example), that\u2019s why predictions would be wrong if the original series were not stationary. Unfortunately most of the time series we see outside of textbooks are non-stationary but we can (and should) change this.", "So, to fight non-stationarity we have to know our enemy so to say. Let\u2019s see how to detect it. To do that we will now take a look at the white noise and random walks and we will learn how to get from one to another for free, without registration and SMS.", "White noise chart:", "So the process generated by standard normal distribution is stationary and oscillates around 0 with with deviation of 1. Now based on this process we will generate a new one where each next value will depend on the previous one: x(t)=\u03c1*x(t\u22121)+e(t)", "Chart rendering code", "On the first chart you can see the same stationary white noise you\u2019ve seen before. On the second one the value of \u03c1\u03c1 increased to 0.6, as a result wider cycles appeared on the chart but overall it is still stationary. The third chart deviates even more from the 0 mean but still oscillates around it. Finally, the value of \u03c1 equal to 1 gives us a random walk process \u2014 non-stationary time series.", "This happens because after reaching the critical value the series x(t)=\u03c1*x(t\u22121)+e(t) does not return to its mean value. If we subtract x(t\u22121) from the left and the right side we will get x(t)\u2212x(t\u22121)=(\u03c1\u22121)*x(t\u22121)+e(t), where the expression on the left is called the first difference. If \u03c1=1 then the first difference gives us stationary white noise e(t). This fact is the main idea of the ", " for the stationarity of time series (presence of a unit root). If we can get stationary series from non-stationary using the first difference we call those series integrated of order 1. Null hypothesis of the test \u2014 time series is non-stationary, was rejected on the first three charts and was accepted on the last one. We\u2019ve got to say that the first difference is not always enough to get stationary series as the process might be integrated of order d, d > 1 (and have multiple unit roots), in such cases the augmented Dickey-Fuller test is used that checks multiple lags at once.", "We can fight non-stationarity using different approaches \u2014 various order differences, trend and seasonality removal, smoothing, also using transformations like Box-Cox or logarithmic.", "Now let\u2019s build an ARIMA model by walking through all the circles of hell stages of making series stationary.", "Chart rendering code", "Surprisingly, initial series are stationary, Dickey-Fuller test rejected null hypothesis that a unit root is present. Actually, it can be seen on the plot itself \u2014 we don\u2019t have a visible trend, so mean is constant, variance is pretty much stable throughout the series. The only thing left is seasonality which we have to deal with before modelling. To do so let\u2019s take \u201cseasonal difference\u201d which means a simple subtraction of series from itself with a lag that equals the seasonal period.", "That\u2019s better, visible seasonality is gone, however autocorrelation function still has too many significant lags. To remove them we\u2019ll take first differences \u2014 subtraction of series from itself with lag 1", "Perfect! Our series now look like something undescribable, oscillating around zero, Dickey-Fuller indicates that it\u2019s stationary and the number of significant peaks in ACF has dropped. We can finally start modelling!", "A few words about the model. Letter by letter we\u2019ll build the full name \u2014 ", ", Seasonal Autoregression Moving Average model:", "Let\u2019s have a small break and combine the first 4 letters:", "What we have here is the Autoregressive\u2013moving-average model! If the series is stationary, it can be approximated with those 4 letters. Shall we continue?", "Adding this letter to four previous gives us ", " model which knows how to handle non-stationary data with the help of nonseasonal differences. Awesome, last letter left!", "After attaching the last letter we find out that instead of one additional parameter we get three in a row \u2014 ", "Now, knowing how to set initial parameters, let\u2019s have a look at the final plot once again and set the parameters:", "Now we want to test various models and see which one is better", "Let\u2019s inspect the residuals of the model", "Well, it\u2019s clear that the residuals are stationary, there are no apparent autocorrelations, let\u2019s make predictions using our model", "In the end we got quite adequate predictions, our model on average was wrong by 4.01%, which is very very good, but overall costs of preparing data, making series stationary and brute-force parameters selecting might not be worth this accuracy.", "Small lyrical digression again. Often in my job I have to build models with the only principle guiding me known as ", ". That means some of the models will never be \u201cproduction ready\u201d as they demand too much time for the data preparation (for example, SARIMA), or require frequent re-training on new data (again, SARIMA), or are difficult to tune (good example \u2014 SARIMA), so it\u2019s very often much easier to select a couple of features from the existing time series and build a simple linear regression or, say, a random forest. Good and cheap.", "Maybe this approach is not backed up by theory, breaks different assumptions (like, Gauss-Markov theorem, especially about the errors being uncorrelated), but it\u2019s very useful in practice and quite frequently used in machine learning competitions.", "Alright, model needs features and all we have is a 1-dimentional time series to work with. What features can we exctract?", " (though we can lose the speed of prediction this way)", "Let\u2019s run through some of the methods and see what we can extract from our ads series", "Shifting the series ", " steps back we get a feature column where the current value of time series is aligned with its value at the time ", ". If we make a 1 lag shift and train a model on that feature, the model will be able to forecast 1 step ahead having observed current state of the series. Increasing the lag, say, up to 6 will allow the model to make predictions 6 steps ahead, however it will use data, observed 6 steps back. If something fundamentally changes the series during that unobserved period, the model will not catch the changes and will return forecasts with big error. So, during the initial lag selection one has to find a balance between the optimal prediction quality and the length of forecasting horizon.", "Wonderful, we got ourselves a dataset here, why don\u2019t we train a model?", "Well, simple lags and linear regression gave us predictions that are not that far from SARIMA in quality. There are lot\u2019s of unnecessary features, but we\u2019ll do feature selection a bit later. Now let\u2019s continue engineering!", "We\u2019ll add into our dataset hour, day of the week and boolean for the weekend. To do so we need to transform current dataframe index into ", " format and exctract ", " and ", " out of it.", "We can visualize the resulting features", "Since now we have different scales of variables \u2014 thousands for lag features and tens for categorical, it\u2019s reasonable to transform them into same scale to continue exploring feature importances and later \u2014 regularization.", "Test error goes down a little bit and judging by the coefficients plot we can say that ", " and ", " are rather useful features", "I\u2019d like to add another variant of encoding categorical variables \u2014 by mean value. If it\u2019s undesirable to explode dataset by using tons of dummy variables that can lead to the loss of information about the distance, and if they can\u2019t be used as real values because of the conflicts like \u201c0 hours < 23 hours\u201d, then it\u2019s possible to encode a variable with a little bit more interpretable values. Natural idea is to encode with the mean value of the target variable. In our example every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It\u2019s very important to make sure that the mean value is calculated over train set only (or over current cross-validation fold only), so that the model is not aware of the future.", "Let\u2019s have a look at hour averages", "Finally, put all the transformations together in a single function", "Here comes ", "! ", " variable was so great on train dataset that the model decided to concentrate all its forces on it - as a result the quality of prediction dropped. This problem can be approached in a variety of ways, for example, we can calculate target encoding not for the whole train set, but for some window instead, that way encodings from the last observed window will probably describe current series state better. Or we can just drop it manually, since we're sure here it makes things only worse.", "As we already know, not all features are equally healthy, some may lead to overfitting and should be removed. Besides manual inspecting we can apply regularization. Two most popular regression models with regularization are Ridge and Lasso regressions. They both add some more constrains to our loss function.", "In case of ", " \u2014 those constrains are the sum of squares of coefficients, multiplied by the regularization coefficient. I.e. the bigger coefficient feature has \u2014 the bigger our loss will be, hence we will try to optimize the model while keeping coefficients fairly low.", "As a result of such regularization which has a proud name ", " we\u2019ll have higher bias and lower variance, so the model will generalize better (at least that\u2019s what we hope will happen).", "Second model \u2014", ", here we add to the loss function not squares but absolute values of the coefficients, as a result during the optimization process coefficients of unimportant features may become zeroes, so Lasso regression allows for automated feature selection. This regularization type is called ", ".", "First, make sure we have things to drop and data truly has highly correlated features", "We can clearly see how coefficients are getting closer and closer to zero (thought never actually reach it) as their importance in the model drops", "Lasso regression turned out to be more conservative and removed 23-rd lag from most important features (and also dropped 5 features completely) which only made the quality of prediction better.", "Why not try XGBoost now?", "Here is the winner! The smallest error on the test set among all the models we\u2019ve tried so far.", "Yet this victory is decieving and it might not be the brightest idea to fit xgboost as soon as you get your hands over time series data. Generally tree-based models poorly handle trends in data, compared to linear models, so you have to detrend your series first or use some tricks to make the magic happen. Ideally \u2014 make the series stationary and then use XGBoost, for example, you can forecast trend separately with a linear model and then add predictions from xgboost to get final forecast.", "We got acquainted with different time series analysis and prediction methods and approaches. Unfortunately, or maybe luckily, there\u2019s no silver bullet to solve this kind of problems. Methods developed in the 60s of the last century (and some even in the beginning of the XIX century) are still popular along with the LSTM and RNN (not covered in this article). Partially this is related to the fact that the prediction task as any other data related task is creative in so many aspects and definitely requires research. In spite of the large number of formal quality metrics and approaches to parameters estimation, it\u2019s often required to seek and try something different for each time series. Last but not least the balance between quality and cost is important. As a good example SARIMA model mentioned here not once or twice can produce spectacular results after due tuning but might require many hours of tambourine dancing time series manipulation, as in the same time simple linear regression model can be build in 10 minutes giving more or less comparable results.", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:12:18.318Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 4. Linear Classification and Regression", "content": ["Welcome to the 4-th week of our course! Now we will present our most important topic \u2014 linear models. If you have your data prepared and want to start training models, then you will most probably first try either linear or logistic regression, depending on your task (regression or classification).", "This week\u2019s material covers both theory of linear models and practical aspects of their usage in real-world tasks. There\u2019s going to be a lot of math in this topic, and we won\u2019t even try to render all the formulas on Medium. Instead, we provide a Jupyter Notebook for each part of this article. In the assignment, you\u2019ll beat two simple benchmarks in a Kaggle competition solving a problem of identifying a user based on her session of visited websites.", "We will be solving the intruder detection problem analyzing users\u2019 behavior on the Internet. It is a complicated and interesting problem combining data analysis and behavioral psychology. As an illustration of one of such tasks, Yandex solves the mailbox intruder detection problem based on the user\u2019s behavior patterns. In a nutshell, intruder\u2019s behavior patterns may differ from those of the mailbox owner:", "So the intruder could be detected and thrown out from the mailbox forcing the user to authenticate via SMS-code.", "Similar approaches are being developed in Google Analytics and described in scientific research papers. You can find more on this topic by searching \u201cTraversal Pattern Mining\u201d and \u201cSequential Pattern Mining\u201d.", "In this competition we are going to solve a similar problem: our algorithm is supposed to analyze the sequence of websites consequently visited by a particular person and predict whether this person is a user named Alice or an intruder (somebody else). As a metric, we will use ", ".", "Register on ", ", if you have not done it before. Go to the competition ", " and download the data.", "First, load the training and test sets. Then explore the data and perform a couple of simple exercises:", "The training dataset contains the following features:", "User sessions are chosen in such a way that they are not longer than half an hour and/or contain more than ten websites; i.e. a session is considered as ended either if the user has visited ten websites or if the session has lasted over thirty minutes.", "There are some empty values in the table, which means that some sessions contain less than ten websites. Replace empty values with 0 and change column types to integer. Also, load the website dictionary and see how it looks:", "In order to train our first model, we need to prepare the data. First of all, exclude the target variable from the training set. Now both training and test sets have the same number of columns, and we can aggregate them into one dataframe. Thus, all transformations will be performed simultaneously on both the training and test datasets.", "On the one hand, it leads to the fact that both of our datasets have one feature space (so you don\u2019t have to worry that you may have forgotten to transform a feature in one of the datasets). On the other hand, the processing time will increase. In case of enormously large sets, it may turn out that it is impossible to transform both datasets simultaneously (and sometimes you have to split your transformations into several stages, separately for the train/test dataset). In our case, we are going to perform all the transformations for the united dataframe at once; and, before training the model or making predictions, we will just use the corresponding part of it.", "For the sake of simplicity, we will use only the visited websites in the session (and we will not take into account the timestamp features). The point behind this data selection is: ", "Let\u2019s prepare the data. We will keep only the features ", " in the dataframe. Keep in mind that missing values were replaced with zeros. Here is how the first rows of the dataframe look like:", "Sessions are the sequences of website indices, and such representation of data is inconvenient for linear methods. According to our hypothesis (Alice has favorite websites), we need to transform this dataframe so that each website has the corresponding feature (column) which value is equal to the number of visits on this website within the session. All of this can be done in two lines:", "If you understand what just happened here, then you can skip the next section (perhaps, you can handle logistic regression too?). If not, then let us figure it out.", "Let\u2019s estimate how much memory it would require to store our data in the example above. Our united dataframe contains 336 thousand samples of 48 thousand integer features in each. It\u2019s easy to calculate the required amount of memory, roughly:", "336K * 48K * 8 bytes = 16M * 8 bytes = 128 GB", "Obviously, mere mortals don\u2019t have such volumes of memory (strictly speaking, Python may allow you to create such a matrix, but it would not be easy to do anything with it). An interesting fact is that most of the elements of our matrix are zeros. If we counted non-zero elements, then it would make out about 1.8 million, i.\u0435. slightly more than 10% of all matrix elements. Such a matrix, where most elements are zeros, is called ", ", and the ratio between the number of zero elements and the total number of elements is called the ", ".", "To work with such matrices, you can use ", " library, check the ", " to understand what possible types of sparse matrices are, how to work with them and in which cases their usage is most effective. You can learn how they are arranged, for example, be reading the Wikipedia ", ". Note that a sparse matrix contains only non-zero elements. Finally, you can get the allocated memory size (significant memory savings are obvious):", "Let\u2019s explore how the matrix with the websites has been formed using a mini example. Suppose we have the following table with user sessions:", "There are 3 sessions, and no more than 3 websites in each. Users visited four different sites in total (there are numbers from 1 to 4 in the table cells). And let us assume that:", "If the user has visited less than 3 websites during the session, the last few values will be zero. We want to convert the original dataframe in such a way that each session would have the corresponding row which shows the number of visits on each particular site; i.e. we want to transform the previous table into the following form:", "To do this, use the constructor: ", " and create a frequency table (see examples, code and comments on the links above to see how it works). Here, we set all the parameters explicitly for greater clarity:", "As you might have noticed, the number of the columns in the resulting matrix is not four (by the number of different websites), but five. A zero column has been added, which shows on how many sites the session was shorter (in our mini example we took sessions of length 3). This column is excessive and it should be removed from the dataframe.", "Another benefit of using sparse matrices is that there are special implementations of both matrix operations and machine learning algorithms for them, which sometimes allows to significantly accelerate operations due to the data structure peculiarities. This applies to logistic regression as well. Now, everything is ready to build our first model.", "Let\u2019s build our first model, using ", " implementation from ", " with default parameters. We will use the first 90% of the data for training (the training data set is sorted by time), and the remaining 10% for validation. Let's write a simple function that returns the quality of the model, and then train our first classifier:", "The first model demonstrated the quality of approximately 0.92 ROC AUC on the validation set. Let\u2019s take it as the first baseline and a starting point. To make a prediction on the test set, ", " (until this moment, our model used only part of the data for training), which will increase its generalizing ability:", "If you follow these steps and upload the answer to the competition ", ", then you should get the quality of ", " on the public leaderboard.", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:03:25.487Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors", "content": ["Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!", "3. Nearest Neighbors Method", "4. Choosing Model Parameters and Cross-Validation", "5. Application Examples and Complex Cases", "6. Pros and Cons of Decision Trees and the Nearest Neighbors Method", "7. Assignment #3", "8. Useful resources", "Before we dive into the material for this week\u2019s article, let\u2019s talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell\u2019s book ", " (1997) gives a classic, general definition of machine learning as follows:", "A computer program is said to learn from experience ", " with respect to some class of tasks ", " and performance measure ", ", if its performance at tasks in ", ", as measured by ", ", improves with experience ", ".", "In the various problem settings ", ", ", ", and ", " can refer to completely different things. Some of the most popular ", " are the following:", "A good overview is provided in the \u201cMachine Learning basics\u201d chapter of ", " (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).", " refers to data (we can\u2019t go anywhere without it). Machine learning algorithms can be divided into those that are trained in ", " or ", " manner. In unsupervised learning tasks, one has a ", " consisting of ", " described by a set of ", ". In supervised learning problems, there\u2019s also a ", ", which is what we would like to be able to predict, known for each instance in a ", ".", "Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience ", " is the available training data: a set of ", " (clients), a collection of ", " (such as age, salary, type of loan, past loan defaults, etc.) for each, and a ", " (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting ", " the loan payment is overdue, this would become a regression problem.", "Finally, the third term used in the definition of machine learning is a ", " Such metrics differ for various problems and algorithms, and we\u2019ll discuss them as we study new algorithms. For now, we\u2019ll refer to a simple metric for classification algorithms, the proportion of correct answers \u2014 ", " \u2014 on the test set.", "Let\u2019s take a look at two supervised learning problems: classification and regression.", "We begin our overview of classification and regression methods with one of the most popular ones \u2014 a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.", "In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the \u201cHigher School of Economics and the Media\u201d) based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.", "A decision tree is often a generalization of the experts\u2019 experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree.", "In our next case, we solve a binary classification problem (approve/deny a loan) on the grounds of \u201cAge\u201d, \u201cHome-ownership\u201d, \u201cIncome\u201d and \u201cEducation\u201d.", "The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form \u201cfeature ", " value is less than ", "and feature ", " value is less than ", " \u2026 => Category 1\u201d into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.", "As we\u2019ll see later, many other models, although more accurate, do not have this property and can be regarded as more of a \u201cblack box\u201d approach, where it is harder to interpret how the input data was transformed into the output. Due to this \u201cunderstandability\u201d and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (\u201cTop 10 Algorithms in Data Mining\u201d, Knowledge and Information Systems, 2008. ", ").", "Earlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let\u2019s discuss a simple example where all the variables are binary.", "Recall the game of \u201c20 Questions\u201d, which is often referenced when introducing decision trees. You\u2019ve probably played this game \u2014 one person thinks of a celebrity while the other tries to guess by asking only \u201cYes\u201d or \u201cNo\u201d questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking \u201cIs it Angelina Jolie?\u201d would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking \u201cIs the celebrity a woman?\u201d would reduce the possibilities to roughly half. That is to say, the \u201cgender\u201d feature separates the celebrity dataset much better than other features like \u201cAngelina Jolie\u201d, \u201cSpanish\u201d, or \u201cloves football.\u201d This reasoning corresponds to the concept of information gain based on entropy.", "Shannon\u2019s entropy is defined for a system with N possible states as follows:", "where ", "is the probability of finding the system in the ", "-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize \u201ceffective data splitting\u201d, which we alluded to in the context of \u201c20 Questions\u201d.", "To illustrate how entropy can help us identify good features for building a decision tree, let\u2019s look at a toy example. We will predict the color of the ball based on its position.", "There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability p1 = 9/20 and yellow with probability p2 = 11/20, which gives us an entropy S0 = -9/20 log2(9/20) - 11/20 log2(11/20) \u2248 1. This value by itself may not tell us much, but let\u2019s see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12.", "The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is S1 = -5/13 log2(5/13) - 8/13 log2(8/13) \u2248 0.96. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is S2 = -1/7 log2(1/7) - 6/7 log2(6/7) \u2248 0.6. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable ", "(in this example it\u2019s a variable \u201c", "\u201d) is defined as", "where ", "is the number of groups after the split, ", " is number of objects from the sample in which variable ", " is equal to the ", "-th value. In our example, our split yielded two groups (q = 2), one with 13 elements (N1 = 13), the other with 7 (N2 = 7). Therefore, we can compute the information gain as", "It turns out that dividing the balls into two groups by splitting on \u201ccoordinate is less than or equal to 12\u201d gave us a more ordered system. Let\u2019s continue to divide them into groups until the balls in each group are all of the same color.", "For the right group, we can easily see that we only need one extra partition using \u201ccoordinate less than or equal to 18\u201d. But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 (log2(1) = 0).", "We have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer \u201cquestions\u201d or splits would be more accurate, even if it does not perfectly fit the training set. We will discuss the problem of overfitting later.", "We can make sure that the tree built in the previous example is optimal: it took only 5 \u201cquestions\u201d (conditioned on the variable ", ") to perfectly fit a decision tree to the training set. Under other split conditions, the resulting tree would be deeper, i.e. take more \u201cquestions\u201d to reach an answer.", "At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for \u201cearly stopping\u201d or \u201ccut-off\u201d to avoid constructing an overfitted tree.", "We discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exist others.", "Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).", "In practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.", "For binary classification, entropy and Gini uncertainty take the following form:", "where (", " is the probability of an object having a label +).", "If we plot these two functions against the argument ", ", we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.", "Let\u2019s consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.", "Let\u2019s plot the data. Informally, the classification problem in this case is to build some \u201cgood\u201d boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or, at least, a straight line or a hyperplane, would work well on new data.", "Let\u2019s try to separate these two classes by training an ", " decision tree. We will use ", " parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.", "And how does the tree itself look? We see that the tree \u201ccuts\u201d the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it.", "In the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, S=1. Then, the first partition of the samples into 2 groups was made by comparing the value of ", " with 1.211 (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.", "Suppose we have a numeric feature \u201cAge\u201d that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as \u201cAge <17\u201d, \u201cAge < 22.87\u201d, and so on. But what if the age range is large? Or what if another quantitative variable, \u201csalary\u201d, can also be \u201ccut\u201d in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.", "Let\u2019s consider an example. Suppose we have the following dataset:", "We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class \u201cswitches\u201d from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for \u201ccutting\u201d a quantitative variable.", "Given this information, why do you think it makes no sense here to consider a feature like \u201cAge <17.5\u201d?", "Let\u2019s consider a more complex example by adding the \u201cSalary\u201d variable (in the thousands of dollars per year).", "If we sort by age, the target class ( \u201cloan default\u201d) switches (from 1 to 0 or vice versa) 5 times. And if we sort by salary, it switches 7 times. How will the tree choose features now? Let\u2019s see.", "We see that the tree partitioned by both salary and age. Moreover, the thresholds for feature comparisons are 43.5 and 22.5 years of age and 95k and 30.5k per year. Again, we see that 95 is the average between 88 and 102; the individual with a salary of 88k proved to be \u201cbad\u201d while the one with 102k was \u201cgood\u201d. The same goes for 30.5k. That is, only a few values for comparisons by age and salary were searched. Why did the tree choose these features? Because they gave better partitioning (according to Gini uncertainty).", ": the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes.", "Furthermore, when there are a lot of numeric features in a dataset, each with a lot of unique values, only the top-N of the thresholds described above are selected, i.e. only use the top-N that give maximum gain. The process is to construct a tree of depth 1, compute the entropy (or Gini uncertainty), and select the best thresholds for comparison.", "To illustrate, if we split by \u201cSalary \u2264 34.5\u201d, the left subgroup will have the entropy of 0 (all clients are \u201cbad\u201d), and the right one will have the entropy of 0.954 (3 \u201cbad\u201d and 5 \u201cgood\u201d, you can check this yourself as it will be part of the assignment). The information gain is roughly 0.3. If we split by \u201cSalary \u2264 95\u201d, the left subgroup will have an entropy of 0.97 (6 \u201cbad\u201d and 4 \u201cgood\u201d), and the right one will have an entropy of 0 (a group containing only one object). The information gain is about 0.11. If we calculate information gain for each partition in that manner, we can select the thresholds for comparison of each numeric feature before the construction of a large tree (using all features).", "More examples of numeric feature discretization can be found in posts like ", " or ", ". One of the most prominent scientific papers on this subject is \u201cOn the handling of continuous-valued attributes in decision tree generation\u201d (UM Fayyad. KB Irani, \u201cMachine Learning\u201d, 1992).", "Technically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be ", ", or too tuned to the training set, and will not predict labels for new data well. At the bottom of the tree, at some great depth, there will be partitions on less important features (e.g. whether a client came from Leeds or New York). We can exaggerate this story further and find that all four clients who came to the bank for a loan in green trousers did not return the loan. Even if that were true in training, we do not want our classification model to generate such specific rules.", "There are two exceptions where the trees are built to the maximum depth:", "The picture below is an example of a dividing border built in an overfitted tree.", "The most common ways to deal with overfitting in decision trees are as follows:", "The main parameters of the ", " class are:", "The parameters of the tree need to be set depending on input data, and it is usually done by means of ", ", more on this below.", "When predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes.", "where ", " is the number of samples in a leaf, ", "is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.", "Let\u2019s generate some data distributed by the function", "with some noise. Then we will train a tree on it and show what predictions it makes.", "We see that the decision tree approximates the data with a piecewise constant function.", " (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.", "According to the nearest neighbors method, the green ball would be classified as \u201cblue\u201d rather than \u201cred\u201d.", "For another example, if you do not know how to tag a Bluetooth-headset on an online listing, you can find 5 similar headsets, and, if 4 of them are tagged as \u201caccessories\u201d and only 1 as \u201cTechnology\u201d, then you will also label it under \u201caccessories\u201d.", "To classify each sample from the test set, one needs to perform the following operations in order:", "The method adapts quite easily for the regression problem: on step 3, it returns not the class, but the number \u2014 a mean (or median) of the target variable among neighbors.", "A notable feature of this approach is its laziness \u2014 calculations are only done during the prediction phase, when a test sample needs to be classified. No model is constructed from the training examples beforehand. In contrast, recall that for decision trees in the first half of this article the tree is constructed based on the training set, and the classification of test cases occurs relatively quickly by traversing through the tree.", "Nearest neighbors is a well-studied approach. There exist many important theorems claiming that, on \u201cendless\u201d datasets, it is the optimal method of classification. The authors of the classic book \u201cThe Elements of Statistical Learning\u201d consider k-NN to be a theoretically ideal algorithm which usage is only limited by computation power and the ", ".", "The quality of classification/regression with k-NN depends on several parameters:", "The main parameters of the class ", " are:", "The main task of learning algorithms is to be able to ", " to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.", "This is often done in one of two ways:", "In k-fold cross-validation, the model is trained ", " times on different (", ") subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange). We obtain ", " model quality assessments that are usually averaged to give an overall average quality of classification/regression.", "Cross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.", "Cross-validation is a very important technique in machine learning and can also be applied in statistics and econometrics. It helps with hyperparameter tuning, model comparison, feature evaluation, etc. More details can be found ", " (blog post by Sebastian Raschka) or in any classic textbook on machine (statistical) learning.", "Let\u2019s read data into a ", " and preprocess it. Store ", " in a separate ", " object for now and remove it from the dataframe. We will train the first model without the ", " feature, and then we will see if it helps.", "Let\u2019s allocate 70% of the set for training (", ", ", ") and 30% for the hold-out set (", ", ", "). The hold-out set will not be involved in tuning the parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: a decision tree and k-NN. We do not know what parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10.", "Let\u2019s assess prediction quality on our hold-out set with a simple metric \u2014 the proportion of correct answers (accuracy). The decision tree did better \u2014 percentage of correct answers is about 94% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using random parameters.", "Now, let\u2019s identify the parameters for the tree using cross-validation. We\u2019ll tune the maximum depth and the maximum number of features used at each split. Here is the essence of how the GridSearchCV works: for each unique pair of values of ", " and ", ", compute model performance with 5-fold cross-validation, and then select the best combination of parameters.", "Let\u2019s list the best parameters and the corresponding mean accuracy from cross-validation.", "Let\u2019s draw the resulting tree. Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can \u201cwalk\u201d over the tree if you locally open the corresponding picture downloaded from the course repo.", "Now, let\u2019s tune the number of neighbors ", " for k-NN:", "Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%/94.6% accuracy for cross-validation and hold-out respectively. Decision trees perform very well, and even random forest (let\u2019s think of it for now as a bunch of trees that work better together) in this example cannot achieve better performance (95.1%/95.3%) despite being trained for much longer.", "To continue the discussion of the pros and cons of the methods in question, let\u2019s consider a simple classification task, where a tree would perform well but does it in an \u201coverly complicated\u201d manner. Let\u2019s create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line.", "However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the 30 x 30 squares that frame the training set.", "We got this overly complex construction, although the solution is just a straight line ", ".", "The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our ", ").", "Now let\u2019s have a look at how these 2 algorithms perform on a real-world task. We will use the ", " built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.", "Pictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is \u200b\u200b\u201dunfolded\u201d into a vector of length 64, and we obtain a feature description of an object.", "Let\u2019s draw some handwritten digits. We see that they are distinguishable.", "Next, let\u2019s do the same experiment as in the previous task, but, this time, let\u2019s change the ranges for tunable parameters.", "Let\u2019s select 70% of the dataset for training (", ", ", ") and 30% for holdout (", ", ", "). The holdout set will not participate in model parameters tuning; we will use it at the end to check the quality of the resulting model.", "Let\u2019s train a decision tree and k-NN with our random parameters and make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random parameters.", "Now let\u2019s tune our model parameters using cross-validation as before, but now we\u2019ll take into account that we have more features than in the previous task: 64.", "Let\u2019s see the best parameters combination and the corresponding accuracy from cross-validation:", "That has already passed 66% but not quite 97%. k-NN works better on this dataset. In the case of one nearest neighbor, we were able to reach 99% guesses on cross-validation.", "Let\u2019s train a random forest on the same dataset, it works better than k-NN on the majority of datasets. But we here have an exception.", "You would be right to point out that we have not tuned any ", " parameters here. Even with tuning, the training accuracy doesn\u2019t reach 98% as it did with one nearest neighbor.", "The ", " of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors (next time we will also add logistic regression to this list). It might be the case that these methods already work well enough.", "Let\u2019s consider another simple example. In the classification problem, one of the features will just be proportional to the vector of responses, but this won\u2019t help for the nearest neighbors method.", "As always, we will look at the accuracy for cross-validation and the hold-out set. Let\u2019s construct curves reflecting the dependence of these quantities on the ", " parameter in the method of nearest neighbors. These curves are called validation curves.", "One can see that k-NN with the Euclidean distance does not work well on the problem, even when you vary the number of nearest neighbors over a wide range.", "In contrast, the decision tree easily \u201cdetects\u201d hidden dependencies in the data despite a restriction on the maximum depth.", "In the second example, the tree solved the problem perfectly while k-NN experienced difficulties. However, this is more of a disadvantage of using Euclidian distance than of the method. It did not allow us to reveal that one feature was much better than the others.", "Pros:", "Cons:", "Pros:", "Cons:", "This is a lot of information, but, hopefully, this article will be a great reference for you for a long time :)", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T08:59:27.200Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Arseny Kravchenko", "articleTile": "Open Machine Learning Course. Topic 6. Feature Engineering and Feature Selection", "content": ["In this course, we have already seen several key machine learning algorithms. However, before moving on to the more fancy ones, we\u2019d like to take a small detour and talk about data preparation. The well-known concept of \u201cgarbage in \u2014 garbage out\u201d applies 100% to any task in machine learning. Any experienced professional can recall numerous times when a simple model trained on high-quality data was proven to be better than a complicated multi-model ensemble built on data that wasn\u2019t clean.", "To start, I wanted to review three similar but different tasks:", "This article will contain almost no math, but there will be a fair amount of code. (", " version). Some examples will use the dataset from Renthop company, which is used in the ", ". In this task, you need to predict the popularity of a new rental listing, i.e. classify the listing into three classes: ", ". To evaluate the solutions, we will use the log loss metric (the smaller, the better). Those who do not have a Kaggle account, will have to register; you will also need to accept the rules of the competition in order to download the data.", "1. Feature Extraction", "1.1. Texts", "1.2. Images", "1.3. Geospatial data", "1.4. Date and time", "1.5. Time series, web, etc.", "2. Feature transformations", "2.1. Normalization and changing distribution", "2.2. Interactions", "2.3. Filling in the missing values", "3. Feature selection", "3.1. Statistical approaches", "3.2. Selection by modeling", "3.3. Grid search", "In practice, data rarely comes in the form of ready-to-use matrices. That's why every task begins with feature extraction. Sometimes, it can be enough to read the csv file and convert it into ", ", but this is a rare exception. Let's look at some of the popular types of data from which features can be extracted.", "Text is a type of data that can come in different formats; there are so many text processing methods that cannot fit in a single article. Nevertheless, we will review the most popular ones.", "Before working with text, one must tokenize it. Tokenization implies splitting the text into units (hence, tokens). Most simply, tokens are just the words. But splitting by word can lose some of the meaning -- \"Santa Barbara\" is one token, not two, but \"rock'n'roll\" should not be split into two tokens. There are ready-to-use tokenizers that take into account peculiarities of the language, but they make mistakes as well, especially when you work with specific sources of text (newspapers, slang, misspellings, typos).", "After tokenization, you will normalize the data. For text, this is about stemming and/or lemmatization; these are similar processes used to process different forms of a word. One can read about the difference between them ", ".", "So, now that we have turned the document into a sequence of words, we can represent it with vectors. The easiest approach is called Bag of Words: we create a vector with the length of the dictionary, compute the number of occurrences of each word in the text, and place that number of occurrences in the appropriate position in the vector. The process described looks simpler in code:", "Expected result:", "Here is an illustration of the process:", "This is an extremely naive implementation. In practice, you need to consider stop words, the maximum length of the dictionary, more efficient data structures (usually text data is converted to a sparse vector), etc.", "When using algorithms like Bag of Words, we lose the order of the words in the text, which means that the texts \"i have no cows\" and \"no, i have cows\" will appear identical after vectorization when, in fact, they have the opposite meaning. To avoid this problem, we can revisit our tokenization step and use N-grams (the ", " of N consecutive tokens) instead.", "Also note that one does not have to use only words. In some cases, it is possible to generate N-grams of characters. This approach would be able to account for similarity of related words or handle typos.", "Adding onto the Bag of Words idea: words that are rarely found in the corpus (in all the documents of this dataset) but are present in this particular document might be more important. Then it makes sense to increase the weight of more domain-specific words to separate them out from common words. This approach is called TF-IDF (term frequency-inverse document frequency), which cannot be written in a few lines, so you should look into the details in references such as ", ". The default option is as follows:", "Analogs of Bag of Words can be found outside of text problems e.g. bag of sites in the ", ", ", ", ", ", etc.", "Using these algorithms, it is possible to obtain a working solution for a simple problem, which can serve as a baseline. However, for those who do not like the classics, there are new approaches. The most popular method in the new wave is Word2Vec, but there are a few alternatives as well (GloVe, Fasttext, etc.).", "Word2Vec is a special case of the word embedding algorithms. Using Word2Vec and similar models, we can not only vectorize words in a high-dimensional space (typically a few hundred dimensions) but also compare their semantic similarity. This is a classic example of operations that can be performed on vectorized concepts: king - man + woman = queen.", "It is worth noting that this model does not comprehend the meaning of the words but simply tries to position the vectors such that words used in common context are close to each other. If this is not taken into account, a lot of fun examples will come up.", "Such models need to be trained on very large datasets in order for the vector coordinates to capture the semantics. A pretrained model for your own tasks can be downloaded ", ".", "Similar methods are applied in other areas such as bioinformatics. An unexpected application is ", ". You can probably think of a few other fresh ideas; the concept is universal enough.", "Working with images is easier and harder at the same time. It is easier because it is possible to just use one of the popular pretrained networks without much thinking but harder because, if you need to dig into the details, you may end up going really deep. Let's start from the beginning.", "In a time when GPUs were weaker and the \"renaissance of neural networks\" had not happened yet, feature generation from images was its own complex field. One had to work at a low level, determining corners, borders of regions, color distributions statistics, and so on. Experienced specialists in computer vision could draw a lot of parallels between older approaches and neural networks; in particular, convolutional layers in today's networks are similar to ", ". If you are interested in reading more, here are a couple of links to some interesting libraries: ", " and ", ".", "Often for problems associated with images, a convolutional neural network is used. You do not have to come up with the architecture and train a network from scratch. Instead, download a pretrained state-of-the-art network with the weights from public sources. Data scientists often do so-called fine-tuning to adapt these networks to their needs by \"detaching\" the last fully connected layers of the network, adding new layers chosen for a specific task, and then training the network on new data. If your task is to just vectorize the image (for example, to use some non-network classifier), you only need to remove the last layers and use the output from the previous layers:", "Nevertheless, we should not focus too much on neural network techniques. Features generated by hand are still very useful: for example, for predicting the popularity of a rental listing, we can assume that bright apartments attract more attention and create a feature such as \"the average value of the pixel\". You can find some inspiring examples in the documentation of ", ".", "If there is text on the image, you can read it without unraveling a complicated neural network. For example, check out ", ".", "One must understand that ", " is not a solution for everything.", "Another case where neural networks cannot help is extracting features from meta-information. For images, EXIF stores many useful meta-information: manufacturer and camera model, resolution, use of the flash, geographic coordinates of shooting, software used to process image and more.", "Geographic data is not so often found in problems, but it is still useful to master the basic techniques for working with it, especially since there are quite a number of ready-to-use solutions in this field.", "Geospatial data is often presented in the form of addresses or coordinates of (Latitude, Longitude). Depending on the task, you may need two mutually-inverse operations: geocoding (recovering a point from an address) and reverse geocoding (recovering an address from a point). Both operations are accessible in practice via external APIs from Google Maps or OpenStreetMap. Different geocoders have their own characteristics, and the quality varies from region to region. Fortunately, there are universal libraries like ", " that act as wrappers for these external services.", "If you have a lot of data, you will quickly reach the limits of external API. Besides, it is not always the fastest to receive information via HTTP. Therefore, it is necessary to consider using a local version of OpenStreetMap.", "If you have a small amount of data, enough time, and no desire to extract fancy features, you can use ", " in lieu of OpenStreetMap:", "When working with geo\u0441oding, we must not forget that addresses may contain typos, which makes the data cleaning step necessary. Coordinates contain fewer misprints, but its position can be incorrect due to GPS noise or bad accuracy in places like tunnels, downtown areas, etc. If the data source is a mobile device, the geolocation may not be determined by GPS but by WiFi networks in the area, which leads to holes in space and teleportation. While traveling along in Manhattan, there can suddenly be a WiFi location from Chicago.", "WiFi location tracking is based on the combination of SSID and MAC-addresses, which may correspond to different points e.g. federal provider standardizes the firmware of routers up to MAC-address and places them in different cities. Even a company's move to another office with its routers can cause issues.", "The point is usually located among infrastructure. Here, you can really unleash your imagination and invent features based on your life experience and domain knowledge: the proximity of a point to the subway, the number of stories in the building, the distance to the nearest store, the number of ATMs around, etc. For any task, you can easily come up with dozens of features and extract them from various external sources. For problems outside an urban environment, you may consider features from more specific sources e.g. the height above sea level.", "If two or more points are interconnected, it may be worthwhile to extract features from the route between them. In that case, distances (great circle distance and road distance calculated by the routing graph), number of turns with the ratio of left to right turns, number of traffic lights, junctions, and bridges will be useful. In one of my own tasks, I generated a feature called \"the complexity of the road\", which computed the graph-calculated distance divided by the GCD.", "You would think that date and time are standardized because of their prevalence, but, nevertheless, some pitfalls remain.", "Let's start with the day of the week, which are easy to turn into 7 dummy variables using one-hot encoding. In addition, we will also create a separate binary feature for the weekend called ", ".", "Some tasks may require additional calendar features. For example, cash withdrawals can be linked to a pay day; the purchase of a metro card, to the beginning of the month. In general, when working with time series data, it is a good idea to have a calendar with public holidays, abnormal weather conditions, and other important events.", "Q: What do Chinese New Year, the New York marathon, and the Trump inauguration have in common?", "A: They all need to be put on the calendar of potential anomalies.", "Dealing with hour (minute, day of the month ...) is not as simple as it seems. If you use the hour as a real variable, we slightly contradict the nature of data: ", " while ", ". For some problems, this can be critical. At the same time, if you encode them as categorical variables, you'll breed a large numbers of features and lose information about proximity -- the difference between 22 and 23 will be the same as the difference between 22 and 7.", "There also exist some more esoteric approaches to such data like projecting the time onto a circle and using the two coordinates.", "This transformation preserves the distance between points, which is important for algorithms that estimate distance (kNN, SVM, k-means ...)", "However, the difference between such coding methods is down to the third decimal place in the metric.", "Regarding time series \u2014 we will not go into too much detail here (mostly due to my personal lack of experience), but I will point you to ", ".", "If you are working with web data, then you usually have information about the user's User Agent. It is a wealth of information. First, one needs to extract the operating system from it. Secondly, make a feature ", ". Third, look at the browser.", "As in other domains, you can come up with your own features based on intuition about the nature of the data. At the time of this writing, Chromium 56 was new, but, after some time, only users who haven't rebooted their browser for a long time will have this version. In this case, why not introduce a feature called \"lag behind the latest version of the browser\"?", "In addition to the operating system and browser, you can look at the referrer (not always available), ", ", and other meta information.", "The next useful piece of information is the IP-address, from which you can extract the country and possibly the city, provider, and connection type (mobile/stationary). You need to understand that there is a variety of proxy and outdated databases, so this feature can contain noise. Network administration gurus may try to extract even fancier features like suggestions for ", ". By the way, the data from the IP-address is well combined with ", ": if the user is sitting at the Chilean proxies and browser locale is ", ", something is unclean and worth a look in the corresponding column in the table (", ").", "Any given area has so many specifics that it is too much for an individual to absorb completely. Therefore, I invite everyone to share their experiences and discuss feature extraction and generation in the comments section.", "Monotonic feature transformation is critical for some algorithms and has no effect on others. This is one of the reasons for the increased popularity of decision trees and all its derivative algorithms (random forest, gradient boosting). Not everyone can or want to tinker with transformations, and these algorithms are robust to unusual distributions.", "There are also purely engineering reasons: ", " is a way of dealing with large numbers that do not fit in ", ". This is an exception rather than a rule; often it's driven by the desire to adapt the dataset to the requirements of the algorithm. Parametric methods usually require a minimum of symmetric and unimodal distribution of data, which is not always given in real data. There may be more stringent requirements; recall ", ".", "However, data requirements are imposed not only by parametric methods; ", " will predict complete nonsense if features are not normalized e.g. when one distribution is located in the vicinity of zero and does not go beyond (-1, 1) while the other\u2019s range is on the order of hundreds of thousands.", "A simple example: suppose that the task is to predict the cost of an apartment from two variables \u2014 the distance from city center and the number of rooms. The number of rooms rarely exceeds 5 whereas the distance from city center can easily be in the thousands of meters.", "The simplest transformation is Standard Scaling (or Z-score normalization):", "Note that Standard Scaling does not make the distribution normal in the strict sense.", "But, to some extent, it protects against outliers:", "Another fairly popular option is MinMax Scaling, which brings all the points within a predetermined interval (typically (0, 1)).", "StandardScaling and MinMax Scaling have similar applications and are often more or less interchangeable. However, if the algorithm involves the calculation of distances between points or vectors, the default choice is StandardScaling. But MinMax Scaling is useful for visualization by bringing features within the interval (0, 255).", "If we assume that some data is not normally distributed but is described by the ", ", it can easily be transformed to a normal distribution:", "The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc. However, to apply this procedure, the underlying distribution does not necessarily have to be lognormal; you can try to apply this transformation to any distribution with a heavy right tail. Furthermore, one can try to use other similar transformations, formulating their own hypotheses on how to approximate the available distribution to a normal. Examples of such transformations are ", " (logarithm is a special case of the Box-Cox transformation) or ", " (extends the range of applicability to negative numbers). In addition, you can also try adding a constant to the feature \u2014 ", ".", "In the examples above, we have worked with synthetic data and strictly tested normality using the Shapiro-Wilk test. Let\u2019s try to look at some real data and test for normality using a less formal method \u2014 ", ". For a normal distribution, it will look like a smooth diagonal line, and visual anomalies should be intuitively understandable.", "Let\u2019s see whether transformations can somehow help the real model. There is no silver bullet here.", "If previous transformations seemed rather math-driven, this part is more about the nature of the data; it can be attributed to both feature transformations and feature creation.", "Let\u2019s come back again to the Two Sigma Connect: Rental Listing Inquiries problem. Among the features in this problem are the number of rooms and the price. Logic suggests that the cost per single room is more indicative than the total cost, so we can generate such a feature.", "You should limit yourself in this process. If there are a limited number of features, it is possible to generate all the possible interactions and then weed out the unnecessary ones using the techniques described in the next section. In addition, not all interactions between features must have a physical meaning; for example, polynomial features (see ", ") are often used in linear models and are almost impossible to interpret.", "Not many algorithms can work with missing values, and the real world often provides data with gaps. Fortunately, this is one of the tasks for which one doesn\u2019t need any creativity. Both key python libraries for data analysis provide easy-to-use solutions: ", " and ", ".", "These solutions do not have any magic happening behind the scenes. Approaches to handling missing values are pretty straightforward:", "Easy-to-use library solutions sometimes suggest sticking to something like ", " and not sweat the gaps. But this is not the best solution: data preparation takes more time than building models, so thoughtless gap-filling may hide a bug in processing and damage the model.", "Why would it even be necessary to select features? To some, this idea may seem counterintuitive, but there are at least two important reasons to get rid of unimportant features. The first is clear to every engineer: the more data, the higher the computational complexity. As long as we work with toy datasets, the size of the data is not a problem, but, for real loaded production systems, hundreds of extra features will be quite tangible. The second reason is that some algorithms take noise (non-informative features) as a signal and overfit.", "The most obvious candidate for removal is a feature whose value remains unchanged, i.e., it contains no information at all. If we build on this thought, it is reasonable to say that features with low variance are worse than those with high variance. So, one can consider cutting features with variance below a certain threshold.", "There are other ways that are also ", ".", "We can see that our selected features have improved the quality of the classifier. Of course, this example is ", " artificial; however, it is worth using for real problems.", "Another approach is to use some baseline model for feature evaluation because the model will clearly show the importance of the features. Two types of models are usually used: some \u201cwooden\u201d composition such as ", " or a linear model with Lasso regularization so that it is prone to nullify weights of weak features. The logic is intuitive: if features are clearly useless in a simple model, there is no need to drag them to a more complex one.", "We must not forget that this is not a silver bullet again \u2014 it can make the performance worse.", "Finally, we get to the most reliable method, which is also the most computationally complex: trivial grid search. Train a model on a subset of features, store results, repeat for different subsets, and compare the quality of models to identify the best feature set. This approach is called ", ".", "Searching all combinations usually takes too long, so you can try to reduce the search space. Fix a small number N, iterate through all combinations of N features, choose the best combination, and then iterate through the combinations of (N + 1) features so that the previous best combination of features is fixed and only a single new feature is considered. It is possible to iterate until we hit a maximum number of characteristics or until the quality of the model ceases to increase significantly. This algorithm is called ", ".", "This algorithm can be reversed: start with the complete feature space and remove features one by one until it does not impair the quality of the model or until the desired number of features is reached.", "Take a look how this approach was done in one ", ".", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ". We don\u2019t have a separate assignment on feature engineering (this is done in Kaggle Inclass competitions) so this demo assignment is on regression due.", "Open Data Science wishes you good luck with the assignment as well as clean data and useful features in your real work!", "Written by"], "postingTime": "2018-09-19T09:07:48.338Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 1. Exploratory Data Analysis with Pandas", "content": ["With this article, we, ", ", launch an open Machine Learning course. This is not aimed at developing another ", " introductory course on machine learning or data analysis (so this is not a substitute for fundamental education or online/offline courses/specializations and books). The purpose of this series of articles is to quickly refresh your knowledge and help you find topics for further advancement. Our approach is similar to that of the authors of ", ", which starts off with a review of mathematics and basics of machine learning \u2014 short, concise, and with many references to other resources.", " YouTube ", " with videolectures", "The course is designed to perfectly balance theory and practice; therefore, each topic is followed by an ", "with a deadline in a week. You can also take part in several Kaggle Inclass ", " held during the course.", "All materials are available as a ", " and in a ", ".", "The course is going to be actively discussed in the OpenDataScience Slack team. Please fill in ", " form to be invited. The next session of the course will start on ", ", 2018. Invitations will be sent in September.", "1. About the course", "2. Assignments", "3. Demonstration of main Pandas methods", "4. First attempt on predicting telecom churn", "5. Assignment #1", "6. Useful resources", "One of the most vivid advantages of our course is active community. If you join the OpenDataScience Slack team, you\u2019ll find the authors of articles and assignments right there in the same channel (#eng_mlcourse_open) eager to help you. This can help very much when you make your first steps in any discipline. Fill in ", " form to be invited. The form will ask you several questions about your background and skills, including a few easy math questions.", "We chat informally, like humor and emoji. Not every MOOC can boast to have such an alive community.", "The prerequisites are the following: basic concepts from calculus, linear algebra, probability theory and statistics, and Python programming skills. If you need to catch up, a good resource will be ", " from the \u201cDeep Learning\u201d book and various math and Python online courses (for Python, CodeAcademy will do). More info is available on the corresponding ", ".", "As for now, you\u2019ll only need ", " (built with Python 3.6) to reproduce the code in the course. Later in the course you\u2019ll have to install other libraries like Xgboost and Vowpal Wabbit.", "You can also resort to the ", " with all necessary software already installed. More info is available on the corresponding ", ".", "Well... There are dozens of cool tutorials on Pandas and visual data analysis. If you are familiar with these topics, just wait for the 3rd article in the series, where we get into machine learning.", " is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like ", ", ", ", or ", ". Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with ", " and ", ", ", " provides a wide range of opportunities for visual analysis of tabular data.", "The main data structures in ", " are implemented with ", " and ", " classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of ", "instances. ", " are great for representing real data: rows correspond to instances (objects, observations, etc.), and columns correspond to features for each of the instances.", "We\u2019ll demonstrate the main methods in action by analyzing a ", " on the churn rate of telecom operator clients. Let\u2019s read the data (using ", "), and take a look at the first 5 lines using the ", " method:", "Recall that each row corresponds to one client, the ", " of our research, and columns are ", " of the object.", "From the output, we can see that the table contains 3333 rows and 20 columns. Now let\u2019s try printing out the column names using ", ":", "We can use the ", " method to output some general information about the dataframe:", ", ", ", ", " and ", " are the data types of our features. We see that one feature is logical (", "), 3 features are of type ", ", and 16 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 3333 observations, the same number of rows we saw before with ", ".", "We can ", " with the ", " method. Let\u2019s apply this method to the ", " feature to convert it into ", ":", "The ", " method shows basic statistical characteristics of each numerical feature (", " and ", " types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles.", "In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the ", " parameter.", "For categorical (type ", ") and boolean (type ", ") features we can use the ", " method. Let\u2019s have a look at the distribution of ", ":", "2850 users out of 3333 are loyal; their ", " value is ", ". To calculate the proportion, pass ", " to the ", " function", "A DataFrame can be sorted by the value of one of the variables (i.e columns). For example, we can sort by Total day charge (use ", " to sort in descending order):", "Alternatively, we can also sort by multiple columns:", "DataFrame can be indexed in different ways.", "To get a single column, you can use a ", " construction. Let's use this to answer a question about that column alone: ", "14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt.", " with one column is also very convenient. The syntax is ", ", where ", " is some logical condition that is checked for each element of the ", " column. The result of such indexing is the DataFrame consisting only of rows that satisfy the ", " condition on the ", " column.", "Let\u2019s use it to answer the question:", "DataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The ", " method is used for ", ", while ", " is used for ", ".", "In the first case, we would say ", ", and, in the second case, we would say ", ".", "If we need the first or last line of the data frame, we use the ", " or ", " syntax.", "The ", " method can also be used to apply a function to each line. To do this, specify ", ". Lambda functions are very convenient in such scenarios. For example, if we need to select all states starting with W, we can do it like this:", "The ", " method can be used to ", " by passing a dictionary of the form ", " as its argument:", "Same thing can be done with the ", " method:", "In general, grouping data in Pandas goes as follows:", "Here is an example where we group the data according to the values of the ", " variable and display statistics of three columns in each group:", "Let\u2019s do the same thing, but slightly differently by passing a list of functions to ", ":", "Suppose we want to see how the observations in our sample are distributed in the context of two variables \u2014 ", " and ", ". To do so, we can build a ", " using the ", " method:", "We can see that most of the users are loyal and do not use additional services (International Plan/Voice mail).", "This will resemble ", " to those familiar with Excel. And, of course, pivot tables are implemented in Pandas: the ", " method takes the following parameters:", "Let\u2019s take a look at the average numbers of day, evening and night calls by area code:", "Like many other things in Pandas, adding columns to a DataFrame is doable in several ways.", "For example, if we want to calculate the total number of calls for all users, let\u2019s create the ", " Series and paste it into the DataFrame:", "It is possible to add a column more easily without creating an intermediate Series instance:", "To delete columns or rows, use the ", " method, passing the required indexes and the ", " parameter (", " if you delete columns, and nothing or ", " if you delete rows). The ", " argument tells whether to change the original DataFrame. With ", ", the ", " method doesn't change the existing DataFrame and returns a new one with dropped rows or columns. With ", ", it alters the DataFrame.", "Let\u2019s see how churn rate is related to the ", " variable. We\u2019ll do this using a ", " contingency table and also through visual analysis with ", " (however, visual analysis will be covered more thoroughly in the next article).", "We see that, with ", ", the churn rate is much higher, which is an interesting observation! Perhaps large and poorly controlled expenses with international calls are very conflict-prone and lead to dissatisfaction among the telecom operator\u2019s customers.", "Next, let\u2019s look at another important feature \u2014 ", ". Let\u2019s also make a summary table and a picture.", "Perhaps, it is not so obvious from the summary table, but the picture clearly states that the churn rate strongly increases starting from 4 calls to the service center.", "Let\u2019s now add a binary attribute to our DataFrame \u2014 ", ". And once again, let's see how it relates to the churn.", "Let\u2019s construct another contingency table that relates ", " with both ", " and freshly created ", ".", "Therefore, predicting that a customer will churn (", "=1) in the case when the number of calls to the service center is greater than 3 and the ", " is added (and predicting ", "=0 otherwise), we might expect an accuracy of 85.8% (we are mistaken only 464 + 9 times). This number, 85.8%, that we got with very simple reasoning serves as a good starting point (", ") for the further machine learning models that we will build.", "As we move on in this course, recall that, before the advent of machine learning, the data analysis process looked something like this. Let\u2019s recap what we\u2019ve covered:", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-10-07T12:47:36.075Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Egor Polusmak", "articleTile": "Open Machine Learning Course. Topic 9. Part 2. Predicting the future with Facebook Prophet", "content": ["Time series forecasting finds wide application in data analytics. These are only some of the conceivable predictions of future trends that might be useful:", "For another example, we can make a prediction of some team\u2019s performance and then use it as a baseline: first to set goals for the team, and then to measure the actual team performance relative to the baseline.", "There are quite a few different methods to predict future trends, for example, ", ", ", ", ", ", ", ".", "In this article, we will look at ", ", a library for time series forecasting released by Facebook and open-sourced on February 23, 2017. We will also try it out in the problem of predicting the daily number of posts published on Medium.", "1. Introduction", "2. The Prophet Forecasting Model", "3. Practice with Prophet", "4. Box-Cox Transformation", "5. Summary", "6. References", "According to the ", " by Facebook Research, Prophet was initially developed for the purpose of creating high quality business forecasts. This library tries to address the following difficulties common to many business time series:", "The authors claim that, even with default settings, in many cases, their library produces forecasts as accurate as those delivered by experienced analysts.", "Moreover, Prophet has a number of intuitive and easily interpretable customizations that allow gradually improving the quality of the forecasting model. What is especially important, these parameters are quite comprehensible even for non-experts in time series analysis, which is a field of data science requiring certain skill and experience.", "By the way, the original article is called \u201cForecasting at Scale\u201d, but it is not about the scale in the \u201cusual\u201d sense, that it\u2019s addressing computational and infrastructure problems of a large number of working programs. According to the authors, Prophet should scale well in the following 3 areas:", "Now, let\u2019s take a closer look at how Prophet works. In its essence, this library utilizes the ", " ", " comprising the following components:", "where:", "Below, we will consider some important properties of these model components.", "The Prophet library implements two possible trend models for ", ".", "The first one is called ", ". It is represented in the form of the ", ":", "where:", "This logistic equation allows modelling non-linear growth with saturation, that is when the growth rate of a value decreases with its growth. One of the typical examples would be representing the growth of the audience of an application or a website.", "Actually, CC and kk are not necessarily constants and may vary over time. Prophet supports both automatic and manual tuning of their variability. The library can itself choose optimal points of trend changes by fitting the supplied historical data.", "Also, Prophet allows analysts to manually set changepoints of the growth rate and capacity values at different points in time. For instance, analysts may have insights about dates of past releases that prominently influenced some key product indicators.", "The second trend model is a simple ", " with a constant rate of growth. It is best suited for problems without saturating growth.", "The seasonal component ", " provides a flexible model of periodic changes due to weekly and yearly seasonality.", "Weekly seasonal data is modeled with dummy variables. Six new variables are added: ", ", ", ", ", ", ", ", ", ", ", ", which take values 0 or 1 depending on the day of the week. The feature ", " is not added because it would be a linear combination of the other days of the week, and this fact would have an adverse effect on the model.", "Yearly seasonality model in Prophet relies on Fourier series.", "Since ", " you can also use ", " and make ", " as well as employ the new ", " feature.", "The component ", " represents predictable abnormal days of the year including those on irregular schedules, e.g., Black Fridays.", "To utilize this feature, the analyst needs to provide a custom list of events.", "The error term ", " represents information that was not reflected in the model. Usually it is modeled as normally distributed noise.", "For a detailed description of the model and algorithms behind Prophet refer to the paper ", " by Sean J. Taylor and Benjamin Letham.", "The authors also compared their library with several other methods for time series forecasting. They used ", " as a measure of prediction accuracy. In this research, Prophet has shown substantially lower forecasting error than the other models.", "Let\u2019s look closer at how the forcasting quality was measured in the article. To do this, we will need the formula of Mean Absolute Percentage Error.", "Let ", " be the ", " and ", " be the ", " given by our model.", "Then ", " is the ", " and pi=ei/yi is the ", ".", "We define", "MAPE is widely used as a measure of prediction accuracy because it expresses error as a percentage and thus can be used in model evaluations on different datasets.", "In addition, when evaluating a forecasting algorithm, it may prove useful to calculate ", " in order to have a picture of errors in absolute numbers. Using previously defined components, its equation will be", "A few words about the algorithms that Prophet was compared with. Most of them are quite simple and often are used as a baseline for other models:", "First, you need to install the library. Prophet is available for Python and R. The choice will depend on your personal preferences and project requirements. Further in this article we will use Python.", "In Python you can install Prophet using PyPI:", "In R you can find the corresponing CRAN package. Refer to the ", " for details.", "Let\u2019s import the modules that we will need, and initialize our environment:", "We will predict the daily number of posts published on ", ".", "First, we load our ", ":", "Next, we leave out all columns except ", " and ", ". The former corresonds to the time dimension while the latter uniquely identifies a post by its URL. Along the way we get rid of possible duplicates and missing values in the data:", "Next, we need to convert ", " to the datetime format because by default ", " treats this field as string-valued.", "Let\u2019s sort the dataframe by time and take a look at what we\u2019ve got:", "Medium\u2019s public release date was August 15, 2012. But, as you can see from the data above, there are at least several rows with much earlier publication dates. They have somehow turned up in our dataset, but they are hardly legitimate ones. We will just trim our time series to keep only those rows that fall onto the period from August 15, 2012 to June 25, 2017:", "As we are going to predict the number of published posts, we will aggregate and count unique posts at each given point in time. We will name the corresponding new column ", ":", "In this practice, we are interested in the number of posts ", ". But at this moment all our data is divided into irregular time intervals that are less than a day. This is called a ", ". To see it, let\u2019s print out the first 3 rows:", "To fix this, we need to aggregate the post counts by \u201cbins\u201d of a date size. In time series analysis, this process is referred to as ", ". And if we ", " the sampling rate of data it is often called ", ".", "Luckily, ", " has a built-in functionality for this task. We will resample our time index down to 1-day bins:", "As always, it may be helpful and instructive to look at a graphical representation of your data.", "We will create a time series plot for the whole time range. Displaying data over such a long period of time can give clues about seasonality and conspicuous abnormal deviations.", "First, we import and initialize the ", " library, which allows creating beautiful interactive plots:", "We also define a helper function, which will plot our dataframes throughout the article:", "Let\u2019s try and plot our dataset ", ":", "High-frequency data can be rather difficult to analyze. Even with the ability to zoom in provided by ", ", it is hard to infer anything meaningful from this chart apart from the prominent upward and accelerating trend.", "To reduce the noise, we will resample the post counts down to weekly bins. Besides ", ", other possible techniques of noise reduction include ", " and ", ", among others.", "We save our downsampled dataframe in a separate variable because further in this practice we will work only with daily series:", "Finally, we plot the result:", "This downsampled chart proves to be somewhat better for an analyst\u2019s perception.", "One of the most useful functions that ", " provides is the ability to quickly dive into different periods of timeline in order to better understand the data and find visual clues about possbile trends, periodic and irregular effects.", "For example, zooming-in on a couple of consecutive years shows us time points corresponding to Christmas holidays, which greatly influence human behaviors.", "Now, we\u2019re going to omit the first few years of observations, up to 2015. First, they won\u2019t contribute much into the forecast quality in 2017. Second, these first years, having very low number of posts per day, are likely to increase noise in our predictions, as the model would be forced to fit this abnormal historical data along with more relevant and indicative data from the recent years.", "To sum up, from visual analysis we can see that our dataset is non-stationary with a prominent growing trend. It also demonstrates weekly and yearly seasonality and a number of abnormal days in each year.", "Prophet\u2019s API is very similar to the one you can find in ", ". First we create a model, then call the method ", ", and, finally, make a forecast. The input to the method ", " is a ", " with two columns:", "To get started, we\u2019ll import the library and mute unimportant diagnostic messages:", "Let\u2019s convert our dataframe to the format required by Prophet:", "The authors of the library generally advise to make predictions based on at least several months, ideally, more than a year of historical data. Luckily, in our case we have more than a couple of years of data to fit the model.", "To measure the quality of our forecast, we need to split our dataset into the ", ", which is the first and biggest slice of our data, and the ", ", which will be located at the end of the timeline. We will remove the last month from the dataset in order to use it later as a prediction target:", "Now we need to create a new ", " object. Here we can pass the parameters of the model into the constructor. But in this article we will use the defaults. Then we train our model by invoking its ", " method on our training dataset:", "Using the helper method ", ", we create a dataframe which will contain all dates from the history and also extend into the future for those 30 days that we left out before.", "We predict values with ", " by passing in the dates for which we want to create a forecast. If we also supply the historical dates (as in our case), then in addition to the prediction we will get an in-sample fit for the history. Let's call the model's ", " method with our ", " dataframe as an input:", "In the resulting dataframe you can see many columns characterizing the prediction, including trend and seasonality components as well as their confidence intervals. The forecast itself is stored in the ", " column.", "The Prophet library has its own built-in tools for visualization that enable us to quickly evaluate the result.", "First, there is a method called ", " that plots all the points from the forecast:", "This chart doesn\u2019t look very informative. The only definitive conclusion that we can draw here is that the model treated many of the data points as outliers.", "The second function ", " might be much more useful in our case. It allows us to observe different components of the model separately: trend, yearly and weekly seasonality. In addition, if you supply information about holidays and events to your model, they will also be shown in this plot.", "Let\u2019s try it out:", "As you can see from the trend graph, Prophet did a good job by fitting the accelerated growth of new posts at the end of 2016. The graph of weekly seasonality leads to the conclusion that usually there are less new posts on Saturdays and Sundays than on the other days of the week. In the yearly seasonality graph there is a prominent dip on Christmas Day.", "Let\u2019s evaluate the quality of the algorithm by calculating the error metrics for the last 30 days that we predicted. For this, we will need the observations ", " and the corresponding predicted values", ".", "Let\u2019s look into the object ", " that the library created for us:", "We can see that this dataframe contains all the information we need except for the historical values. We need to join the ", " object with the actual values ", " from the original dataset ", ". For this we will define a helper function that we will reuse later:", "Let\u2019s apply this function to our last forecast:", "We are also going to define a helper function that we will use to gauge the quality of our forecasting with MAPE and MAE error measures:", "Let\u2019s use our function:", "As a result, the relative error of our forecast (MAPE) is about 22.72%, and on average our model is wrong by 70.45 posts (MAE).", "Let\u2019s create our own visualization of the model built by Prophet. It will comprise the actual values, forecast and confidence intervals.", "First, we will plot the data for a shorter period of time to make the data points easier to distinguish. Second, we will show the model performance only for the period that we predicted, that is the last 30 days. It seems that these two measures should give us a more legible plot.", "Third, we will use ", " to make our chart interactive, which is great for exploring.", "We will define a custom helper function ", " and call it (for more on how it works please refer to the comments in the code and the ", "):", "At first glance, the prediction of the mean values by our model seems to be sensible. The high value of MAPE that we got above may be explained by the fact that the model failed to catch on to increasing peak-to-peak amplitude of weakly seasonality.", "Also, we can conclude from the graph above that many of the actual values lie outside the confidence interval. Prophet may not be suitable for time series with unstable variance, at least when the default settings are used. We will try to fix this by applying a transform to our data.", "So far we have used Prophet with the default settings and the original data. We will leave the parameters of the model alone. But despite this we still have some room for improvement. In this section, we will apply the ", " to our original series. Let\u2019s see where it will lead us.", "A few words about this transformation. This is a monotonic data transformation that can be used to stabilize variance. We will use the one-parameter Box\u2013Cox transformation, which is defined by the following expression:", "We will need to implement the inverse of this function in order to be able to restore the original data scale. It is easy to see that the inverse is defined as:", "The corresponding function in Python is implemented as follows:", "First, we prepare our dataset by setting its index:", "Then, we apply the function ", " from ", ", which applies the Box\u2013Cox transformation. In our case it will return two values. The first one is the transformed series and the second one is the found value of ", " that is optimal in terms of the maximum log-likelihood:", "We create a new ", " model and repeat the fit-predict cycle that we have already done above:", "At this point, we need to revert the Box\u2013Cox transformation with our inverse function and the known value of ", ":", "Here we will reuse our tools for making the comparison dataframe and calculating the errors:", "So, we can definitely state an increase in the quality of the model.", "Finally, let\u2019s plot our previous performance with the latest results side-by-side. Note that we use ", " for the third parameter in order to zoom in on the interval being predicted:", "We see that the forecast of weekly changes in the second graph is much closer to the real values now.", "We have taken a look at ", ", an open-source forecasting library that is specifically targeted at business time series. We have also done some hands-on practice in time series prediction.", "As we have seen, the Prophet library does not make wonders, and its predictions out-of-box are not ", ". It is still up to the data scientist to explore the forecast results, tune model parameters and transform data when necessary.", "However, this library is user-friendly and easily customizable. The sole ability to take into account abnormal days that are known to the analyst beforehand might make a difference in some cases.", "All in all, the Prophet library is worth being a part of your analytical toolbox.", "Written by"], "postingTime": "2018-09-19T09:16:53.360Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors", "content": ["Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!", "3. Nearest Neighbors Method", "4. Choosing Model Parameters and Cross-Validation", "5. Application Examples and Complex Cases", "6. Pros and Cons of Decision Trees and the Nearest Neighbors Method", "7. Assignment #3", "8. Useful resources", "Before we dive into the material for this week\u2019s article, let\u2019s talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell\u2019s book ", " (1997) gives a classic, general definition of machine learning as follows:", "A computer program is said to learn from experience ", " with respect to some class of tasks ", " and performance measure ", ", if its performance at tasks in ", ", as measured by ", ", improves with experience ", ".", "In the various problem settings ", ", ", ", and ", " can refer to completely different things. Some of the most popular ", " are the following:", "A good overview is provided in the \u201cMachine Learning basics\u201d chapter of ", " (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).", " refers to data (we can\u2019t go anywhere without it). Machine learning algorithms can be divided into those that are trained in ", " or ", " manner. In unsupervised learning tasks, one has a ", " consisting of ", " described by a set of ", ". In supervised learning problems, there\u2019s also a ", ", which is what we would like to be able to predict, known for each instance in a ", ".", "Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience ", " is the available training data: a set of ", " (clients), a collection of ", " (such as age, salary, type of loan, past loan defaults, etc.) for each, and a ", " (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting ", " the loan payment is overdue, this would become a regression problem.", "Finally, the third term used in the definition of machine learning is a ", " Such metrics differ for various problems and algorithms, and we\u2019ll discuss them as we study new algorithms. For now, we\u2019ll refer to a simple metric for classification algorithms, the proportion of correct answers \u2014 ", " \u2014 on the test set.", "Let\u2019s take a look at two supervised learning problems: classification and regression.", "We begin our overview of classification and regression methods with one of the most popular ones \u2014 a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.", "In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the \u201cHigher School of Economics and the Media\u201d) based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.", "A decision tree is often a generalization of the experts\u2019 experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree.", "In our next case, we solve a binary classification problem (approve/deny a loan) on the grounds of \u201cAge\u201d, \u201cHome-ownership\u201d, \u201cIncome\u201d and \u201cEducation\u201d.", "The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form \u201cfeature ", " value is less than ", "and feature ", " value is less than ", " \u2026 => Category 1\u201d into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.", "As we\u2019ll see later, many other models, although more accurate, do not have this property and can be regarded as more of a \u201cblack box\u201d approach, where it is harder to interpret how the input data was transformed into the output. Due to this \u201cunderstandability\u201d and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (\u201cTop 10 Algorithms in Data Mining\u201d, Knowledge and Information Systems, 2008. ", ").", "Earlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let\u2019s discuss a simple example where all the variables are binary.", "Recall the game of \u201c20 Questions\u201d, which is often referenced when introducing decision trees. You\u2019ve probably played this game \u2014 one person thinks of a celebrity while the other tries to guess by asking only \u201cYes\u201d or \u201cNo\u201d questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking \u201cIs it Angelina Jolie?\u201d would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking \u201cIs the celebrity a woman?\u201d would reduce the possibilities to roughly half. That is to say, the \u201cgender\u201d feature separates the celebrity dataset much better than other features like \u201cAngelina Jolie\u201d, \u201cSpanish\u201d, or \u201cloves football.\u201d This reasoning corresponds to the concept of information gain based on entropy.", "Shannon\u2019s entropy is defined for a system with N possible states as follows:", "where ", "is the probability of finding the system in the ", "-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize \u201ceffective data splitting\u201d, which we alluded to in the context of \u201c20 Questions\u201d.", "To illustrate how entropy can help us identify good features for building a decision tree, let\u2019s look at a toy example. We will predict the color of the ball based on its position.", "There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability p1 = 9/20 and yellow with probability p2 = 11/20, which gives us an entropy S0 = -9/20 log2(9/20) - 11/20 log2(11/20) \u2248 1. This value by itself may not tell us much, but let\u2019s see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12.", "The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is S1 = -5/13 log2(5/13) - 8/13 log2(8/13) \u2248 0.96. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is S2 = -1/7 log2(1/7) - 6/7 log2(6/7) \u2248 0.6. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable ", "(in this example it\u2019s a variable \u201c", "\u201d) is defined as", "where ", "is the number of groups after the split, ", " is number of objects from the sample in which variable ", " is equal to the ", "-th value. In our example, our split yielded two groups (q = 2), one with 13 elements (N1 = 13), the other with 7 (N2 = 7). Therefore, we can compute the information gain as", "It turns out that dividing the balls into two groups by splitting on \u201ccoordinate is less than or equal to 12\u201d gave us a more ordered system. Let\u2019s continue to divide them into groups until the balls in each group are all of the same color.", "For the right group, we can easily see that we only need one extra partition using \u201ccoordinate less than or equal to 18\u201d. But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 (log2(1) = 0).", "We have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer \u201cquestions\u201d or splits would be more accurate, even if it does not perfectly fit the training set. We will discuss the problem of overfitting later.", "We can make sure that the tree built in the previous example is optimal: it took only 5 \u201cquestions\u201d (conditioned on the variable ", ") to perfectly fit a decision tree to the training set. Under other split conditions, the resulting tree would be deeper, i.e. take more \u201cquestions\u201d to reach an answer.", "At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for \u201cearly stopping\u201d or \u201ccut-off\u201d to avoid constructing an overfitted tree.", "We discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exist others.", "Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).", "In practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.", "For binary classification, entropy and Gini uncertainty take the following form:", "where (", " is the probability of an object having a label +).", "If we plot these two functions against the argument ", ", we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.", "Let\u2019s consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.", "Let\u2019s plot the data. Informally, the classification problem in this case is to build some \u201cgood\u201d boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or, at least, a straight line or a hyperplane, would work well on new data.", "Let\u2019s try to separate these two classes by training an ", " decision tree. We will use ", " parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.", "And how does the tree itself look? We see that the tree \u201ccuts\u201d the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it.", "In the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, S=1. Then, the first partition of the samples into 2 groups was made by comparing the value of ", " with 1.211 (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.", "Suppose we have a numeric feature \u201cAge\u201d that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as \u201cAge <17\u201d, \u201cAge < 22.87\u201d, and so on. But what if the age range is large? Or what if another quantitative variable, \u201csalary\u201d, can also be \u201ccut\u201d in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.", "Let\u2019s consider an example. Suppose we have the following dataset:", "We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class \u201cswitches\u201d from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for \u201ccutting\u201d a quantitative variable.", "Given this information, why do you think it makes no sense here to consider a feature like \u201cAge <17.5\u201d?", "Let\u2019s consider a more complex example by adding the \u201cSalary\u201d variable (in the thousands of dollars per year).", "If we sort by age, the target class ( \u201cloan default\u201d) switches (from 1 to 0 or vice versa) 5 times. And if we sort by salary, it switches 7 times. How will the tree choose features now? Let\u2019s see.", "We see that the tree partitioned by both salary and age. Moreover, the thresholds for feature comparisons are 43.5 and 22.5 years of age and 95k and 30.5k per year. Again, we see that 95 is the average between 88 and 102; the individual with a salary of 88k proved to be \u201cbad\u201d while the one with 102k was \u201cgood\u201d. The same goes for 30.5k. That is, only a few values for comparisons by age and salary were searched. Why did the tree choose these features? Because they gave better partitioning (according to Gini uncertainty).", ": the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes.", "Furthermore, when there are a lot of numeric features in a dataset, each with a lot of unique values, only the top-N of the thresholds described above are selected, i.e. only use the top-N that give maximum gain. The process is to construct a tree of depth 1, compute the entropy (or Gini uncertainty), and select the best thresholds for comparison.", "To illustrate, if we split by \u201cSalary \u2264 34.5\u201d, the left subgroup will have the entropy of 0 (all clients are \u201cbad\u201d), and the right one will have the entropy of 0.954 (3 \u201cbad\u201d and 5 \u201cgood\u201d, you can check this yourself as it will be part of the assignment). The information gain is roughly 0.3. If we split by \u201cSalary \u2264 95\u201d, the left subgroup will have an entropy of 0.97 (6 \u201cbad\u201d and 4 \u201cgood\u201d), and the right one will have an entropy of 0 (a group containing only one object). The information gain is about 0.11. If we calculate information gain for each partition in that manner, we can select the thresholds for comparison of each numeric feature before the construction of a large tree (using all features).", "More examples of numeric feature discretization can be found in posts like ", " or ", ". One of the most prominent scientific papers on this subject is \u201cOn the handling of continuous-valued attributes in decision tree generation\u201d (UM Fayyad. KB Irani, \u201cMachine Learning\u201d, 1992).", "Technically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be ", ", or too tuned to the training set, and will not predict labels for new data well. At the bottom of the tree, at some great depth, there will be partitions on less important features (e.g. whether a client came from Leeds or New York). We can exaggerate this story further and find that all four clients who came to the bank for a loan in green trousers did not return the loan. Even if that were true in training, we do not want our classification model to generate such specific rules.", "There are two exceptions where the trees are built to the maximum depth:", "The picture below is an example of a dividing border built in an overfitted tree.", "The most common ways to deal with overfitting in decision trees are as follows:", "The main parameters of the ", " class are:", "The parameters of the tree need to be set depending on input data, and it is usually done by means of ", ", more on this below.", "When predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes.", "where ", " is the number of samples in a leaf, ", "is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.", "Let\u2019s generate some data distributed by the function", "with some noise. Then we will train a tree on it and show what predictions it makes.", "We see that the decision tree approximates the data with a piecewise constant function.", " (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.", "According to the nearest neighbors method, the green ball would be classified as \u201cblue\u201d rather than \u201cred\u201d.", "For another example, if you do not know how to tag a Bluetooth-headset on an online listing, you can find 5 similar headsets, and, if 4 of them are tagged as \u201caccessories\u201d and only 1 as \u201cTechnology\u201d, then you will also label it under \u201caccessories\u201d.", "To classify each sample from the test set, one needs to perform the following operations in order:", "The method adapts quite easily for the regression problem: on step 3, it returns not the class, but the number \u2014 a mean (or median) of the target variable among neighbors.", "A notable feature of this approach is its laziness \u2014 calculations are only done during the prediction phase, when a test sample needs to be classified. No model is constructed from the training examples beforehand. In contrast, recall that for decision trees in the first half of this article the tree is constructed based on the training set, and the classification of test cases occurs relatively quickly by traversing through the tree.", "Nearest neighbors is a well-studied approach. There exist many important theorems claiming that, on \u201cendless\u201d datasets, it is the optimal method of classification. The authors of the classic book \u201cThe Elements of Statistical Learning\u201d consider k-NN to be a theoretically ideal algorithm which usage is only limited by computation power and the ", ".", "The quality of classification/regression with k-NN depends on several parameters:", "The main parameters of the class ", " are:", "The main task of learning algorithms is to be able to ", " to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.", "This is often done in one of two ways:", "In k-fold cross-validation, the model is trained ", " times on different (", ") subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange). We obtain ", " model quality assessments that are usually averaged to give an overall average quality of classification/regression.", "Cross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.", "Cross-validation is a very important technique in machine learning and can also be applied in statistics and econometrics. It helps with hyperparameter tuning, model comparison, feature evaluation, etc. More details can be found ", " (blog post by Sebastian Raschka) or in any classic textbook on machine (statistical) learning.", "Let\u2019s read data into a ", " and preprocess it. Store ", " in a separate ", " object for now and remove it from the dataframe. We will train the first model without the ", " feature, and then we will see if it helps.", "Let\u2019s allocate 70% of the set for training (", ", ", ") and 30% for the hold-out set (", ", ", "). The hold-out set will not be involved in tuning the parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: a decision tree and k-NN. We do not know what parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10.", "Let\u2019s assess prediction quality on our hold-out set with a simple metric \u2014 the proportion of correct answers (accuracy). The decision tree did better \u2014 percentage of correct answers is about 94% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using random parameters.", "Now, let\u2019s identify the parameters for the tree using cross-validation. We\u2019ll tune the maximum depth and the maximum number of features used at each split. Here is the essence of how the GridSearchCV works: for each unique pair of values of ", " and ", ", compute model performance with 5-fold cross-validation, and then select the best combination of parameters.", "Let\u2019s list the best parameters and the corresponding mean accuracy from cross-validation.", "Let\u2019s draw the resulting tree. Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can \u201cwalk\u201d over the tree if you locally open the corresponding picture downloaded from the course repo.", "Now, let\u2019s tune the number of neighbors ", " for k-NN:", "Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%/94.6% accuracy for cross-validation and hold-out respectively. Decision trees perform very well, and even random forest (let\u2019s think of it for now as a bunch of trees that work better together) in this example cannot achieve better performance (95.1%/95.3%) despite being trained for much longer.", "To continue the discussion of the pros and cons of the methods in question, let\u2019s consider a simple classification task, where a tree would perform well but does it in an \u201coverly complicated\u201d manner. Let\u2019s create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line.", "However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the 30 x 30 squares that frame the training set.", "We got this overly complex construction, although the solution is just a straight line ", ".", "The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our ", ").", "Now let\u2019s have a look at how these 2 algorithms perform on a real-world task. We will use the ", " built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.", "Pictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is \u200b\u200b\u201dunfolded\u201d into a vector of length 64, and we obtain a feature description of an object.", "Let\u2019s draw some handwritten digits. We see that they are distinguishable.", "Next, let\u2019s do the same experiment as in the previous task, but, this time, let\u2019s change the ranges for tunable parameters.", "Let\u2019s select 70% of the dataset for training (", ", ", ") and 30% for holdout (", ", ", "). The holdout set will not participate in model parameters tuning; we will use it at the end to check the quality of the resulting model.", "Let\u2019s train a decision tree and k-NN with our random parameters and make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random parameters.", "Now let\u2019s tune our model parameters using cross-validation as before, but now we\u2019ll take into account that we have more features than in the previous task: 64.", "Let\u2019s see the best parameters combination and the corresponding accuracy from cross-validation:", "That has already passed 66% but not quite 97%. k-NN works better on this dataset. In the case of one nearest neighbor, we were able to reach 99% guesses on cross-validation.", "Let\u2019s train a random forest on the same dataset, it works better than k-NN on the majority of datasets. But we here have an exception.", "You would be right to point out that we have not tuned any ", " parameters here. Even with tuning, the training accuracy doesn\u2019t reach 98% as it did with one nearest neighbor.", "The ", " of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors (next time we will also add logistic regression to this list). It might be the case that these methods already work well enough.", "Let\u2019s consider another simple example. In the classification problem, one of the features will just be proportional to the vector of responses, but this won\u2019t help for the nearest neighbors method.", "As always, we will look at the accuracy for cross-validation and the hold-out set. Let\u2019s construct curves reflecting the dependence of these quantities on the ", " parameter in the method of nearest neighbors. These curves are called validation curves.", "One can see that k-NN with the Euclidean distance does not work well on the problem, even when you vary the number of nearest neighbors over a wide range.", "In contrast, the decision tree easily \u201cdetects\u201d hidden dependencies in the data despite a restriction on the maximum depth.", "In the second example, the tree solved the problem perfectly while k-NN experienced difficulties. However, this is more of a disadvantage of using Euclidian distance than of the method. It did not allow us to reveal that one feature was much better than the others.", "Pros:", "Cons:", "Pros:", "Cons:", "This is a lot of information, but, hopefully, this article will be a great reference for you for a long time :)", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T08:59:27.200Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Egor Polusmak", "articleTile": "Open Machine Learning Course. Topic 2. Visual Data Analysis with Python", "content": ["In the field of Machine Learning, ", " is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project.", "To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools.", "Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures.", "All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox.", "In this article, we are going to get hands-on experience with visual exploration of data using popular libraries such as ", ", ", " and ", ".", "Before we get to the data, let\u2019s initialize our environment:", "In the first article, we looked at the data on customer churn for a telecom operator. We will load again that dataset into a ", ":", "To get acquainted with our data, let\u2019s look at the first 5 entries using ", ":", "Here is the description of our features:", "The last data column, ", ", is our target variable. It is binary: ", " indicates that that the company eventually lost this customer, and ", " indicates that the customer was retained. Later, we will build models that predict this variable based on the remaining features. This is why we call it a ", ".", " analysis looks at one variable at a time. When we analyze a feature independently, we are usually mostly interested in the ", " and ignore the other variables in the dataset.", "Below, we will consider different statistical types of variables and the corresponding tools for their individual visual analysis.", " take on ordered numerical values. Those values can be ", ", like integers, or ", ", like real numbers, and usually express a count or a measurement.", "The easiest way to take a look at the distribution of a numerical variable is to plot its ", " using the ", "'s method ", ".", "A histogram groups values into ", " of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type of it, most often Gaussian.", "In the above plot, we see that the variable ", " is normally distributed, while ", " is prominently skewed right (its tail is longer on the right).", "There is also another, often clearer, way to grasp the distribution: ", " or, more formally, ", ". They can be considered a ", " version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let\u2019s create density plots for the same two variables:", "It is also possible to plot a distribution of observations with ", "'s ", ". For example, let's look at the distribution of ", ". By default, the plot displays both the histogram with the ", " (KDE) on top.", "The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.", "Another useful type of visualization is a ", ". ", " does a great job here:", "Let\u2019s see how to interpret a box plot. Its components are a ", " (obviously, this is why it is called a ", "), the so-called ", ", and a number of individual points (", ").", "The box by itself illustrates the interquartile spread of the distribution; its length is determined by the 25th (Q1) and 75th (Q3) percentiles. The vertical line inside the box marks the median (50%) of the distribution.", "The whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval (Q1\u22121.5\u22c5IQR, Q3+1.5\u22c5IQR), where IQR=Q3\u2212Q1 is the ", ".", "Outliers that fall out of the range bounded by the whiskers are plotted individually as black points along the central axis.", "We can see that a large number of international calls is quite rare in our data.", "The last type of distribution plots that we will consider is a ", ".", "Look at the figures below. On the left, we see the already familiar box plot. To the right, there is a ", " with the kernel density estimate on both sides.", "The difference between the box and violin plots is that the former illustrates certain statistics concerning ", " in a dataset while the violin plot concentrates more on the smoothed ", ".", "In our case, the violin plot does not contribute any additional information about the data as everything is clear from the box plot alone.", "In addition to graphical tools, in order to get the exact numerical statistics of the distribution, we can use the method ", " of a ", ":", "Its output is mostly self-explanatory. ", ", ", " and ", " are the corresponding ", ".", " take on a fixed number of values. Each of these values assigns an observation to a corresponding group, known as a ", ", which reflects some qualitative property of this example. ", " variables are an important special case of categorical variables when the number of possible values is exactly 2. If the values of a categorical variable are ordered, it is called ", ".", "Let\u2019s check the class balance in our dataset by looking at the distribution of the target variable: the ", ". First, we will get a frequency table, which shows how frequent each value of the categorical variable is. For this, we will use the ", " method:", "By default, the entries in the output are sorted from the most to the least frequently-occurring values.", "In our case, the data is not ", "; that is, our two target classes, loyal and disloyal customers, are not represented equally in the dataset. Only a small part of the clients canceled their subscription to the telecom service. As we will see in the following articles, this fact may imply some restrictions on measuring the classification performance, and, in the future, we may want to additionaly penalize our model errors in predicting the minority \u201cChurn\u201d class.", "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the ", "'s function ", ". There is another function in ", " that is somewhat confusingly called ", " and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.", "Let\u2019s plot the distributions for two categorical variables:", "While the histograms, discussed above, and bar plots may look similar, there are several differences between them:", "The left chart above vividly illustrates the imbalance in our target variable. The bar plot for ", " on the right gives a hint that the majority of customers resolve their problems in maximum 2\u20133 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let\u2019s move on to some more interesting and powerful visual techniques.", " plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.", "We are going to start with the interaction between quantitative variables.", "Let\u2019s look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.", "First, we will use the method ", " on a ", " that calculates the correlation between each pair of features. Then, we pass the resulting ", " to ", " from ", ", which renders a color-coded matrix for the provided values:", "From the colored correlation matrix generated above, we can see that there are 4 variables such as ", " that have been calculated directly from the number of minutes spent on phone calls (", "). These are called ", " variables and can therefore be left out since they do not contribute any additional information. Let\u2019s get rid of them:", "The ", " displays values of two numerical variables as ", " in 2D space. Scatter plots in 3D are also possible.", "Let\u2019s try out the function ", " from the ", " library:", "We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellpise-like shape is aligned with the axes.", "There is a slightly fancier option to create a scatter plot with the ", " library:", "The function ", " plots two histograms that may be useful in some cases. Using the same function, we can also get a smoothed version of our bivariate distribution:", "This is basically a bivariate version of the ", " discussed earlier.", "In some cases, we may want to plot a ", " such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix.", "Sometimes, such visualization may help draw conclusions about data; but, in this case, everything is pretty clear with no surprises.", "In this section, we will make our simple quantitative plots a little more exciting. We will try to gain new insights for churn prediction from the interactions between the numerical and categorical features.", "More specifically, let\u2019s see how the input variables are related to the target variable Churn.", "Previously, you learned about scatter plots. Additionally, their points can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can achieve this with the ", " function seen above, but, let's try a new function called ", " and use the parameter ", " to indicate our categorical feature of interest:", "It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won\u2019t make any definitive conclusions from this chart.", "Now, let\u2019s create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers (", ") and those who left (", ").", "From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables: ", ", ", ", and ", ". Later in this course, we will learn how to determine feature importance in classification using ", " or ", "; there, we will see that the first two features are indeed very important for churn prediction.", "Let\u2019s look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for ", " grouped by the target variable.", "In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more.", ": on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial.", "When we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the ", " library called ", ". For example, let's visualize the interaction between ", " and two categorical variables in the same plot:", "From this, we could conclude that, starting with 4 calls, ", " may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls.", "As we saw earlier in this article, the variable ", " has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a ", ". Now, we are interested in the relationship between this ordinal feature and the target variable ", ".", "Let\u2019s look at the distribution of the number of calls to the customer service, again using a ", ". This time, let\u2019s also pass the parameter ", " that adds a categorical dimension to the plot:", ": the churn rate increases significantly after 4 or more calls to the customer service.", "Now, let\u2019s look at the relationship between ", " and the binary features, ", " and ", ".", ": when ", " is enabled, the churn rate is much higher; the usage of the international plan by the customer is a strong feature. We do not observe the same effect with ", ".", "In addition to using graphical means for categorical analysis, there is a traditional tool from statistics: a ", ", also called a ", ". It represents multivariate frequency distribution of categorical variables in tabular form. In particular, it allows us to see the distribution of one variable conditional on the other by looking along a column or row.", "Let\u2019s try to see how ", " is related to the categorical variable ", " by creating a cross tabulation:", "In the case of ", ", the number of distinct values is rather high: 51. We see that there are only a few data points available for each individual state \u2014 only 3 to 17 customers in each state abandoned the operator. Let\u2019s ignore that for a second and calculate the churn rate for each state, sorting it from high to low:", "At first glance, it seems that the churn rate in ", " and ", " are above 25% and less than 6% for Hawaii and Alaska. However, these conclusions are based on too few examples, and our observation could be a mere property of our particular dataset. We can confirm this with the ", " and ", " correlation hypotheses, but this would be beyond the scope of this article.", "We have been looking at different ", " of our dataset by guessing interesting features and selecting a small number of them at a time for visualization. We have only delt with two to three variables at once and were easily able to observe the structure and relationships in data. But, what if we want to display all the features and still be able to interpret the resulting visualization?", "We could use ", " or create a scatterplot matrix with ", " for the whole dataset to look at all of our features simultaneously. But, when the number of features is high enough, this kind of visual analysis quickly becomes slow and inefficient. Besides, we would still be analyzing our variables in a pairwise fashion, not all at once.", "Most real-world datasets have many features, sometimes, many thousands of them. Each of them can be considered as a dimension in the space of data points. Consequently, more often than not, we deal with high-dimensional datasets, where entire visualization is quite hard.", "To look at a dataset as a whole, we need to decrease the number of dimensions used in visualization without losing much information about data. This task is called ", " and is an example of an ", " problem because we need to derive new, low-dimensional features from the data itself, without any supervised input.", "One of the well-known dimensionality reduction methods is ", " (PCA), which we will study later in this course. Its limitation is that it is a ", " algorithm that implies certain restrictions on the data.", "There are also many non-linear methods, collectively called ", ". One of the best-known of them is ", ".", "Let\u2019s create a ", " representation of the same churn data we have been using.", "The name of the method looks complex and a bit intimidating: ", ". Its math is also impressive (we will not delve into it here, but, if you feel brave, here is the ", " by Laurens van der Maaten and Geoffrey Hinton from ", "). Its basic idea is simple: to find a projection for a high-dimensional feature space onto a plane (or a 3D hyperplane, but it is almost always 2D) such that those points that were far apart in the initial n-dimensional space will end up far apart on the plane. Those that were originally close would remain close to each other.", "Essentially, ", " is a search for a new and less-dimensional data representation that preserves neighborship of examples.", "Now, let\u2019s do some practice. First, we need to import some additional classes:", "We will leave out the ", " and ", " features and convert the values \u201cYes\u201d/\u201dNo\u201d of the binary features into numerical values using ", ":", "We also need to normalize the data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with ", ".", "Now, let\u2019s build a t-SNE representation:", "and plot it:", "Let\u2019s color this t-SNE representation according to the churn (green for loyal customers, and red for those who left).", "We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space.", "To better understand the picture, we can also color it with the remaining binary features: ", " and ", ". The green dots here indicate the objects that are positive for the corresponding binary feature.", "Now it is clear that, for example, many dissatisfied customers who canceled their subscription are crowded together in the most south-west cluster that represents the people with the international plan but no voice mail.", "Finally, let\u2019s note some disadvantages of t-SNE:", "Occasionally, using t-SNE, you can get a really good intuition for the data. The following is a good paper that shows an example of this for handwritten digits: ", ".", "Sometimes t-SNE really helps to understand something, and sometimes you can just draw a Christmas tree toy :-)", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:01:30.447Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Egor Polusmak", "articleTile": "Open Machine Learning Course. Topic 2. Visual Data Analysis with Python", "content": ["In the field of Machine Learning, ", " is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project.", "To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools.", "Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures.", "All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox.", "In this article, we are going to get hands-on experience with visual exploration of data using popular libraries such as ", ", ", " and ", ".", "Before we get to the data, let\u2019s initialize our environment:", "In the first article, we looked at the data on customer churn for a telecom operator. We will load again that dataset into a ", ":", "To get acquainted with our data, let\u2019s look at the first 5 entries using ", ":", "Here is the description of our features:", "The last data column, ", ", is our target variable. It is binary: ", " indicates that that the company eventually lost this customer, and ", " indicates that the customer was retained. Later, we will build models that predict this variable based on the remaining features. This is why we call it a ", ".", " analysis looks at one variable at a time. When we analyze a feature independently, we are usually mostly interested in the ", " and ignore the other variables in the dataset.", "Below, we will consider different statistical types of variables and the corresponding tools for their individual visual analysis.", " take on ordered numerical values. Those values can be ", ", like integers, or ", ", like real numbers, and usually express a count or a measurement.", "The easiest way to take a look at the distribution of a numerical variable is to plot its ", " using the ", "'s method ", ".", "A histogram groups values into ", " of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type of it, most often Gaussian.", "In the above plot, we see that the variable ", " is normally distributed, while ", " is prominently skewed right (its tail is longer on the right).", "There is also another, often clearer, way to grasp the distribution: ", " or, more formally, ", ". They can be considered a ", " version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let\u2019s create density plots for the same two variables:", "It is also possible to plot a distribution of observations with ", "'s ", ". For example, let's look at the distribution of ", ". By default, the plot displays both the histogram with the ", " (KDE) on top.", "The height of the histogram bars here is normed and shows the density rather than the number of examples in each bin.", "Another useful type of visualization is a ", ". ", " does a great job here:", "Let\u2019s see how to interpret a box plot. Its components are a ", " (obviously, this is why it is called a ", "), the so-called ", ", and a number of individual points (", ").", "The box by itself illustrates the interquartile spread of the distribution; its length is determined by the 25th (Q1) and 75th (Q3) percentiles. The vertical line inside the box marks the median (50%) of the distribution.", "The whiskers are the lines extending from the box. They represent the entire scatter of data points, specifically the points that fall within the interval (Q1\u22121.5\u22c5IQR, Q3+1.5\u22c5IQR), where IQR=Q3\u2212Q1 is the ", ".", "Outliers that fall out of the range bounded by the whiskers are plotted individually as black points along the central axis.", "We can see that a large number of international calls is quite rare in our data.", "The last type of distribution plots that we will consider is a ", ".", "Look at the figures below. On the left, we see the already familiar box plot. To the right, there is a ", " with the kernel density estimate on both sides.", "The difference between the box and violin plots is that the former illustrates certain statistics concerning ", " in a dataset while the violin plot concentrates more on the smoothed ", ".", "In our case, the violin plot does not contribute any additional information about the data as everything is clear from the box plot alone.", "In addition to graphical tools, in order to get the exact numerical statistics of the distribution, we can use the method ", " of a ", ":", "Its output is mostly self-explanatory. ", ", ", " and ", " are the corresponding ", ".", " take on a fixed number of values. Each of these values assigns an observation to a corresponding group, known as a ", ", which reflects some qualitative property of this example. ", " variables are an important special case of categorical variables when the number of possible values is exactly 2. If the values of a categorical variable are ordered, it is called ", ".", "Let\u2019s check the class balance in our dataset by looking at the distribution of the target variable: the ", ". First, we will get a frequency table, which shows how frequent each value of the categorical variable is. For this, we will use the ", " method:", "By default, the entries in the output are sorted from the most to the least frequently-occurring values.", "In our case, the data is not ", "; that is, our two target classes, loyal and disloyal customers, are not represented equally in the dataset. Only a small part of the clients canceled their subscription to the telecom service. As we will see in the following articles, this fact may imply some restrictions on measuring the classification performance, and, in the future, we may want to additionaly penalize our model errors in predicting the minority \u201cChurn\u201d class.", "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the ", "'s function ", ". There is another function in ", " that is somewhat confusingly called ", " and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.", "Let\u2019s plot the distributions for two categorical variables:", "While the histograms, discussed above, and bar plots may look similar, there are several differences between them:", "The left chart above vividly illustrates the imbalance in our target variable. The bar plot for ", " on the right gives a hint that the majority of customers resolve their problems in maximum 2\u20133 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let\u2019s move on to some more interesting and powerful visual techniques.", " plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.", "We are going to start with the interaction between quantitative variables.", "Let\u2019s look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.", "First, we will use the method ", " on a ", " that calculates the correlation between each pair of features. Then, we pass the resulting ", " to ", " from ", ", which renders a color-coded matrix for the provided values:", "From the colored correlation matrix generated above, we can see that there are 4 variables such as ", " that have been calculated directly from the number of minutes spent on phone calls (", "). These are called ", " variables and can therefore be left out since they do not contribute any additional information. Let\u2019s get rid of them:", "The ", " displays values of two numerical variables as ", " in 2D space. Scatter plots in 3D are also possible.", "Let\u2019s try out the function ", " from the ", " library:", "We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellpise-like shape is aligned with the axes.", "There is a slightly fancier option to create a scatter plot with the ", " library:", "The function ", " plots two histograms that may be useful in some cases. Using the same function, we can also get a smoothed version of our bivariate distribution:", "This is basically a bivariate version of the ", " discussed earlier.", "In some cases, we may want to plot a ", " such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix.", "Sometimes, such visualization may help draw conclusions about data; but, in this case, everything is pretty clear with no surprises.", "In this section, we will make our simple quantitative plots a little more exciting. We will try to gain new insights for churn prediction from the interactions between the numerical and categorical features.", "More specifically, let\u2019s see how the input variables are related to the target variable Churn.", "Previously, you learned about scatter plots. Additionally, their points can be color or size coded so that the values of a third categorical variable are also presented in the same figure. We can achieve this with the ", " function seen above, but, let's try a new function called ", " and use the parameter ", " to indicate our categorical feature of interest:", "It seems that our small proportion of disloyal customers lean towards the top-right corner; that is, such customers tend to spend more time on the phone during both day and night. But this is not absolutely clear, and we won\u2019t make any definitive conclusions from this chart.", "Now, let\u2019s create box plots to visualize the distribution statistics of the numerical variables in two disjoint groups: the loyal customers (", ") and those who left (", ").", "From this chart, we can see that the greatest discrepancy in distribution between the two groups is for three variables: ", ", ", ", and ", ". Later in this course, we will learn how to determine feature importance in classification using ", " or ", "; there, we will see that the first two features are indeed very important for churn prediction.", "Let\u2019s look at the distribution of day minutes spoken for the loyal and disloyal customers separately. We will create box and violin plots for ", " grouped by the target variable.", "In this case, the violin plot does not contribute any additional information about our data as everything is clear from the box plot alone: disloyal customers tend to talk on the phone more.", ": on average, customers that discontinue their contracts are more active users of communication services. Perhaps they are unhappy with the tariffs, so a possible measure to prevent churn could be a reduction in call rates. The company will need to undertake additional economic analysis to find out whether such measures would be beneficial.", "When we want to analyze a quantitative variable in two categorical dimensions at once, there is a suitable function for this in the ", " library called ", ". For example, let's visualize the interaction between ", " and two categorical variables in the same plot:", "From this, we could conclude that, starting with 4 calls, ", " may no longer be the main factor for customer churn. Perhaps, in addition to our previous guess about the tariffs, there are customers that are dissatisfied with the service due to other problems, which might lead to fewer number of day minutes spent on calls.", "As we saw earlier in this article, the variable ", " has few unique values and, thus, can be considered either numerical or ordinal. We have already seen its distribution with a ", ". Now, we are interested in the relationship between this ordinal feature and the target variable ", ".", "Let\u2019s look at the distribution of the number of calls to the customer service, again using a ", ". This time, let\u2019s also pass the parameter ", " that adds a categorical dimension to the plot:", ": the churn rate increases significantly after 4 or more calls to the customer service.", "Now, let\u2019s look at the relationship between ", " and the binary features, ", " and ", ".", ": when ", " is enabled, the churn rate is much higher; the usage of the international plan by the customer is a strong feature. We do not observe the same effect with ", ".", "In addition to using graphical means for categorical analysis, there is a traditional tool from statistics: a ", ", also called a ", ". It represents multivariate frequency distribution of categorical variables in tabular form. In particular, it allows us to see the distribution of one variable conditional on the other by looking along a column or row.", "Let\u2019s try to see how ", " is related to the categorical variable ", " by creating a cross tabulation:", "In the case of ", ", the number of distinct values is rather high: 51. We see that there are only a few data points available for each individual state \u2014 only 3 to 17 customers in each state abandoned the operator. Let\u2019s ignore that for a second and calculate the churn rate for each state, sorting it from high to low:", "At first glance, it seems that the churn rate in ", " and ", " are above 25% and less than 6% for Hawaii and Alaska. However, these conclusions are based on too few examples, and our observation could be a mere property of our particular dataset. We can confirm this with the ", " and ", " correlation hypotheses, but this would be beyond the scope of this article.", "We have been looking at different ", " of our dataset by guessing interesting features and selecting a small number of them at a time for visualization. We have only delt with two to three variables at once and were easily able to observe the structure and relationships in data. But, what if we want to display all the features and still be able to interpret the resulting visualization?", "We could use ", " or create a scatterplot matrix with ", " for the whole dataset to look at all of our features simultaneously. But, when the number of features is high enough, this kind of visual analysis quickly becomes slow and inefficient. Besides, we would still be analyzing our variables in a pairwise fashion, not all at once.", "Most real-world datasets have many features, sometimes, many thousands of them. Each of them can be considered as a dimension in the space of data points. Consequently, more often than not, we deal with high-dimensional datasets, where entire visualization is quite hard.", "To look at a dataset as a whole, we need to decrease the number of dimensions used in visualization without losing much information about data. This task is called ", " and is an example of an ", " problem because we need to derive new, low-dimensional features from the data itself, without any supervised input.", "One of the well-known dimensionality reduction methods is ", " (PCA), which we will study later in this course. Its limitation is that it is a ", " algorithm that implies certain restrictions on the data.", "There are also many non-linear methods, collectively called ", ". One of the best-known of them is ", ".", "Let\u2019s create a ", " representation of the same churn data we have been using.", "The name of the method looks complex and a bit intimidating: ", ". Its math is also impressive (we will not delve into it here, but, if you feel brave, here is the ", " by Laurens van der Maaten and Geoffrey Hinton from ", "). Its basic idea is simple: to find a projection for a high-dimensional feature space onto a plane (or a 3D hyperplane, but it is almost always 2D) such that those points that were far apart in the initial n-dimensional space will end up far apart on the plane. Those that were originally close would remain close to each other.", "Essentially, ", " is a search for a new and less-dimensional data representation that preserves neighborship of examples.", "Now, let\u2019s do some practice. First, we need to import some additional classes:", "We will leave out the ", " and ", " features and convert the values \u201cYes\u201d/\u201dNo\u201d of the binary features into numerical values using ", ":", "We also need to normalize the data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with ", ".", "Now, let\u2019s build a t-SNE representation:", "and plot it:", "Let\u2019s color this t-SNE representation according to the churn (green for loyal customers, and red for those who left).", "We can see that customers who churned are concentrated in a few areas of the lower dimensional feature space.", "To better understand the picture, we can also color it with the remaining binary features: ", " and ", ". The green dots here indicate the objects that are positive for the corresponding binary feature.", "Now it is clear that, for example, many dissatisfied customers who canceled their subscription are crowded together in the most south-west cluster that represents the people with the international plan but no voice mail.", "Finally, let\u2019s note some disadvantages of t-SNE:", "Occasionally, using t-SNE, you can get a really good intuition for the data. The following is a good paper that shows an example of this for handwritten digits: ", ".", "Sometimes t-SNE really helps to understand something, and sometimes you can just draw a Christmas tree toy :-)", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:01:30.447Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 5. Bagging and Random Forest", "content": ["In the previous articles, you saw different classification algorithms as well as techniques for how to properly validate and evaluate the quality of your models.", "Now, suppose that you have chosen the best possible model for a particular problem and are struggling to further improve its accuracy. In this case, you would need to apply some more advanced machine learning techniques that are collectively referred to as ", ".", "An ", " is a set of elements that collectively contribute to a whole. A familiar example is a musical ensemble, which blends the sounds of several musical instruments to create a beautiful harmony, or architectural ensembles, which are a set of buildings designed as a unit. In ensembles, the (whole) harmonious outcome is more important than the performance of any individual part.", " (1784) is about an ensemble in some sense. It states that, if each member of the jury makes an independent judgement and the probability of the correct decision by each juror is more than 0.5, then the probability of the correct decision by the whole jury increases with the total number of jurors and tends to one. On the other hand, if the probability of being right is less than 0.5 for each juror, then the probability of the correct decision by the whole jury decreases with the number of jurors and tends to zero.", "One can find an analytic expression for this theorem in a ", " accompanying this article.", "Let\u2019s look at another example of ensembles: an observation known as ", ". In 1906, ", " visited a country fair in Plymouth where he saw a contest being held for farmers. 800 participants tried to estimate the weight of a slaughtered bull. The real weight of the bull was 1198 pounds. Although none of the farmers could guess the exact weight of the animal, the average of their predictions was 1197 pounds.", "A similar idea for error reduction was adopted in the field of Machine Learning.", " (also known as ", ") is one of the first and most basic ensemble techniques. It was proposed by ", " in 1994. Bagging is based on the statistical method of ", ", which makes the evaluation of many statistics of complex models feasible.", "The bootstrap method goes as follows. Let there be a sample ", " of size ", ". We can make a new sample from the original sample by drawing ", " elements from the latter randomly and uniformly, with replacement. In other words, we select a random element from the original sample of size ", " and do this ", " times. All elements are equally likely to be selected, thus each element is drawn with the equal probability ", ".", "Let\u2019s say we are drawing balls from a bag one at a time. At each step, the selected ball is put back into the bag so that the next selection is made equiprobably i.e. from the same number of balls ", ". Note that, because we put the balls back, there may be duplicates in the new sample. Let\u2019s call this new sample ", ".", "By repeating this procedure ", " times, we create ", " ", " ", ", \u2026, ", ". In the end, we have a sufficient number of samples and can compute various statistics of the original distribution.", "For our example, we\u2019ll use the familiar ", " dataset. Previously, when we discussed feature importance, we saw that one of the most important features in this dataset is the number of calls to customer service. Let's visualize the data and look at the distribution of this feature.", "As you can see, loyal customers make fewer calls to customer service than those who eventually left. Now, it might be a good idea to estimate the average number of customer service calls in each group. Since our dataset is small, we would not get a good estimate by simply calculating the mean of the original sample. We will be better off applying the bootstrap method. Let\u2019s generate 1000 new bootstrap samples from our original population and produce an interval estimate of the mean.", "In the end, we see that, with 95% probability, the average number of customer service calls from loyal customers lies between 1.4 and 1.49 while the churned clients called 2.06 through 2.40 times on average. Also, note that the interval for the loyal customers is narrower, which is reasonable since they make fewer calls (0, 1 or 2) in comparison with the churned clients who called until they became fed up and switched providers.", "Now that you\u2019ve grasped the idea of bootstrapping, we can move on to ", ".", "Suppose that we have a training set ", ". Using bootstrapping, we generate samples ", ", \u2026, ", ". Now, for each bootstrap sample, we train its own classifier ", ". The final classifier will average the outputs from all these individual classifiers. In the case of classification, this technique corresponds to voting:", "The picture below illustrates this algorithm:", "In a regression problem, by averaging individual answers, bagging reduces the mean squared error by a factor of ", ", the number of regressors. See the mathematical proof in our ", ".", "From our previous lesson, let\u2019s recall the components that make up the total out-of-sample error:", "Bagging reduces the variance of a classifier by decreasing the difference in error when we train the model on different datasets. In other words, bagging prevents overfitting. The efficacy of bagging comes from the fact that individual models are quite different due to the different training data and their errors cancel out during voting. Additionally, outliers are likely omitted in some of the training bootstap samples.", "The ", " library supports bagging with meta-estimators ", " and ", ". You can use most of the algorithms as a base.", "Let\u2019s examine how bagging works in practice and compare it with the decision tree. For this, we will use an example from ", ".", "The error for the decision tree:", "0.0255 = 0.0003", "+ 0.0152 ", " + 0.0098 ", "The error when using bagging:", "0.0196 = 0.0004 ", "+ 0.0092 ", "+ 0.0098 (", "\u00b2)", "As you can see from the graph above, the variance in the error is much lower for bagging. Remember that we have already proved this theoretically.", "Bagging is effective on small datasets. Dropping even a small part of training data leads to constructing substantially different base classifiers. If you have a large dataset, you would generate bootstrap samples of a much smaller size.", "The example above is unlikely to be applicable to any real work. This is because we made a strong assumption that our individual errors are uncorrelated. More often than not, this is way too optimistic for real-world applications. When this assumption is false, the reduction in error will not be as significant. In the following lectures, we will discuss some more sophisticated ensemble methods, which enable more accurate predictions in real-world problems.", "Looking ahead, in case of Random Forest, there is no need to use cross-validation or hold-out samples in order to get an unbiased error estimation. Why? Because, in ensemble techniques, the error estimation takes place internally.", "Random trees are constructed using different bootstrap samples of the original dataset. Approximately 37% of inputs are left out of a particular bootstrap sample and are not used in the construction of the ", "-th tree. The proof is easy and elegant, it\u2019s done in our ", ".", "Let\u2019s visualize how ", "ut-", "f-", "ag ", "rror (or OOBE) estimation works:", "The top part of the figure above represents our original dataset. We split it into the training (left) and test (right) sets. In the left image, we draw a grid that perfectly divides our dataset according to classes. Now, we use the same grid to estimate the share of the correct answers on our test set. We can see that our classifier gave incorrect answers in those 4 cases that have not been used during training (on the left). Hence, the accuracy of our classifier is 11/15*100% = 73.33%.", "To sum up, each base algorithm is trained on ~ 63% of the original examples. It can be validated on the remaining ~37%. The Out-of-Bag estimate is nothing more than the mean estimate of the base algorithms on those ~37% of inputs that were left out of training.", "Leo Breiman managed to apply bootstrapping not only in statistics but also in machine learning. He, along with Adel Cutler, extended and improved the Random Forest algorithm ", ". They combined the construction of uncorrelated trees using ", ", bagging, and the ", ".", "Decision trees are a good choice for the base classifier in bagging because they are quite sophisticated and can achieve zero classification error on any sample. The random subspace method reduces the correlation between the trees and thus prevents overfitting. With bagging, the base algorithms are trained on different random subsets of the original feature set.", "The following algorithm constructs an ensemble of models using the random subspace method:", "The algorithm for constructing a random forest of ", " trees goes as follows:", "The final classifier is defined by:", "We use the majority voting for classification and the mean for regression.", "For classification problems, it is advisable to set ", " to be equal the square root of ", ". For regression problems, we usually take ", ", where ", "is the number of features. It is recommended to build each tree until all of its leaves contain only 1 instance for classification and 5 instances for regression.", "You can see Random Forest as bagging of decision trees with the modification of selecting a random subset of features at each split.", "As we can see from our graphs and the MSE values above, a Random Forest of 10 trees achieves a better result than a single decision tree and is comparable to bagging with 10 trees. The main difference between Random Forests and bagging is that, in a Random Forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.", "We can also look at the advantages of Random Forests and bagging in classification problems.", "The figures above show that the decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.", "Now, let\u2019s investigate some parameters which can help us increase the model accuracy.", "The ", " implements random forests by providing two estimators: ", " and ", ".", "Below are the parameters which we need to pay attention to when we are building a new model:", "Please refer to this ", " to find an example of Random Forest tuning in a telecom churn problem. We get the following validation curves.", "The most important fact about Random Forests is that it\u2019s accuracy doesn\u2019t decrease when we add trees, so the number of trees is not a ", "hyperparameter as opposed to ", "and ", "This means that you can tune hyperparameters with, say, 10 trees, and then increase the number of trees till 500 and be sure that accuracy will only get better.", "In ", " you\u2019ll also find more analysis of bias and variance for Random Forests and it\u2019s analogies with k Nearset Neighbors.", "Extremely Randomized Trees employ a greater degree of randomization at the cut-point choice when splitting a tree node. As in Random Forests, a random subset of features is used. But, instead of the search for optimal thresholds, their values are selected at random for each possible feature, and the best one among these randomly generated thresholds is used as the best rule to split the node. This usually trades off a slight reduction in the model variance with a small increase of bias.", "In the ", " library, there are 2 implementations of Extremely Randomized Trees: ", " and ", ".", "This method should be used if you have greatly overfit with Random Forests or gradient boosting.", "Random forests are mostly used in supervised learning, but there is a way to apply them in the unsupervised setting.", "Using the ", " method ", ", we can transform our dataset into a high-dimensional, ", " representation. We first build extremely randomized trees and then use the index of the leaf containing the example as a new feature.", "For example, if the input appears in the first leaf, we assign 1 as the feature value; if not, we assign 0. This is a so-called ", ". We can control the number of features and the sparseness of data by increasing or decreasing the number of trees and their depth. Because nearby data points are likely to fall into the same leaf, this transformation provides an implicit nonparametric estimate of their density.", "Pretty often, you would like to make out the exact reasons why the algorithm outputted a particular answer. Or, not being able to understand the algorithm completely, then st lest we\u2019d like to find out which input features contributed the most to the result. With Random Forest, you can obtain such information quite easily.", "From the picture below, it is intuitively clear that, in our credit scoring problem, ", " is much more important than ", ". This can be formally explained using the concept of ", ".", "In the case of many decision trees (i.e. a Random Forest), the closer the mean position of a feature over all trees to the root, the more important it is for a given classification or regression problem. Gains in the splitting criterion, such as the ", ", obtained at each optimal split in every tree, is a measure of importance that is directly associated with the splitting feature. The value of this score is distinct for each feature and accumulates over all the trees.", "We go a little deeper into details in ", ".", "Let\u2019s consider the results of a survey given to visitors of hostels listed on Booking.com and TripAdvisor.com. Our features here are the average ratings for different categories include service quality, room condition, value for money, etc. Our target variable is the hostel\u2019s overall rating on the website.", "The picture above shows that, more often than not, customers pay great attention to staff and the price-quality ratio. This couple of factors affects the resulting overall rating the most. But, the difference between these two features and other festures is not very large. We can therefore conclude that exclusion of any of these features will lead to a reduction in model accuracy. Based on our analysis, we csn recommend hostel owners to focus primarily on staff training and price-to-quality ratio.", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:08:54.419Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Anastasiia Manokhina", "articleTile": "Open Machine Learning Course. Topic 10. Gradient Boosting", "content": ["Hello everyone!", "So far, we\u2019ve covered 9 topics from Exploratory Data Analysis to Time Series Analysis in Python. Today we are going to have a look at one of the most popular and practical machine learning algorithms: gradient boosting. You can find more detailed math in the ", " format of the article.", "Almost everyone in machine learning has heard about gradient boosting. Many data scientists include this algorithm in their data scientist\u2019s toolbox because of the good results it yields on any given (unknown) problem.", "Furthermore, XGBoost is often the standard recipe for ", " ", ". It is so popular that the idea of stacking XGBoosts has become a meme. Moreover, boosting is an important component in ", "; sometimes, it is even considered a ", ". Let\u2019s look at the history and development of boosting.", "Boosting was born out of ", " is it possible to get one strong model from a large amount of relatively weak and simple models?", " By saying \u201cweak models\u201d, we do not mean simple basic models like decision trees but models with poor accuracy performance, where poor is a little bit better than random.", " to this question was identified, but it took a few years to develop fully functioning algorithms based on this solution e.g. AdaBoost. These algoritms take a greedy approach: first, they build a linear combination of simple models (basic algorithms) by re-weighing the input data. Then, the model (usually a decision tree) is built on earlier incorrectly predicted objects, which are now given larger weights.", "Many machine learning courses study AdaBoost \u2014 the ancestor of GBM (Gradient Boosting Machine). However, since AdaBoost merged with GBM, it has become apparent that AdaBoost is just a particular variation of GBM.", "The algorithm itself has a very clear visual interpretation and intuition for defining weights. Let\u2019s have a look at the following toy classification problem where we are going to split the data between the trees of depth 1 (also known as \u2018stumps\u2019) on each iteration of AdaBoost. For the first two iterations, we have the following picture:", "The size of point corresponds to its weight, which was assigned for an incorrect prediction. On each iteration, we can see that these weights are growing \u2014 the stumps cannot cope with this problem. Although, if we take a weighted vote for the stumps, we will get the correct classifications:", "Here is a more detailed example of AdaBoost where, as we iterate, we can see the weights increase, especially on the border between classes.", "AdaBoost works well, but ", " of explanation for why the algorithm is successful sewed the seeds of doubt. Some considered it a super-algorithm, a silver bullet, but others were skeptical and believed AdaBoost was just overfitting.", "The overfitting problem did indeed exist, especially when data had strong outliers. Therefore, in those types of problems, AdaBoost was unstable. Fortunately, a few professors in the statistics department at Stanford, who had created Lasso, Elastic Net, and Random Forest, started researching the algorithm. In 1999, Jerome Friedman came up with the generalization of boosting algorithms development \u2014 Gradient Boosting (Machine), also known as GBM. With this work, Friedman set up the statistical foundation for many algorithms providing the general approach of boosting for optimization in the functional space.", "CART, bootstrap, and many other algorithms have originated from Stanford\u2019s statistics department. In doing so, the department has solidified their names in future textbooks. These algorithms are very practical, and some recent works have yet to be widely adopted. For example, check out ", ".", "Not many video recordings of Friedman are available. Although, there is a very interesting ", " with him about the creation of CART and how they solved statistics problems (which is similar to data analysis and data science today) more than 40 years ago. There is also a great ", ", a retrospective on data analysis from one of the creators of methods that we use everyday.", "In general, there has been a transition from engineering and algorithmic research to a full-fledged approach to building and studying algorithms. From a mathematical perspective, this is not a big change \u2014 we are still adding (or boosting) weak algorithms and enlarging our ensemble with gradual improvements for parts of the data where the model was inaccurate. But, this time, the next simple model is not just built on re-weighted objects but improves its approximation of the gradient of overall objective function. This concept greatly opens up our algorithms for imagination and extensions.", "It took more than 10 years after the introduction of GBM for it to become an essential part of the data science toolbox. GBM was extended to apply to different statistics problems: GLMboost and GAMboost for strengthening already existing GAM models, CoxBoost for survival curves, and RankBoost and LambdaMART for ranking.", "Many realizations of GBM also appeared under different names and on different platforms: Stochastic GBM, GBDT (Gradient Boosted Decision Trees), GBRT (Gradient Boosted Regression Trees), MART (Multiple Additive Regression Trees), and more. In addition, the ML community was very segmented and dissociated, which made it hard to track just how widespread boosting had become.", "At the same time, boosting had been actively used in search ranking. This problem was rewritten in terms of a loss function that penalizes errors in the output order, so it became convenient to simply insert it into GBM. AltaVista was one of the first companies who introduced boosting to ranking. Soon, the ideas spread to Yahoo, Yandex, Bing, etc. Once this happened, boosting became one of the main algorithms that was used not only in research but also in core technologies in industry.", "ML competitions, especially Kaggle, played a major role in boosting\u2019s popularization. Now, researchers had a common platform where they could compete in different data science problems with large number of participants from around the world. With Kaggle, one could test new algorithms on the real data, giving algoritms oppurtunity to \u201cshine\u201d, and provide full information in sharing model performance results across competition data sets. This is exactly what happened to boosting when it was used at ", " (check interviews with Kaggle winners starting from 2011 who mostly used boosting). The ", " library quickly gained popularity after its appearance. XGBoost is not a new, unique algorithm; it is just an extremely effective realization of classic GBM with additional heuristics.", "This algorithm has gone through very typical path for ML algorithms today: mathematical problem and algorithmic crafts to successful practical applications and mass adoption years after its first appearance.", "We are going to solve the problem of function approximation in a general supervised learning setting. We have a set of features ", " and target variables ", " which we use to restore the dependence ", ". We restore the dependence by approximating ", " and by understanding which approximation is better when we use the loss function", " , which we want to minimize:", "At this moment, we do not make any assumptions regarding the type of dependence", ", the model of our approximation, or the distribution of the target variable. We only expect that the function ", " is differentiable. Our formula is very general; let\u2019s define it for a particular data set with a population mean. Our expression for minimizing the loss of the data is the following:", "Unfortunately, the number of these functions is not just large, its functional space is infinite-dimensional. That is why it is acceptable for us to limit the search space by some family of functions. This simplifies the objective a lot because now we have a solvable optimization of parameter values.", "Simple analytical solutions for finding the optimal parameters often do not exist, so the parameters are usually approximated iteratively. To start, we write down the empirical loss function that will allow us to evaluate our parameters using our data. Additionally, let\u2019s write out our approximation for a number of ", " iterations as a sum:", "Then, the only thing left is to find a suitable, iterative algorithm to minimize the last expression. Gradient descent is the simplest and most frequently used option. We define the gradient and add our iterative evaluations to it (since we are minimizing the loss, we add the minus sign). Our last step is to initialize our first approximation and choose the number of iterations ", ". Let\u2019s review the steps for this inefficient and naive algorithm:", "Let\u2019s imagine for a second that we can perform optimization in the function space and iteratively search for the approximations as functions themselves. We will express our approximation as a sum of incremental improvements, each being a function.", "Nothing has happened yet; we have only decided that we will search for our approximation not as a big model with plenty of parameters (as an example, neural network), but as a sum of functions, pretending we move in functional space.", "In order to accomplish this task, we need to limit our search by some function family. There are a few issues here \u2014 first of all, the sum of models can be more complicated than any model from this family; secondly, the general objective is still in functional space. Let\u2019s note that, on every step, we will need to select an optimal coefficient. For step ", ", the problem is the following:", "Here is where the magic happens. We have defined all of our objectives in general terms, as if we could have trained any kind of model for any type of loss functions. In practice, this is extremely difficult, but, fortunately, there is a simple way to solve this task.", "Knowing the expression of loss function\u2019s gradient, we can calculate its value on our data. So, let\u2019s train the models such that our predictions will be more correlated with this gradient (with a minus sign). In other words, we will use least squares to correct the predictions with these residuals. For classification, regression, and ranking tasks, we will minimize the squared difference between pseudo-residuals ", " and our predictions. For step ", ", the final problem looks the following:", "We can now define the classic GBM algorithm suggested by Jerome Friedman in 1999. It is a supervised algorithm that has the following components:", "The only thing left is the initial approximation. For simplicity, for an initial approximation, a constant value ", " is used. The constant value, as well as the optimal coefficient, are identified via binary search or another line search algorithm over the initial loss function (not a gradient). So, we have our GBM algorithm described as follows:", "Let\u2019s see an example of how GBM works. In this toy example, we will restore a noisy function", "This is a regression problem with a real-valued target, so we will choose to use the mean squared error loss function. We will generate 300 pairs of observations and approximate them with decision trees of depth 2. Let\u2019s put together everything we need to use GBM:", "We will run GBM and draw two types of graphs: the current approximation (blue graph) and every tree built on its pseudo-residuals (green graph). The graph\u2019s number corresponds to the iteration number:", "By the second iteration, our trees have recovered the basic form of the function. However, at the first iteration, we see that the algorithm has built only the \u201cleft branch\u201d of the function. This was due to the fact that our trees simply did not have enough depth to build a symmetrical branch at once, and it focused on the left branch with the larger error. Therefore, the right branch appeared only after the second iteration.", "The rest of the process goes as expected \u2014 on every step, our pseudo-residuals decreased, and GBM approximated the original function better and better with each iteration. However, by construction, trees cannot approximate a continuous function, which means that GBM is not ideal in this example. To play with GBM function approximations, you can use the awesome interactive demo in this blog called ", ":", "If we want to solve a classification problem instead of regression, what would change? We only need to choose a suitable loss function ", ". This is the most important, high-level moment that determines exactly how we will optimize and what characteristics we can expect in the final model.", "As a rule, we do not need to invent this ourselves \u2014 researchers have already done it for us. Today, we will explore loss functions for the two most common objectives: regression and binary classification.", "Let\u2019s start with a regression problem for ", "a real number. In order to choose the appropriate loss function, we need to consider which of the properties of the conditional distribution ", " we want to restore. The most common options are:", "Let\u2019s use loss function ", " on our data. The goal is to restore the conditional 75%-quantile of cosine. Let us put everything together for GBM:", "For our initial approximation, we will take the needed quantile of ", ". However, we do not know anything about optimal coefficients, so we\u2019ll use standard line search. The results are the following:", "The overall results of GBM with quantile loss function are the same as the results with quadratic loss function offset by ~ 0.135. But if we were to use the 90%-quantile, we would not have enough data due to the fact that classes would become unbalanced. We need to remember this when we deal with non-standard problems.", "For regression tasks, many loss functions have been developed, some of them with extra properties. For example, they can be robust like in the ", ". For a small number of outliers, the loss function works as ", ", but after a defined threshold, the function changes to ", ". This allows for decreasing the effect of outliers and focusing on the overall picture.", "We can illustrate this with the following example. Data is generated from the function ", " with added noise, a mixture from normal and Bernoulli distributions. We show the functions on graphs A-D and the relevant GBM on F-H (graph E represents the initial function):", ".", "In this example, we used splines as the base algorithm. See, it does not always have to be trees for boosting?", "We can clearly see the difference between the functions ", ", ", ", and Huber loss. If we choose optimal parameters for the Huber loss, we can get the best possible approximation among all our options. The difference can be seen as well in the 10%, 50%, and 90%-quantiles.", "Unfortunately, Huber loss function is supported only by very few popular libraries/packages; h2o supports it, but XGBoost does not. It is relevant to other things that are more exotic like ", ", but it may still be interesting knowledge.", "Now, let\u2019s look at the binary classification problem. We saw that GBM can even optimize non-differentiable loss functions. Technically, it is possible to solve this problem with a regression ", " loss, but it wouldn\u2019t be correct.", "The distribution of the target variable requires us to use log-likehood, so we need to have different loss functions for targets multiplied by their predictions. The most common choices would be the following:", "Let\u2019s generate some new toy data for our classification problem. As a basis, we will take our noisy cosine, and we will use the sign function for classes of the target variable. Our toy data looks like the following (jitter-noise is added for clarity):", "We will use logistic loss to look for what we actually boost. So, again, we put together what we will use for GBM:", "This time, the initialization of the algorithm is a little bit harder. First, our classes are imbalanced (63% versus 37%). Second, there is no known analytical formula for the initialization of our loss function, so we have to look for it via search:", "Our optimal initial approximation is around -0.273. You could have guessed that it was negative because it is more profitable to predict everything as the most popular class, but there is no formula for the exact value. Now let\u2019s finally start GBM, and look what actually happens under the hood:", "The algorithm successfully restored the separation between our classes. You can see how the \u201clower\u201d areas are separating because the trees are more confident in the correct prediction of the negative class and how the two steps of mixed classes are forming. It is clear that we have a lot of correctly classified observations and some amount of observations with large errors that appeared due to the noise in the data.", "Sometimes, there is a situation where we want a more specific loss function for our problem. For example, in financial time series, we may want to give bigger weight to large movements in the time series; for churn prediction, it is more useful to predict the churn of clients with high LTV (or lifetime value: how much money a client will bring in the future).", "The statistical warrior would invent their own loss function, write out the gradient for it (for more effective training, include the Hessian), and carefully check whether this function satisfies the required properties. However, there is a high probability of making a mistake somewhere, running up against computational difficulties, and spending an inordinate amount of time on research.", "In lieu of this, a very simple instrument was invented (which is rarely remembered in practice): weighing observations and assigning weight functions. The simplest example of such weighting is the setting of weights for class balance. In general, if we know that some subset of data, both in the input variables and in the target variable, has greater importance for our model, then we just assign them a larger weight ", ". The main goal is to fulfill the general requirements for weights:", "Weights can significantly reduce the time spent adjusting the loss function for the task we are solving and also encourages experiments with the target models\u2019 properties. Assigning these weights is entirely a function of creativity. We simply add scalar weights:", "It is clear that, for arbitrary weights, we do not know the statistical properties of our model. Often, linking the weights to the values ", " can be too complicated. For example, the usage of weights proportional to ", " in ", " loss function is not equivalent to ", " loss because the gradient will not take into account the values of the predictions themselves.", "We mention all of this so that we can understand our possibilities better. Let\u2019s create some very exotic weights for our toy data. We will define a strongly asymmetric weight function as follows:", "With these weights, we expect to get two properties: less detailing for negative values of ", " and the form of the function, similar to the initial cosine. We take the other GBM\u2019s tunings from our previous example with classification including the line search for optimal coefficients. Let\u2019s look what we\u2019ve got:", "We achieved the result that we expected. First, we can see how strongly the pseudo-residuals differ; on the initial iteration, they look almost like the original cosine. Second, the left part of the function\u2019s graph was often ignored in favor of the right one, which had larger weights. Third, the function that we got on the third iteration received enough attention and started looking similar to the original cosine (also started to slightly overfit).", "Weights are a powerful but risky tool that we can use to control the properties of our model. If you want to optimize your loss function, it is worth trying to solve a more simple problem first but add weights to the observations at your discretion.", "Today, we learned the theory behind gradient boosting. GBM is not just some specific algorithm but a common methodology for building ensembles of models. In addition, this methodology is sufficiently flexible and expandable \u2014 it is possible to train a large number of models, taking into consideration different loss-functions with a variety of weighting functions.", "Practice and ML competitions show that, in standard problems (except for image, audio, and very sparse data), GBM is often the most effective algorithm (not to mention stacking and high-level ensembles, where GBM is almost always a part of them). Also, there are many adaptations of GBM ", " (Minecraft, ICML 2016). By the way, the Viola-Jones algorithm, which is still used in computer vision, ", ".", "In this article, we intentionally omitted questions concerning GBM\u2019s regularization, stochasticity, and hyper-parameters. It was not accidental that we used a small number of iterations ", " = 3 throughout. If we used 30 trees instead of 3 and trained the GBM as described, the result would not be that predictable:", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:13:45.709Z"}
{"nameOfPublication": "Applied Data Science", "nameOfAuthor": "David Foster", "articleTile": "How to build your own AlphaZero AI using Python and Keras", "content": ["In this article I\u2019ll attempt to cover three things:", "In March 2016, Deepmind\u2019s AlphaGo beat 18 times world champion Go player Lee Sedol 4\u20131 in a series watched by over 200 million people. A machine had learnt a super-human strategy for playing Go, a feat previously thought impossible, or at the very least, at least a decade away from being accomplished.", "This in itself, was a remarkable achievement. However, on 18th October 2017, DeepMind took a giant leap further.", "The paper", "unveiled a new variant of the algorithm, AlphaGo Zero, that had defeated AlphaGo 100\u20130. Incredibly, it had done so by learning solely through self-play, starting \u2018tabula rasa\u2019 (blank state) and gradually finding strategies that would beat previous incarnations of itself. No longer was a database of human expert games required to build a super-human AI .", "A mere 48 days later, on 5th December 2017, DeepMind released another paper ", " showing how AlphaGo Zero could be adapted to beat the world-champion programs StockFish and Elmo at chess and shogi. The entire learning process, from being shown the games for the first time, to becoming the best computer program in the world, had taken under 24 hours.", "With this, AlphaZero was born \u2014 the general algorithm for getting good at something, quickly, without any prior knowledge of human expert strategy.", "There are two amazing things about this achievement:", "It cannot be overstated how important this is. This means that the underlying methodology of AlphaGo Zero can be applied to ", " game with perfect information (the game state is fully known to both players at all times) because no prior expertise is required beyond the rules of the game.", "This is how it was possible for DeepMind to publish the chess and shogi papers only 48 days after the original AlphaGo Zero paper. Quite literally, all that needed to change was the input file that describes the mechanics of the game and to tweak the hyper-parameters relating to the neural network and Monte Carlo tree search.", "If AlphaZero used super-complex algorithms that only a handful of people in the world understood, it would still be an incredible achievement. What makes it extraordinary is that a lot of the ideas in the paper are actually far less complex than previous versions. At its heart, lies the following beautifully simple mantra for learning:", "Mentally play through possible future scenarios, giving priority to promising paths, whilst also considering how others are most likely to react to your actions and continuing to explore the unknown.", "After reaching a state that is unfamiliar, evaluate how favourable you believe the position to be and cascade the score back through previous positions in the mental pathway that led to this point.", "After you\u2019ve finished thinking about future possibilities, take the action that you\u2019ve explored the most.", "At the end of the game, go back and evaluate where you misjudged the value of the future positions and update your understanding accordingly.", "Doesn\u2019t that sound a lot like how you learn to play games? When you play a bad move, it\u2019s either because you misjudged the future value of resulting positions, or you misjudged the likelihood that your opponent would play a certain move, so didn\u2019t think to explore that possibility. These are exactly the two aspects of gameplay that AlphaZero is trained to learn.", "Firstly, check out the ", " for a high level understanding of how AlphaGo Zero works. It\u2019s worth having that to refer to as we walk through each part of the code. There\u2019s also a great article ", " that explains how AlphaZero works in more detail.", "Clone ", " Git repository, which contains the code I\u2019ll be referencing.", "To start the learning process, run the top two panels in the ", "Jupyter notebook. Once it\u2019s built up enough game positions to fill its memory the neural network will begin training. Through additional self-play and training, it will gradually get better at predicting the game value and next moves from any position, resulting in better decision making and smarter overall play.", "We\u2019ll now have a look at the code in more detail, and show some results that demonstrate the AI getting stronger over time.", "N.B \u2014 This is my own understanding of how AlphaZero works based on the information available in the papers referenced above. If any of the below is incorrect, apologies and I\u2019ll endeavour to correct it!", "The game that our algorithm will learn to play is Connect4 (or Four In A Row). Not quite as complex as Go\u2026 but there are still 4,531,985,219,092 game positions in total.", "The game rules are straightforward. Players take it in turns to enter a piece of their colour in the top of any available column. The first player to get four of their colour in a row \u2014 each vertically, horizontally or diagonally, wins. If the entire grid is filled without a four-in-a-row being created, the game is drawn.", "Here\u2019s a summary of the key files that make up the codebase:", "This file contains the game rules for Connect4.", "Each squares is allocated a number from 0 to 41, as follows:", "The game.py file gives the logic behind moving from one game state to another, given a chosen action. For example, given the empty board and action 38, the ", " method return a new game state, with the starting player\u2019s piece at the bottom of the centre column.", "You can replace the game.py file with any game file that conforms to the same API and the algorithm will in principal, learn strategy through self play, based on the rules you have given it.", "This contains the code that starts the learning process. It loads the game rules and then iterates through the main loop of the algorithm, which consist of three stages:", "There are two agents involved in this loop, the ", " and the ", ".", "The best_player contains the best performing neural network and is used to generate the self play memories. The current_player then retrains its neural network on these memories and is then pitched against the best_player. If it wins, the neural network inside the best_player is switched for the neural network inside the current_player, and the loop starts again.", "This contains the Agent class (a player in the game). Each player is initialised with its own neural network and Monte Carlo Search Tree.", "The ", " method runs the Monte Carlo Tree Search process. Specifically, the agent moves to a leaf node of the tree, evaluates the node with its neural network and then backfills the value of the node up through the tree.", "The ", " method repeats the simulation multiple times to understand which move from the current position is most favourable. It then returns the chosen action to the game, to enact the move.", "The ", " method retrains the neural network, using memories from previous games.", "This file contains the Residual_CNN class, which defines how to build an instance of the neural network.", "It uses a condensed version of the neural network architecture in the AlphaGoZero paper \u2014 i.e. a convolutional layer, followed by many residual layers, then splitting into a value and policy head.", "The depth and number of convolutional filters can be specified in the config file.", "The Keras library is used to build the network, with a backend of Tensorflow.", "To view individual convolutional filters and densely connected layers in the neural network, run the following inside the the run.ipynb notebook:", "This contains the Node, Edge and MCTS classes, that constitute a Monte Carlo Search Tree.", "The MCTS class contains the ", " and ", "methods previously mentioned, and instances of the Edge class store the statistics about each potential move.", "This is where you set the key parameters that influence the algorithm.", "Adjusting these variables will affect that running time, neural network accuracy and overall success of the algorithm. The above parameters produce a high quality Connect4 player, but take a long time to do so. To speed the algorithm up, try the following parameters instead.", "Contains the ", "and", " functions that play matches between two agents.", "To play against your creation, run the following code (it\u2019s also in the run.ipynb notebook)", "When you run the algorithm, all model and memory files are saved in the ", "folder, in the root directory.", "To restart the algorithm from this checkpoint later, transfer the run folder to the run_archive folder, attaching a run number to the folder name. Then, enter the run number, model version number and memory version number into the initialise.py file, corresponding to the location of the relevant files in the run_archive folder. Running the algorithm as usual will then start from this checkpoint.", "An instance of the Memory class stores the memories of previous games, that the algorithm uses to retrain the neural network of the current_player.", "This file contains a custom loss function, that masks predictions from illegal moves before passing to the cross entropy loss function.", "The locations of the run and run_archive folders.", "Log files are saved to the ", " folder inside the run folder.", "To turn on logging, set the values of the logger_disabled variables to False inside this file.", "Viewing the log files will help you to understand how the algorithm works and see inside its \u2018mind\u2019. For example, here is a sample from the logger.mcts file.", "Equally from the logger.tourney file, you can see the probabilities attached to each move, during the evaluation phase:", "Training over a couple of days produces the following chart of loss against mini-batch iteration number:", "The top line is the error in the policy head (the cross entropy of the MCTS move probabilities, against the output from the neural network). The bottom line is the error in the value head (the mean squared error between the actual game value and the neural network predict of the value). The middle line is an average of the two.", "Clearly, the neural network is getting better at predicting the value of each game state and the likely next moves. To show how this results in stronger and stronger play, I ran a league between 17 players, ranging from the 1st iteration of the neural network, up to the 49th. Each pairing played twice, with both players having a chance to play first.", "Here are the final standings:", "Clearly, the later versions of the neural network are superior to the earlier versions, winning most of their games. It also appears that the learning hasn\u2019t yet saturated \u2014 with further training time, the players would continue to get stronger, learning more and more intricate strategies.", "As an example, one clear strategy that the neural network has favoured over time is grabbing the centre column early. Observe the difference between the first version of the algorithm and say, the 30th version:", "This is a good strategy as many lines require the centre column \u2014 claiming this early ensures your opponent cannot take advantage of this. This has been learnt by the neural network, without any human input.", "There is a game.py file for a game called \u2018Metasquares\u2019 in the ", " folder. This involves placing X and O markers in a grid to try to form squares of different sizes. Larger squares score more points than smaller squares and the player with the most points when the grid is full wins.", "If you switch the Connect4 game.py file for the Metasquares game.py file, the same algorithm will learn how to play Metasquares instead.", "Hopefully you find this article useful \u2014 let me know in the comments below if you find any typos or have questions about anything in the codebase or article and I\u2019ll get back to you as soon as possible.", "If you would like to learn more about how our company, Applied Data Science develops innovative data science solutions for businesses, feel free to get in touch through our ", " or directly through LinkedIn.", "Applied Data Science is a London based consultancy that implements end-to-end data science solutions for businesses, delivering measurable value. If you\u2019re looking to do more with your data, let\u2019s talk.", "Written by"], "postingTime": "2019-07-30T16:38:11.768Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "Arthur Juliani", "articleTile": "Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning", "content": ["Since the ", " of the ML-Agents platform a few months ago, I have been surprised and delighted to find that thanks to it and other tools like OpenAI Gym, a new, wider audience of individuals are building Reinforcement Learning (RL) environments, and using them to train state-of-the-art models. The ability to work with these algorithms, previously something reserved for ML PhDs, is opening up to a wider world. As a result, I have had the unique opportunity to not just write about applying RL to existing problems, but also to help developers and researchers debug their models in a more active way. In doing so, I often get questions which come down to a matter of understanding the unique hyperparameters and learning process around the RL paradigm. In this article, I want to attempt to highlight one of these conceptual pieces: ", ", and attempt to demystify it to some extent. My hope is that in doing so a greater number of people will be able to debug their agent\u2019s learning process with greater confidence.", "Many machine learning practitioners are familiar with the traditional bias-variance trade-off. For those who aren\u2019t, it goes as follows: on the one hand, a \u201cbiased\u201d model generalizes well, but doesn\u2019t fit the data perfectly (\u201cunder-fitting\u201d). On the other hand, a high-variance model fits the training data well, too well in-fact, to the detriment of generalization (\u201coverfitting\u201d). In this situation, the problem becomes one of limiting the capacity of a model with some regularization method. In many cases, dropout or L2 regularization with a large enough data set is enough to do the trick. That is the story for typical supervised learning. RL is a little different, as it has its own separate bias-variance trade-off which operates in addition to, and at a higher level than the typical ML one.", "In RL, bias and variance no longer just refer to how well the model fits the training data, as in supervised learning, but also to ", ". To understand that statement, we have to backup a little. In reinforcement learning, instead of a set of labeled training examples to derive a signal from, an agent receives a reward at every decision-point in an environment. The goal of an agent is to learn a ", " (method for taking actions) which will lead to obtaining the greatest reward over time. We must do this using only the individual rewards that agent receives, without the help of an outside oracle to designate what count as \u201cgood\u201d or \u201cbad\u201d actions.", "A naive approach to an RL learning algorithm would be to encourage actions which were associated with positive rewards, and discourage actions associated with negative rewards. Instead of updating our agent\u2019s policy based on immediate rewards though, we often want to account for actions (and the states of the environment when those actions were taken) which lead up to rewards. For example, imagine walking down a corridor to a rewarding object. It isn\u2019t just the final step we want to perform again, but all the steps up to that rewarding one. There are a number of approaches for doing this, all of which involving doing a form of ", ". This means giving some credit to the series of actions which led to a positive reward, not just the most recent action. This credit assignment is often referred to as learning a value estimate: ", " for state, and ", " for state-action pair.", "We control how rewarding past actions and states are considered to be by using a discount factor (\u03b3, ranging from 0 to 1). Large values of \u03b3 lead to assigning credit to states and actions far into the past, while a small value leads to only assigning credit to more recent states and actions. In the case of RL, variance now refers to a noisy, but on average accurate value estimate, whereas bias refers to a stable, but inaccurate value estimate. To make this more concrete, imagine a game of darts. A high-bias player is one who always hits close to the target, but is always consistently off in some direction. A high-variance player, on the other hand, is one who sometimes hits the target, and is sometimes off, but on average near the target.", "There is a multitude of ways of assigning credit, given an agent\u2019s trajectory through an environment, each with different amounts of variance or bias. ", " of action trajectories as well as ", " are two classic algorithms used for value estimation, and both are prototypical examples of methods which are variance and bias heavy, respectively.", "In ", ", we rely on full trajectories of an agent acting within an episode of the environment to compute the reinforcement signal. Given a trajectory, we produce a value estimate ", "for each step in the path by calculating a discounted sum of future rewards for each step in the trajectory. The problem is that the policies we are learning (and often the environments we are learning in) are stochastic, which means there is a certain level of noise to account for. This stochasticity leads to variance in the rewards received in any given trajectory. Imagine again the example with the reward at the end of the corridor. Given that an agent\u2019s policy might be stochastic, it could be the case that in some trajectories the agent is able to walk to the rewarding state at the end, and in other trajectories it fails to do so. These two kinds of trajectories would provide very different value estimates, with the former suggesting the end of the corridor is valuable, and the latter suggesting it isn\u2019t. This variance is typically mitigated by using a large number of action trajectories, with the hope that the variance introduced in any one trajectory will be reduced in aggregate, and provide an estimate of the \u201ctrue\u201d reward structure of the environment.", "On the other end of the spectrum is one-step ", ". In this approach, the reward signal for each step in a trajectory is composed of the immediate reward plus a learned estimate of the value at the next step. By relying on a value estimate rather than a Monte-Carlo rollout there is much less stochasticity in the reward signal, since our value estimate is relatively stable over time. The problem is that the signal is now biased, due to the fact that our estimate is never completely accurate. In our corridor example, we might have some estimate of the value of the end of the corridor, but it may suggest that the corridor is less valuable than it actually is, since our estimate may not be able to distinguish between it and other similar unrewarding corridors. Furthermore, in the case of Deep Reinforcement Learning, the value estimate is often modeled using a deep neural network, making things worse. In ", " for example, the Q-estimates (value estimates over actions) are computed using an old copy of the network (a \u201ctarget\u201d network), which will provide \u201colder\u201d Q-estimates, with a very specific kind of bias, relating to the belief of an outdated model.", "Now that we understand bias and variance and their causes, how do we address them? There are a number of approaches which attempt to mitigate the negative effect of too much bias or too much variance in the reward signal. I am going to highlight a few of the most commonly used approaches in modern systems such as ", " (PPO), ", " (A3C), ", " (TRPO), and others.", "One of the most common approaches to reducing the variance of an estimate is to employ a baseline which is subtracted from the reward signal to produce a more stable value. Many of the baselines chosen fall into the category of ", ", which utilize both an actor which defines the policy, and a critic (often a parameterized value estimate) which provides a more reduced variance reward signal to update the actor. The thinking goes that variance can simply be subtracted out from a Monte-Carlo sample (R/Q) using a more stable learned value function V(s) in the critic. This value function is typically a neural network, and can be learned using either Monte-Carlo sampling, or Temporal difference (TD) learning. The resulting Advantage A(s, a) is then the difference between the two estimates. This advantage estimate has the other nice property of corresponding to ", " thus allowing for intuitively interpretable values.", "We can also arrive at advantage functions in other ways than employing a simple baseline. For example, the value function can be applied to directly smooth the reinforcement signal obtained from a series of trajectories. The ", " (GAE), introduced by John Schulman in 2016 does just this. The GAE formulation allows for an interpolation between pure TD learning and pure Monte-Carlo sampling using a lambda parameter. By setting lambda to 0, the algorithm reduces to TD learning, while setting it to 1 produces Monte-Carlo sampling. Values in-between (particularly those in the 0.9 to 0.999 range) produce better empirical performance by trading off the bias of ", " with the variance of the trajectory.", "Outside of calculating an advantage function, the bias-variance trade-off presents itself when deciding what to do at the end of a trajectory when learning. Instead of waiting for an entire episode to complete before collecting a trajectory of experience, modern RL algorithms often break experience batches down into smaller sub-trajectories, and use a value-estimate to bootstrap the Monte-Carlo signal when that trajectory doesn\u2019t end with the termination of the episode. By using a bootstrap signal, that estimate can contain information about ", ". It is essentially a guess about how the episode will turn out from that point onward. Take again our example of the corridor. If we are using a time horizon for our trajectories that ends halfway through the corridor, and if our value estimate reflects the fact that there is a rewarding state at the end, we will be able to assign value to the early part of the corridor, even though the agent didn\u2019t experience the reward. As one might expect, the longer the trajectory length we use, the less frequently value estimates are used for bootstrapping, and thus the greater the variance (and lower the bias). In contrast, using short trajectories means relying more on the value estimate, creating a more biased reinforcement signal. By deciding how long the trajectory needs to be before cutting it off and bootstrapping it, we can propagate the reward signal in a more efficient way, but only if we get the balance right.", "Say you have some environment you\u2019d like to have an agent learn to perform a task within (for example, an environment made using ", "). How do you decide how to control the ", " and/or ", "? The outcome of setting these hyperparameters in various ways often depends on the task, and come down to a couple of factors:", "Ultimately, correctly balancing the trade-off comes down to a few things: gaining an intuition for the kind of problem under consideration, and knowing what hyperparameters for any given algorithm correspond to what changes in the learning process. In the case of an algorithm like PPO, this corresponds to the ", ". Below are a few guidelines which may be helpful:", "With all the tweaking and tuning that often goes into the process, it can sometimes feel overwhelming, and like black magic, but hopefully, the information presented above can help contribute, even in a small way, to ensure that Deep Reinforcement Learning is a little more interpretable to those practicing it.", "Written by"], "postingTime": "2018-02-15T08:09:01.797Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Yury Kashnitskiy", "articleTile": "Open Machine Learning Course. Topic 4. Linear Classification and Regression", "content": ["Welcome to the 4-th week of our course! Now we will present our most important topic \u2014 linear models. If you have your data prepared and want to start training models, then you will most probably first try either linear or logistic regression, depending on your task (regression or classification).", "This week\u2019s material covers both theory of linear models and practical aspects of their usage in real-world tasks. There\u2019s going to be a lot of math in this topic, and we won\u2019t even try to render all the formulas on Medium. Instead, we provide a Jupyter Notebook for each part of this article. In the assignment, you\u2019ll beat two simple benchmarks in a Kaggle competition solving a problem of identifying a user based on her session of visited websites.", "We will be solving the intruder detection problem analyzing users\u2019 behavior on the Internet. It is a complicated and interesting problem combining data analysis and behavioral psychology. As an illustration of one of such tasks, Yandex solves the mailbox intruder detection problem based on the user\u2019s behavior patterns. In a nutshell, intruder\u2019s behavior patterns may differ from those of the mailbox owner:", "So the intruder could be detected and thrown out from the mailbox forcing the user to authenticate via SMS-code.", "Similar approaches are being developed in Google Analytics and described in scientific research papers. You can find more on this topic by searching \u201cTraversal Pattern Mining\u201d and \u201cSequential Pattern Mining\u201d.", "In this competition we are going to solve a similar problem: our algorithm is supposed to analyze the sequence of websites consequently visited by a particular person and predict whether this person is a user named Alice or an intruder (somebody else). As a metric, we will use ", ".", "Register on ", ", if you have not done it before. Go to the competition ", " and download the data.", "First, load the training and test sets. Then explore the data and perform a couple of simple exercises:", "The training dataset contains the following features:", "User sessions are chosen in such a way that they are not longer than half an hour and/or contain more than ten websites; i.e. a session is considered as ended either if the user has visited ten websites or if the session has lasted over thirty minutes.", "There are some empty values in the table, which means that some sessions contain less than ten websites. Replace empty values with 0 and change column types to integer. Also, load the website dictionary and see how it looks:", "In order to train our first model, we need to prepare the data. First of all, exclude the target variable from the training set. Now both training and test sets have the same number of columns, and we can aggregate them into one dataframe. Thus, all transformations will be performed simultaneously on both the training and test datasets.", "On the one hand, it leads to the fact that both of our datasets have one feature space (so you don\u2019t have to worry that you may have forgotten to transform a feature in one of the datasets). On the other hand, the processing time will increase. In case of enormously large sets, it may turn out that it is impossible to transform both datasets simultaneously (and sometimes you have to split your transformations into several stages, separately for the train/test dataset). In our case, we are going to perform all the transformations for the united dataframe at once; and, before training the model or making predictions, we will just use the corresponding part of it.", "For the sake of simplicity, we will use only the visited websites in the session (and we will not take into account the timestamp features). The point behind this data selection is: ", "Let\u2019s prepare the data. We will keep only the features ", " in the dataframe. Keep in mind that missing values were replaced with zeros. Here is how the first rows of the dataframe look like:", "Sessions are the sequences of website indices, and such representation of data is inconvenient for linear methods. According to our hypothesis (Alice has favorite websites), we need to transform this dataframe so that each website has the corresponding feature (column) which value is equal to the number of visits on this website within the session. All of this can be done in two lines:", "If you understand what just happened here, then you can skip the next section (perhaps, you can handle logistic regression too?). If not, then let us figure it out.", "Let\u2019s estimate how much memory it would require to store our data in the example above. Our united dataframe contains 336 thousand samples of 48 thousand integer features in each. It\u2019s easy to calculate the required amount of memory, roughly:", "336K * 48K * 8 bytes = 16M * 8 bytes = 128 GB", "Obviously, mere mortals don\u2019t have such volumes of memory (strictly speaking, Python may allow you to create such a matrix, but it would not be easy to do anything with it). An interesting fact is that most of the elements of our matrix are zeros. If we counted non-zero elements, then it would make out about 1.8 million, i.\u0435. slightly more than 10% of all matrix elements. Such a matrix, where most elements are zeros, is called ", ", and the ratio between the number of zero elements and the total number of elements is called the ", ".", "To work with such matrices, you can use ", " library, check the ", " to understand what possible types of sparse matrices are, how to work with them and in which cases their usage is most effective. You can learn how they are arranged, for example, be reading the Wikipedia ", ". Note that a sparse matrix contains only non-zero elements. Finally, you can get the allocated memory size (significant memory savings are obvious):", "Let\u2019s explore how the matrix with the websites has been formed using a mini example. Suppose we have the following table with user sessions:", "There are 3 sessions, and no more than 3 websites in each. Users visited four different sites in total (there are numbers from 1 to 4 in the table cells). And let us assume that:", "If the user has visited less than 3 websites during the session, the last few values will be zero. We want to convert the original dataframe in such a way that each session would have the corresponding row which shows the number of visits on each particular site; i.e. we want to transform the previous table into the following form:", "To do this, use the constructor: ", " and create a frequency table (see examples, code and comments on the links above to see how it works). Here, we set all the parameters explicitly for greater clarity:", "As you might have noticed, the number of the columns in the resulting matrix is not four (by the number of different websites), but five. A zero column has been added, which shows on how many sites the session was shorter (in our mini example we took sessions of length 3). This column is excessive and it should be removed from the dataframe.", "Another benefit of using sparse matrices is that there are special implementations of both matrix operations and machine learning algorithms for them, which sometimes allows to significantly accelerate operations due to the data structure peculiarities. This applies to logistic regression as well. Now, everything is ready to build our first model.", "Let\u2019s build our first model, using ", " implementation from ", " with default parameters. We will use the first 90% of the data for training (the training data set is sorted by time), and the remaining 10% for validation. Let's write a simple function that returns the quality of the model, and then train our first classifier:", "The first model demonstrated the quality of approximately 0.92 ROC AUC on the validation set. Let\u2019s take it as the first baseline and a starting point. To make a prediction on the test set, ", " (until this moment, our model used only part of the data for training), which will increase its generalizing ability:", "If you follow these steps and upload the answer to the competition ", ", then you should get the quality of ", " on the public leaderboard.", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:03:25.487Z"}
{"nameOfPublication": "Open Machine Learning Course", "nameOfAuthor": "Sergey Korolev", "articleTile": "Open Machine Learning Course. Topic 7. Unsupervised Learning: PCA and Clustering", "content": ["Welcome to the seventh part of our Open Machine Learning Course!", "In this lesson, we will work with unsupervised learning methods such as Principal Component Analysis (PCA) and clustering. You will learn why and how we can reduce the dimensionality of the original data and what the main approaches are for grouping similar data points.", "3. Cluster analysis", "4. Assignment #7", "5. Useful links", "The main feature of unsupervised learning algorithms, when compared to classification and regression methods, is that input data are unlabeled (i.e. no labels or classes given) and that the algorithm learns the structure of the data without any assistance. This creates two main differences. First, it allows us to process large amounts of data because the data does not need to be manually labeled. Second, it is difficult to evaluate the quality of an unsupervised algorithm due to the absence of an explicit goodness metric as used in supervised learning.", "One of the most common tasks in unsupervised learning is dimensionality reduction. On one hand, dimensionality reduction may help with data visualization (e.g. t-SNA method) while, on the other hand, it may help deal with the multicollinearity of your data and prepare the data for a supervised learning method (e.g. decision trees).", "Principal Component Analysis is one of the easiest, most intuitive, and most frequently used methods for dimensionality reduction, projecting data onto its orthogonal feature subspace.", "More generally speaking, all observations can be considered as an ellipsoid in a subspace of an initial feature space, and the new basis set in this subspace is aligned with the ellipsoid axes. This assumption lets us remove highly correlated features since basis set vectors are orthogonal. In the general case, the resulting ellipsoid dimensionality matches the initial space dimensionality, but the assumption that our data lies in a subspace with a smaller dimension allows us to cut off the \u201cexcessive\u201d space with the new projection (subspace). We accomplish this in a \u2018greedy\u2019 fashion, sequentially selecting each of the ellipsoid axes by identifying where the dispersion is maximal.", "Let\u2019s take a look at the mathematical formulation of this process:", "In order to decrease the dimensionality of our data from n to k with k \u2264 n, we sort our list of axes in order of decreasing dispersion and take the top-k of them.", "We begin by computing the dispersion and the covariance of the initial features. This is usually done with the covariance matrix. According to the covariance definition, the covariance of two features is computed as follows:", "where \u00b5 is the expected value of the i-th feature. It is worth noting that the covariance is symmetric, and the covariance of a vector with itself is equal to its dispersion.", "Therefore the covariance matrix is symmetric with the dispersion of the corresponding features on the diagonal. Non-diagonal values are the covariances of the corresponding pair of features. In terms of matrices where X is the matrix of observations, the covariance matrix is as follows:", "Quick recap: matrices, as linear operators, have eigenvalues and eigenvectors. They are very convenient because they describe parts of our space that do not rotate and only stretch when we apply linear operators on them; eigenvectors remain in the same direction but are stretched by a corresponding eigenvalue. Formally, a matrix M with eigenvector w and eigenvalue \u03bb satisfy this equation:", "The covariance matrix for a sample X can be written as a product of a transposed matrix X and X itself. According to the ", ", the maximum variation of our sample lies along the eigenvector of this matrix and is consistent with the maximum eigenvalue. Therefore, the principal components we aim to retain from the data are just the eigenvectors corresponding to the top-k largest eigenvalues of the matrix.", "The next steps are easier to digest. We multiply the matrix of our data X by these components to get the projection of our data onto the orthogonal basis of the chosen components. If the number of components was smaller than the initial space dimensionality, remember that we will lose some information upon applying this transformation.", "Let\u2019s start by uploading all of the essential modules and try out the iris example from the ", " documentation.", "Now let\u2019s see how PCA will improve the results of a simple model that is not able to correctly fit all of the training data:", "Let\u2019s try this again, but, this time, let\u2019s reduce the dimensionality to 2 dimensions:", "The accuracy did not increase significantly in this case, but, with other datasets with a high number of dimensions, PCA can drastically improve the accuracy of decision trees and other ensemble methods.", "Now let\u2019s check out the percent of variance that can be explained by each of the selected components.", "Let\u2019s look at the handwritten numbers dataset that we used before in the ", ".", "Let\u2019s start by visualizing our data. Fetch the first 10 numbers. The numbers are represented by 8 x 8 matrixes with the color intensity for each pixel. Every matrix is flattened into a vector of 64 numbers, so we get the feature version of the data.", "Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters.", "Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA.", "In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the ", "). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21.", "The main idea behind clustering is pretty straightforward. Basically, we say to ourselves, \u201cI have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group.\u201d This general idea encourages exploration and opens up a variety of algorithms for clustering.", "The algorithms listed below do not cover all the clustering methods out there, but they are the most commonly used ones.", "K-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:", "This algorithm is easy to describe and visualize. Let\u2019s take a look.", "Here, we used Euclidean distance, but the algorithm will converge with any other metric. You can not only vary the number of steps or the convergence criteria but also the distance measure between the points and cluster centroids.", "Another \u201cfeature\u201d of this algorithm is its sensitivity to the initial positions of the cluster centroids. You can run the algorithm several times and then average all the centroid results.", "In contrast to the supervised learning tasks such as classification and regression, clustering requires more effort to choose the optimization criterion. Usually, when working with k-means, we optimize the sum of squared distances between the observations and their centroids.", "where C \u2014 is a set of clusters with power K, \u00b5 is a centroid of a cluster.", "This definition seems reasonable \u2014 we want our observations to be as close to their centroids as possible. But, there is a problem \u2014 the optimum is reached when the number of centroids is equal to the number of observations, so you would end up with every single observation as its own separate cluster.", "In order to avoid that case, we should choose a number of clusters after which a function J(C) is decreasing less rapidly. More formally,", "Let\u2019s look at an example.", "We see that J(C) decreases significantly until the number of clusters is 3 and then does not change as much anymore. This means that the optimal number of clusters is 3.", "Inherently, K-means is NP-hard. For d dimensions, k clusters, and n observations, we will find a solution in O(n^(dk+1)) in time. There are some heuristics to deal with this; an example is MiniBatch K-means, which takes portions (batches) of data instead of fitting the whole dataset and then moves centroids by taking the average of the previous steps. Compare the implementation of K-means and MiniBatch K-means in the ", ".", "The ", " of the algorithm using ", " has its benefits such as the possibility to state the number of initializations with the ", " function parameter, which enables us to identify more robust centroids. Moreover, these runs can be done in parallel to decrease the computation time.", "Affinity propagation is another example of a clustering algorithm. As opposed to K-means, this approach does not require us to set the number of clusters beforehand. The main idea here is that we would like to cluster our data based on the similarity of the observations (or how they \u201ccorrespond\u201d to each other).", "Let\u2019s define a similarity metric such that s(x, y) > s(x, z) if an observation x is more similar to observation y and less similar to observation z. A simple example of such a similarity metric is a negative square of distance s(x, y) = -||x \u2014 y||\u00b2.", "Now, let\u2019s describe \u201ccorrespondence\u201d by making two zero matrices. One of them, r, determines how well the k-th observation is as a \u201crole model\u201d for the i-th observation with respect to all other possible \u201crole models\u201d. Another matrix, a determines how appropriate it would be for i-th observation to take the k-th observation as a \u201crole model\u201d. This may sound confusing, but it becomes more understandable with some hands-on practice.", "The matrices are updated sequentially with the following rules:", "Spectral clustering combines some of the approaches described above to create a stronger clustering method.", "First of all, this algorithm requires us to define the similarity matrix for observations called the adjacency matrix. This can be done in a similar fashion as in the Affinity Propagation algorithm, so that matrix A hosts negative square of the distances between the corresponding points. This matrix describes a full graph with the observations as vertices and the estimated similarity value between a pair of observations as edge weights for that pair of vertices. For the metric defined above and two-dimensional observations, this is pretty intuitive \u2014 two observations are similar if the edge between them is shorter. We\u2019d like to split up the graph into two subgraphs in such a way that each observation in each subgraph would be similar to another observation in that subgraph. Formally, this is a Normalized cuts problem; for more details, we recommend reading ", ".", "The following algorithm is the simplest and easiest to understand among all the the clustering algorithms without a fixed number of clusters.", "The algorithm is fairly simple:", "The process of searching for the nearest cluster can be conducted with different methods of bounding the observations:", "The 3rd one is the most effective in computation time since it does not require recomputing the distances every time the clusters are merged.", "The results can be visualized as a beautiful cluster tree (dendogram) to help recognize the moment the algorithm should be stopped to get optimal results. There are plenty of Python tools to build these dendograms for agglomerative clustering.", "Let\u2019s consider an example with the clusters we got from K-means:", "As opposed to classfication, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.", "There are ", " and ", " goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics.", "All the metrics described below are implemented in ", ".", "Here, we assume that the true labels of objects are known. This metric does not depend on the labels\u2019 values but on the data cluster split. Let N be the number of observations in a sample. Let a to be the number of observation pairs with the same labels and located in the same cluster, and let b to be the number of observations with different labels and located in different clusters. The Rand Index can be calculated using the following formula: RI = 2(a + b)/n(n \u2014 1) In other words, it evaluates a share of observations for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any n and number of clusters, it is essential to scale it, hence the Adjusted Rand Index: ARI = (RI \u2014 E(RI))/(max(RI) \u2014 E(RI)).", "This metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. ARI takes on values in the [-1, 1] range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match ARI = 1).", "This metric is similar to ARI. It is also symmetric and does not depend on the labels\u2019 values and permutation. It is defined by the [entropy](", " function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The MI index is defined as the ", " for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.", "Similarly to the ARI, the AMI is defined. This allows us to get rid of the MI index\u2019s increase with the number of clusters. The AMI lies in the [0, 1] range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at AMI = 1).", "Formally, these metrics are also defined based on the entropy function and the conditional entropy function, interpreting the sample splits as discrete distributions:", "where K is a clustering result and C is the initial split. Therefore, h evaluates whether each cluster is composed of same class objects, and c measures how well the same class objects fit the clusters. These metrics are not symmetric. Both lie in the [0, 1] range, and values closer to 1 indicate more accurate clustering results. These metrics\u2019 values are not scaled as the ARI or AMI metrics are and thus depend on the number of clusters. A random clustering result will not have metrics\u2019 values closer to zero when the number of clusters is big enough and the number of objects is small. In such a case, it would be more reasonable to use ARI. However, with a large number of observations (more than 100) and the number of clusters less than 10, this issue is less critical and can be ignored.", "V-measure is a combination of h, and c and is their harmonic mean: v = (2hc)/(h + c). It is symmetric and measures how consistent two clustering results are.", "In contrast to the metrics described above, this coefficient does not imply the knowledge about the true labels of the objects. It lets us estimate the quality of the clustering using only the initial, unlabeled sample and the clustering result. To start with, for each observation, the silhouette coefficient is computed. Let a be the mean of the distance between an object and other objects within one cluster and b be the mean distance from an object to an object from the nearest cluster (different from the one the object belongs to). Then the silhouette measure for this object is s = (b \u2014 a)/max(a, b).", "The silhouette of a sample is a mean value of silhouette values from this sample. Therefore, the silhouette distance shows to which extent the distance between the objects of the same class differ from the mean distance between the objects from different clusters. This coefficient takes values in the [-1, 1] range. Values close to -1 correspond to bad clustering results while values closer to 1 correspond to dense, well-defined clusters. Therefore, the higher the silhouette value is, the better the results from clustering.", "With the help of silhouette, we can identify the optimal number of clusters k (if we don\u2019t know it already from the data) by taking the number of clusters that maximizes the silhouette coefficient.", "To conclude, let\u2019s take a look at how these metrics perform with the MNIST handwritten numbers dataset:", "Full versions of assignments are announced each week in a new run of the course (October 1, 2018). Meanwhile, you can practice with a demo version: ", ", ", ".", "Written by"], "postingTime": "2018-09-19T09:10:07.102Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "Aleksandr Movchan", "articleTile": "How to Apply Distance Metric Learning to Street-to-Shop Problem", "content": ["Let\u2019s start with a definition of street-to-shop problem \u2014 identifying a fashion item in a user image and finding it in an online shop. Have you ever seen somebody in the street and thought \u201cWow, this is a nice dress, I wonder where I can buy it?\u201d I haven\u2019t. But for me, it was the cool task to try distance metric learning techniques. I hope that you will find it interesting too.", "Firstly, we need a dataset for it. Actually, I came to this idea after I found out that there are tons of images taken by users on ", ". And I thought \u201cWow, I can make a search by image using this data, just for fun of course\u201d. I have decided to focus on women\u2019s top clothing for simplicity.", "Below is the list of the categories I used for scrapping:", "I used ", " and ", " for scrapping. Seller images can be obtained from the main page of the item, but for user\u2019s images, we need to go through feedback pages. There is a thing called \u201ccolors\u201d on item\u2019s page. Color can be just item of another color or even completely other items. So we will consider different colors as different items.", "You can find the code that I have used to get all information about one item (it scraps even more than we need for our task) by link ", ".", "All we need is to go through search pages by each category, take URLs of all items and use the function above to get the info about each item.", "Finally, we will have two sets of images for each item: images from a seller ( field ", " for each element in", ") and images from users (field ", " for each element in", ").", "For each color, we have only one image from a seller, but it can be more than one image for each color from users (sometimes there are no images for color at all).", "Great! We got data. However, the collected dataset is noisy:", "To mitigate this problem I have labeled 5000 images into two categories: good images and noise images. In the beginning, my plan was to train a classifier for two categories and use it to clean dataset. But later I decided to leave this idea for future work and just added cleaned images to the test and validation sets.", "One of the most popular distance metric learning method is the triplet loss:", "where ", " is the hinge function, ", " is the distance function between ", " and ", " is deep neural network, ", " is the margin, ", " is the anchor, ", " is the positive point, ", " is the negative point.", " are points in high dimensional space (embeddings) produced by a deep neural network. It is worth mentioning that the embeddings often needs to be normalized to have unit length, i.e., ", ", in order to be robust to illumination and contrast changes and for training stability. The anchor and the positive samples belong to the same class, the negative sample is the instance of another class.", "So the main idea of the triplet loss is to separate embeddings of the positive pair (anchor and positive) from embeddings of the negative pair (anchor and negative) by a distance margin ", ".", "But how to select the triplet (", ", ", "? We can just randomly select samples as a triplet but it causes following problems. Firstly, there are N\u00b3 possible triplets. It means that we need a lot of time to go through all possible triplets. But actually, we don\u2019t need to do it, because after few iterations of training there will be many triplets which don\u2019t violate the triplet constraint (give zero loss). It means that these triplets are useless for a training.", "One of the most common way of triplet selection is hard negative mining:", "Selecting the hardest negatives can in practice lead to bad local minima early on in training. Specifically, it can result in a collapsed model (i.e. ", "). In order to mitigate this we can use semi-hard negative mining.", "Semi-hard negative samples are further away from the anchor than the positive sample but they are still hard (violate triplet constraint) because they lie inside the margin M.", "There are two way to generate semi-hard (and hard) negative samples: online and offline.", "Good! We already can start train the model with the triplet loss and offline semi-hard negative mining. But! There is always a \u201cbut\u201d in this imperfect world. We need one more trick to successfully solve street-to-shop problem. Our task is to find seller\u2019s image most similar to user\u2019s image. However, usually seller\u2019s images have much better quality (in terms on lighting, camera, position) than user\u2019s images so we have two domains: seller\u2019s images and user\u2019s images. In order to get efficient model we need to reduce a gap between these two domains. This problem is called domain adaptation.", "I propose a really simple technique to reduce domain gap: let\u2019s select anchors from seller\u2019s images, positive and negative samples from user\u2019s images. That\u2019s all! Simple yet effective.", "To implement my ideas and to do fast experimenting I have used ", " library with Tensorflow backend.", "I chose Inception V3 model as base CNN for my model. As usual, I initialized CNN with ImageNet weights. I have added two fully connected layers after global pooling with L2-normalization at the end of the network. The size of embedding is 128.", "We also need to implement the triple loss function. We pass the anchor, the positive/negative samples as single mini-batch and divide it into 3 tensors inside the loss function. The distance function is squared euclidean distance.", "And compile model:", "Performance is measured in terms of recall at K (R@K).", "Let\u2019s take a look how to calculate R@K. Each user\u2019s image from validation set was used as a query and we need to find the corresponding seller\u2019s image. We take one query image, calculate embedding vector and search nearest neighbors of this vector among vectors of all seller\u2019s images. We use not only seller\u2019s images from the validation set but images from the train set too because it allows to increase the number of distractors and makes our task more challenging.", "So we have a query image and a list of the most similar seller\u2019s images. If there is a corresponding seller image in the K most similar images then we return 1 for this query else return 0. Now we need to make it for each user\u2019s image in the validation set and find an average of scores from each query. It will be R@K.", "As I said before I have cleaned the small amount of user\u2019s images from noisy images. So I have measured a quality of the model on two validation datasets: full validation set and a subset of only clean images.", "Results are far from ideal, there are many things to do:", "I have made a demo of the model. You can check it out here: ", ". You can upload your own image for search or use random image from validation set.", "Code and trained model: ", "Thanks for reading. If you\u2019re enjoying the article, please let me know by clapping. If you want more information, you can connect with me on ", ".", "Written by"], "postingTime": "2018-01-18T17:00:53.331Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "Alibaba Tech", "articleTile": "Helping AI Uncover the Mysterious Veil of Chinese Characters", "content": ["The field of natural language processing (NLP) has recently seen an increasing amount of attention given to word representation learning. Finding a way for AI to analyze text and identify semantically related words holds huge potential for downstream applications \u2014 but it is especially complicated for vast and complex scripts like Chinese.", "Chinese is an ancient language that fascinates people the world over, with millions studying it as a second or third language. Famously difficult to master, Chinese and its varieties, including Mandarin and Cantonese, use logographic scripts that differ vastly from alphabetic scripts like English. For instance, while letters in an alphabetic script represent the language at the phonetic level, Chinese characters represent the language at a semantic level \u2014 not at the word level, however, but at the level of the morpheme.", "The Alibaba tech team, in collaboration with the Singapore University of Technology and Design, have proposed a model called stroke ", "-grams for capturing and codifying Chinese semantics. The \u201cstroke\u201d in stroke ", "-grams refers to the fact that the system draws on Chinese handwriting conventions to identify semantically relevant graphic elements within a word.", "Unlike characters, radicals and components, strokes are not semantic elements of the script. However, stroke ", "-grams use stroke combinations and recurring stroke sequences between words to identify semantic structures within words.", "To explain why stroke ", "-grams are more effective than other approaches, let\u2019s consider the drawbacks of those other approaches first (analysis by character, radical, and component).", "Chinese characters are a useful point of reference for tracing the history and development of the Chinese language and script \u2014 but they offer little utility in indicating which words are semantically related. Simply put, there are far more words in Chinese that share semantic information than those which share one or more characters.", "For example, the Chinese words for \u2018timber\u2019 and \u2018forest\u2019 share semantic roots, but a character-level analysis gives no indication that this is the case. This makes considering only character-level information erroneous and superficial.", "Meanwhile, it is instantly apparent to anyone familiar with the Chinese script that the words timber and forest are related, even if they do not know the words in question. This is because the characters in both words share the common graphic element, wood \u201c\u6728\u201d.", "Radicals have stood the test of time in terms of providing a means of organizing Chinese characters in dictionaries, and in some cases they do provide useful semantic information \u2014 timber and forest being prime examples. However, there are many instances where radicals are wholly incapable of identifying semantic information in a word.", "For example, the radical in the character for wisdom \u201c\u667a\u201d is sun \u201c\u65e5\u201d. Even after studying the historical justification for this radical, it is difficult to claim a credible semantic connection between sun and wisdom.", "Unfortunately, looking beyond radicals to other components \u2014 defined as fundamental graphic elements at the same level of complexity as radicals \u2014 is ultimately a wasted effort. While the timber-forest example suggests that component analysis should offer fruitful results, this is not true in all cases. To revisit the example above, the character for wisdom contains the additional basic components arrow \u201c\u5931\u201d, and mouth \u201c\u53e3\u201d in addition to the sun \u201c\u65e5\u201d component used as the radical.", "Yet once again, anyone familiar with the Chinese script can instantly recognize that the words for \u201cwisdom\u201d and \u201cknowledge\u201d are semantically related, despite them sharing no common characters, radicals, or \u201ccomponents\u201d in the defined sense.", "The character for knowledge \u201c\u77e5\u201d appears as a sub-word graphic structure in the character for wisdom \u201c\u667a\u201d. However, because it does not constitute a character, radical or component, none of the traditional means of classifying Chinese characters are able to produce a system that identifies this as a shared element. Meanwhile, attempting to identify and codify all graphic elements between the component and character levels that convey semantic information would be a monumental manual undertaking.", "So how do stroke \u00ad", "-grams provide a minimalist solution that still ensures this information is systematically identified and stored?", "Stroke ", "-grams rely on the fact that handwritten Chinese characters are always a combination of five basic stroke types, and that characters are always written from top to bottom, left to right, one component at a time.", "To revisit the wisdom-knowledge example, this means that the sub-word structure knowledge \u201c\u77e5\u201d would be written in the same sequence in both cases. By giving each stroke type a number and then representing a combination of strokes with a numerical sequence, a system could identify the same sequence occurring in different contexts. This is why stroke n-grams are capable of capturing morphological and semantic information that is shared between words, even though a stroke by itself conveys no semantic information.", "Chinese words are mapped into stroke n-grams using the following process:", "1. Words (comprising one or more Chinese characters) are divided into their constituent characters.", "2. The stroke sequence for each character is retrieved and concatenated together.", "3. Stroke sequences are designated stroke IDs.", "4. A slide window of size n is imposed to generate stroke n-grams.", "As shown in the above example for the word adult \u201c\u5927\u4eba\u201d the stroke ID is a 5-gram that captures the stroke sequence for the entire word, while 3-gram and 4-gram ", "-grams capture stroke sequences for sub-word graphic components.", "Word embedding, also known as word vectors, help computers comprehend words. First introduced by Google, the model maps a word\u2019s semantic meaning into a low-dimensional vector space. Through this method, synonyms are identified by the measure of distance between two corresponding vectors.", "To incorporate stroke ", "-grams into the function of learning word embeddings, the research team specifically designed a simple yet effective mathematical model that helps computers learn Chinese-style word embeddings. The novel algorithm developed by the research team outperformed Google\u2019s word2vec, Stanford\u2019s GloVe and Tsinghua\u2019s CWE among others in the public test datasets, and yielded better results for several Alibaba and Ant Financial tasks.", "First-hand and in-depth information about Alibaba\u2019s latest technology \u2192 Search ", " on ", "Written by"], "postingTime": "2018-06-13T17:11:59.390Z"}
{"nameOfPublication": "Applied Data Science", "nameOfAuthor": "David Foster", "articleTile": "Hallucinogenic Deep Reinforcement Learning Using Python and Keras", "content": ["If Artificial Intelligence is your thing, you need to check this out:", "In short, it\u2019s a masterpiece, for three reasons:", ".", "We\u2019ll cover the technical details and also walk through how you can get a version running on your own machine.", "We\u2019re going to build a reinforcement learning algorithm (an \u2018agent\u2019) that gets good at driving a car around a 2D racetrack. This environment (Car Racing) is available through the ", "At each time-step, the algorithm is fed an observation (a 64 x 64 pixel colour image of the car and immediate surroundings) and needs to return the next set of actions to take \u2014 specifically, the steering direction (-1 to 1), acceleration (0 to 1) and brake (0 to 1).", "This action is then passed to the environment, which returns the next observation and the cycle starts again.", "An agent scores 1000/N for each of the N track tiles visited and -0.1 for each time-step taken. For example, if the agent completes the track in 732 frames, the reward is 1000\u20130.1*732 = 926.8 points.", "Here\u2019s an example of an agent that chooses the action [0,1 0] for the first 200 time-steps then something random\u2026not a great driving strategy.", "The aim is to train the agent to understand that it can use information from its surroundings to inform the next best action.", "There is an excellent online ", ", written by the authors, so I won\u2019t go into the same level of detail here, but instead will focus on a high-level summary of how the pieces fit together, with an analogy to real driving to explain why the solution intuitively makes sense.", "The solution consists of three distinct parts, which are trained separately:", "When you make decisions whilst driving, you don\u2019t actively analyse every single \u2018pixel\u2019 in your view \u2014 instead your brain condenses the visual information into a smaller number of \u2018latent\u2019 entities, such as the straightness of the road, upcoming bends and your position relative to the road, to inform your next action.", " \u2014 condense the 64x64x3 (RGB) input image into a 32-dimensional latent vector (z) that follows a Gaussian distribution.", "This is useful because the agent can now work with a much smaller representation of its surroundings and therefore can be more efficient in its learning.", "If you didn\u2019t have an MDN-RNN component to your decision making, your driving might look something like this.", "As you drive, each subsequent observation isn\u2019t a complete surprise to you. You know that if the current observation suggests a left turn in the road and you turn the wheel left, you expect the next observation to show that you are still in line with the road.", " \u2014 specifically this a Long Short-Term Memory Network (LSTM) with 256 hidden units. The vector of hidden states is represented by h.", "Similarly to the VAE, the RNN tries to capture a latent understanding of the current state of the car in its environment, but this time with the aim of predicting what the next \u2018z\u2019 might look like, based on the previous \u2018z\u2019 and the previous action.", "The MDN output layer simply allows for the fact that the next \u2018z\u2019 could actually be drawn from any one of several Gaussian distributions.", "The same technique was applied in ", " article, by the same author, for handwriting generation, to describe the fact that the next pen point could land in any one of the red distinct areas.", "Similarly, in the World Models paper, the next observed latent state could be drawn from any one of five Gaussian distributions.", "Up until this point, we haven\u2019t mentioned anything about choosing an action. That responsibility lies with the Controller.", "The Controller is simply a ", ", where the input is a concatenation of z (the current latent state from the VAE \u2014 length 32) and h (the hidden state of the RNN \u2014 length 256). The 3 output neurons correspond to the three actions and are scaled to fall in the appropriate ranges.", "To understand the different roles of the three components and how they work together, we can imagine a dialogue between them:", "VAE: (looks at latest 64*64*3 observation) This looks like a straight road, with a slight left bend approaching, with the car facing in the direction of the road (z).", "RNN: Based on that description (z) and the fact that the Controller chose to accelerate hard at the last time-step (action), I will update my hidden state (h) so that the next observation is predicted to still be a straight road, but with slightly more left turn in view.", "Controller: Based on the description from the VAE (z) and the current hidden state from the RNN (h) my neural network outputs next action to be [0.34, 0.8, 0].", "This action is then passed to the environment, which returns an updated observation and the cycle begins again.", "We\u2019ll now look at how to set up an environment that allows you to train your own version of the agent for car racing.", "Time for some code!", "If you\u2019ve got a high-spec laptop, you can run the solution locally, but I\u2019d recommend using ", " for access to powerful machines that you can use in short bursts.", "The following has been tested on Linux (Ubuntu 16.04) \u2014 just change the relevant commands for package installation if you\u2019re on Mac or Windows.", "In the command line, navigate to the place you want to store the repository and enter the following:", "The repository is adapted from the highly useful ", " library developed by David Ha, the first author of the World Models paper.", "For the neural network training, this implementation uses ", " with a ", " backend, though in the original paper the authors used raw Tensorflow.", "Create yourself a Python 3 virtual environment (I use virutalenv and virtualenvwrapper)", "There are more here than required by the Car Racing example, but you\u2019ll have everything installed in case you want to test out some of the other environments in Open AI gym, that require the additional packages.", "For the Car Racing environment, both the VAE and RNN can be on ", " rollout data \u2014 that is, observation data generated by randomly taking actions at each time-step. Actually, we use pseudo-random actions, which forces the car to accelerate initially, in order to get it off the start line.", "Since the VAE and RNN are independent of the decision-making Controller, all we need to ensure is that we encounter a diverse range of observations and choose a diverse range of actions to save as training data.", "To generate the random rollouts, run the following from the command line", "or if you\u2019re on a server without a display,", "This will produce 2000 rollouts (saved in ten batches of 200), starting with batch number 0. Each rollout will be a maximum of 300 time-steps long", "Two sets of files are saved in ", ", (* is the batch number)", " (stores the 64*64*3 images as numpy arrays)", "(stores the 3 dimensional actions)", "Training the VAE only requires the ", " files. Make sure you\u2019ve completed Step 4, so that these files exist in the ", " folder.", "From the command line, run:", "This will train a new VAE on each batch of data from 0 to 9.", "The model weights will be saved to ", ". The ", " flag tells the script to train the model from scratch.", "If there is an existing ", " in this folder and the ", " flag is not specified, the script will load the weights from this file and continue training the existing model. This way, you can iteratively train your VAE in batches, rather than all in one go.", "The VAE architecture specification in the ", " file.", "Now that we have a trained VAE, we can use it to generate the training set for the RNN.", "The RNN requires encoded image data (z) from the VAE and actions (a) as input and one time-step ahead encoded image data from the VAE as output.", "You can generate this data by running:", "This will take the ", " and ", " files from batches 0 to 9 and convert them to the correct format required by the RNN for training.", "Two sets of files will be saved in ", ", (* is the batch number)", " (stores the [z a] concatenated vectors)", "(stores the z vector one time-step ahead)", "Training the RNN only requires the ", " and ", "files. Make sure you\u2019ve completed Step 6, so that these files exist in the ", " folder.", "From the command line, run:", "This will train a new RNN on each batch of data from 0 to 9.", "The model weights will be saved to ", ". The ", " flag tells the script to train the model from scratch.", "Similarly to the VAE, if there is an existing ", " in this folder and the ", " flag is not specified, the script will load the weights from this file and continue training the existing model. This way, you can iteratively train your RNN in batches, rather than all in one go.", "The RNN architecture specification is in the ", " file.", "Now for the fun part!", "So far, we\u2019ve just used deep learning to build a VAE that can condense high dimension images down to a low dimensional latent space and an RNN that can predict how the latent space will evolve over time. This was possible because we were able to create a training set for each, using random rollout data.", "To train the controller, we\u2019ll use a form of reinforcement learning, that utilises an evolutionary algorithm known called ", ".", "Since the input is a vector of dimension 288 (= 32 + 256) and the output a vector of dimension 3, we have 288 * 3 + 1 (bias) = 867 parameters to train.", "CMA-ES works by first creating multiple randomly initialised copies of the 867 parameters (the \u2018population\u2019). It then tests each member of the population inside the environment and records its average score. In exactly the same principle as natural selection, the weights that generate the highest scores are allowed to \u2018reproduce\u2019 and spawn the next generation.", "To start this process on your machine, run the following command, with the appropriate values for the arguments", "or on a server without display:", " : set this to no more than number of cores available", " : the number of members of the population that each worker will test (", " gives the total population size for each generation)", " : the number of episodes each member of the population will be scored against (i.e. the score will be the average reward across this number of episodes)", " : the maximum number of time-steps in an episode", ": the number of generations between the evaluation of the best set of weights, across 100 episodes", " By default, the controller will start from scratch each time it is run and save the current state of the process to a pickle file in the ", " directory. This argument allows you to continue training from the last save point, by pointing it at the relevant file.", "After each generation, the current state of the algorithm and the best set of weights will be output to the ", " folder.", "At the point of writing, I\u2019ve managed to train an agent to achieve an average score of ", " after 200 generations of training. This was trained on Google Cloud using an Ubuntu 16.04, 18 vCPU, 67.5GB RAM machine with the steps and parameters given in this tutorial.", "The authors of the paper managed to achieve an average score of ", ", after 2000 generations of training, which is believed to be the highest score in this environment to date. This utilised a slightly higher spec set-up (e.g. 10,000 episodes of training data, 64 population size, 64 core machine, 16 episodes per trial etc.)", "To visualise the current state of your Controller, simply run:", " : the path to the json of weights that you want to attach to the controller", " : render the environment on your screen", " : outputs mp4 files into the ", " folder, showing each episode", " : run a 100 episode test of your controller and output the average score.", "Here\u2019s a demo!", "That\u2019s already pretty cool \u2014 but the next part of the paper is mind-blowingly impressive and I think has major implications for AI.", "The paper goes on to show an amazing result, through another environment, ", ". The object here is to move an agent to avoid fireballs and stay alive as long as possible.", "The authors show how it is possible for the agent to actually ", ", rather than inside the environment itself.", "The only required addition is that the RNN is trained to also predict the probability of being killed in the next time-step. This way, the VAE / RNN combination can be wrapped up as an environment in its own right and used to train the Controller. This is the very concept of a \u2018", "\u2019.", "We could summarise the hallucinogenic learning as follows:", "The agent\u2019s initial training data is nothing more than random interactions with the real environment. Through this, it builds up a latent understanding of how the world \u2018works\u2019 \u2014 its natural groupings, physics and how its own actions affect the state of the world.", "It can then use this understanding to establish an optimal strategy for a given task, without ever having to actually test it in the real world, because it can use its own mental model of the environment as the \u2018playground\u2019 for trying things out.", "This could easily be a description of a baby learning to walk. There are striking similarities that perhaps run deeper than mere analogy, making this a truly fascinating area of research.", "Hopefully you find this article useful \u2014 let me know in the comments below if you find any typos or have questions about anything in the codebase or article and I\u2019ll get back to you as soon as possible.", "If you would like to learn more about how our company, ", " develops innovative data science solutions for businesses, feel free to get in touch through our ", " or directly through ", ".", "Written by"], "postingTime": "2019-07-30T16:37:00.279Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "Faizan Patel", "articleTile": "Simple guide to Neural Arithmetic Logic Units (NALU): Explanation, Intuition and Code", "content": ["he research engineers at DeepMind including well known AI researcher and author of the book ", ", Andrew Trask have published an impressive paper on a neural network model that can learn simple to complex numerical functions with great extrapolation (generalisation) ability.", "In this post I will explain NALU, its architecture, its components and significance over traditional neural networks. The primary intention behind this post is to provide ", " explanation of NALU (both with concepts and code) which can be comprehended by researchers, engineers and students who have a limited knowledge of neural networks and deep learning.", ": I strongly recommend readers to read the original paper for more detailed understanding of the subject. The paper can be downloaded from ", ".", "eural networks, in theory, are very good function approximators. They can almost always learn any meaningful relationship between inputs (data or features) and outputs (labels or targets). Hence, they are being used in a wide variety of applications from object detection and classification to speech to text conversion to intelligent game-playing agents that can beat human world champion players. There are many effective neural network models which satisfied various need of such applications such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Autoencoders etc. Advances in deep learning and neural network models is another topic in itself.", "However, according to the authors of the paper, they ", " very basic ability which seems trivial for a ", " or for even ", "That", "is the ability to count/manipulate numbers and also, to extrapolate the numerical relationship from an observable numeric pattern. In the paper, it is shown that the standard neural networks even struggles to learn even an ", " ( a function whose input and output is identical; f(x) = x) which is the most straightforward numeric relationship. Below image shows the mean square error (MSE) of various NNs trained to learn such an identity function.", "The primary reason for NNs to fail to learn such numerical representation is the ", " in hidden layers of the network. Such activation functions are crucial to learn the abstract non linear relationship between inputs and labels but they fail miserably when it comes to learn the numerical representation outside the range of the numbers seen in the training data. Hence, such networks are very good to ", " the numerical pattern seen in the training set but fail to extrapolate this representation well.", "It is like memorizing an answer or a topic without understanding the underlying concept for the exam. Doing so, one can perform very well if the similar questions are asked in the exam, however, would fail in the case of twisted questions are asked designed to test the knowledge and not the memorization ability of a candidate.", "The severity of this failure directly corresponds to the degree of non-linearity within the chosen activation function. From the above image, it can be clearly seen that the hard-constrained non-linear functions such as ", " and ", " have very less ability to generalize well than the soft-constrained non-linear function such as PReLU and ELU.", "he neural accumulator (NAC) forms the base to the NALU model. NAC is a simple but effective neural network model (unit) which supports the ability to learn ", "\u2014 which is a desirable property to learn linear functions effectively.", "AC is a special layer of linearity whose weight parameters have the restrictions of having the only values ", " By constraining the weight values in such a manner prevents the layer from changing the scale of the input data and they remain consistent throughout the network no matter how many layers are stacked together. Hence, the output will be the ", " of input vector which can easily represent addition and subtraction operations.", " To understand this fact, let us consider the following examples of NN layers which performs the linear arithmetic operation on inputs.", "As shown in above NN layers, network can learn to extrapolate simple arithmetic functions like addition and subtraction ( y= x1 + x2 and y = x1\u2014x2) by restricting the weight parameters to -1, 0 and 1.", "Since the constraint on the weight parameters in NAC is hard to learn by standard neural network, authors have described very useful formula to learn such restricted parameter values using standard (unconstrained) parameters W_hat and M_hat. These parameters are like any standard NN weight parameters which can be initialized randomly and can be learnt over the course of training process. The formula to obtain W in terms of W_hat and M_hat is given below:", "Using above equation to calculate the weight parameters in the network, ", "that the value of such parameters will be in the range of [-1,1] with more inclined towards -1, 0 and 1. Also, this equation is a ", " equation (whose derivatives can be computed with respect to weight parameters). Hence, it will be easier for NAC to learn W using ", ". Below is the architectural diagram of a NAC unit.", "As one can imagine, NAC is a simple NN model with few little tweaks. Below I have shown the simple implementation of a single NAC layer in python using Tensorflow and Numpy library.", "In the above code, I used random uniform initialization for trainable parameters W_hat and M_hat but one can use ", " recommended weight initialization technique for these parameters. For full working code kindly checkout my ", " mentioned at the end of this post.", "Though the above mentioned simple neural network model is able to learn basic arithmetic functions like the addition and subtraction, it is desirable to have the ability to learn more complex arithmetic operations such as multiplication, division and power functions.", "Below is the modified architecture of NAC that is able to learn more ", " using the ", "logarithmic values and exponents", "for its weight parameters. Observe that how this NAC is different than the one mentioned first in the post.", "As shown in the diagram above, this cell applies the log function to the input data before matrix multiplication with weight matrix W and then it applies an exponential function on the resultant matrix. The formula to obtain output is given in the below equation.", "Hence, everything is same in terms of the underlying mechanism of simple NAC and complex NAC including the formula for restricted weights W in terms of W_hat and M_hat. The only difference is that complex NAC applies log space on input and output of the layer.", "As with the architectures of both NACs, the python implementation of complex NAC is almost same except mentioned change in the output tensor formula. Below is the code for such NAC.", "Once again, for full working code please checkout my GitHub repo mentioned at the end of this post.", "By now one can imagine that above two NAC models ", " can learn almost all arithmetic operations. That is the ", " behind NALU which comprises the", " of a simple NAC and a complex NAC mentioned above, controlled by a learned gate signal. Thus, NAC forms the basic building block of NALUs. So, if you have understood the NAC properly, NALU is very easy to grasp. If you haven\u2019t, please take your time and go through the both NAC explanations once again. Below image describes the architecture of NALU.", "As shown in the above image, in NALU both NACs (purple cells) are interpolated (combined) by a learned sigmoid gate control (orange cell) such that the output of either NAC can be activated or deactivated by gate based on the numeric function we are trying to train the network for.", "As mentioned above, the simple NAC computes the accumulation function so it is responsible to store NALU\u2019s linear (addition and subtraction) operations. While the complex NAC is responsible to carry out its more complex numeric functions such as multiplication, division and power functions. The ", "of the underlying cells in an NALU can be represented mathematically as follows:", "In the above formula of NALU, we can say that if gate output ", " then the network will learn ", " functions but not the simple ones. In contrast, if ", " then the network will learn", " functions and not the complex ones. Hence, altogether NALU can learn any numeric functions consisting of multiplication, addition, subtraction, division, and power functions in such a way that extrapolates well to the numbers outside of the range of numbers that have been seen during training.", "In the implementation of NALU, we will use both simple and complex NAC defined in the previous code snippets.", "Again, in the above code, I used random normal initialization for gate parameters G but one can use any recommended weight initialization technique.", "believe NALU is a modern breakthrough in AI and specifically in neural networks that seems very promising. They can open doors to many applications which seem to be difficult for standard NNs to deal with.", "uthors have shown various experiments and results implementing NALU in different area of neural network applications in the paper from simple arithmetic function learning tasks to counting the number of handwritten digits in provided series of MNIST images to make the network learning to evaluate computer programs!", "The results are amazing and prove that the NALU ", "involving numerical representation than the standard NN models. I recommend readers to have a look at these experiments and its results to gain the deeper understanding of how NALU can be useful in some interesting numerical tasks.", "However, it is unlikely that NAC or NALU will be the perfect solution for every task. Rather, they demonstrate a general design strategy for creating models that are intended for a target class of numerical functions.", "Below is the link to my ", " that shows the full implementation of the code snippets shown in this post.", "You are welcome to try out various functions to test my model using different hyperprameters to tune the network.", "Let me know if you have any questions or thoughts on this post in the comments below and I will try my best to address them.", "Written by"], "postingTime": "2018-08-30T10:57:03.375Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "MTank", "articleTile": "Multi-Modal Methods: Visual Speech Recognition (Lip Reading)", "content": [" If you want robots \ud83e\udd16 in your home, and would like to see that happen ", ", then please take our very short survey. Your responses help guide our simulated environment research and robotics projects \ud83d\udc47\ud83d\udc47\ud83d\udc47", "Give 3 minutes of your time:", "This is the first instalment of our latest publication series looking at some of the intersections between Computer Vision (CV) and Natural Language Processing (NLP). Readers are encouraged to view the piece through our ", " for the best experience: ", "Part Three: Image Captioning (Reinforcement Learning and Beyond)", "Feedback and comments are welcomed, either through medium or directly to ", ". Thanks for reading!", "In this series of pieces we decided to examine the interplay between Computer Vision (CV) and Natural Language Processing (NLP) \u2014 a fitting segue from the previous CV-centric piece: \u201c", "\u201d (available", ")", ". While advancements within a singular field are often impressive, knowledge, by its very nature, is additive and combinatorial. These characteristics mean that improvements and breakthroughs in one field may catalyse further progress in other fields. Often two seemingly distinct bodies of knowledge coalesce to push our understanding, technologies and solutions into exciting and unforeseen areas.", "In our ", ", we briefly attempted to outline Computer Vision\u2019s claim on intelligence; building systems that can learn, infer and reason about the world from visual data alone. Here we hope to add to this discussion; what part does language play in the creation and recreation of intelligence?", "This topic, when broached, has historically been a source of contention among linguists, neuroscientists and AI researchers. We can at least say that vision and language are inextricably intertwined, from an evolutionary standpoint, with the human experience. An experience that weighs learning heavily. For instance, when the concept of a \u2018cat\u2019 is evoked in your mind, there are numerous different associations around the nexus of cat almost instantly. Such as:", "These basic experiences of the concept \u2018cat\u2019 all inform our understanding of what a cat is and its relationship to us and the world. This knowledge of \u2018cat\u2019 is iterable; it may be altered through direct experience, pondering cat-related things or by gaining information through any medium. Although not all experiences require language when recalling, the articulation of the experience or thought to oneself is often through language.", "If Computer Vision recognises patterns, then perhaps the addition of NLP could augment this process. It could enable processes in machines analogous to how people associate many modalities and experiences with the aforementioned concept of \u2018cat\u2019. The addition of language may eventually provide a means for machines to group, reason and articulate complex concepts in the future.", "Much the same way we iterate, link and update concepts through whatever modality of input our brain takes \u2014 multi-modal approaches in deep learning are coming to the fore. Below are just some of the intersections between CV and NLP:", "Of these fields, we hope to provide insight into the progression and techniques of Lip Reading and Image Captioning in this series. While Visual Question Answering and Image Generation from Captions may be the subject of some future work.", "In ", "(NIPS 2017)", ", prominent researchers offered a simple abstraction \u2014 that ", " all deep learning approaches can be characterised as either augmenting ", ". While an oversimplification, the generalisability of current deep learning approaches is impressive. And as we shall see, these general approaches are also circumscribing new territories of competence as they progress.", "There are also interesting second-order effects due to the generalisability of these methods and their relatively recent successes across-domains. This is despite the seemingly-troublesome issue of handling completely different inputs and output formats. Researchers can now work in many different areas and apply their techniques to issues across the spectrum, from social sciences to healthcare, and from sports to finance. Regardless of application, the tricks and knowledge gathered on architectures and loss functions may be repurposed and used anew somewhere else.", "This partial disintegration of some research silos, or the encouragement of greater interdisciplinary work using AI-tools and techniques, follows on from our remarks about the combinatorial nature of knowledge. Second-order effects mean that CV researchers often understand NLP techniques, and vice-versa. Introductory courses and books on deep learning cover use cases within NLP, CV, Reinforcement Learning and Generative models.", "In some senses, we are getting closer to a generalisable artificial intelligence; knowledge in deep learning is consolidating into a more paradigmatic approach. Such congruency allows researchers from all disciplines to leverage AI in new and exciting ways. Perhaps, a true general intelligence lies ahead, although how many paradigms must be disequilibrated and reinstated anew before such a point is reached is unknown. What we do know is that work in generalisable models continues to captivate us, as we watch techniques perform across multiple tasks, domains and modalities ", ".", "In keeping with the last publication, we aim to be as accessible as possible for our audience, and to provide individuals with the tools to learn about AI at whatever depth they desire. However, in this piece we sacrificed expanse for greater depth into the research areas themselves. We will continue to experiment with scope and timelines, to understand how best to convey topics to the reader. For those lacking technical proficiency there may be short sections which are tedious; but their omission won\u2019t impinge the lay-reader greatly. We hope that one should be able to take something of value away regardless of their skillset.", "Further inroads will be made in the coming years into a greater number of fields, with better techniques deployed at an ever-increasing rate. Understanding that our assumptions may be incorrect, about what AI can and can\u2019t do, is an important step for society. Ultimately, these technologies aim to emulate and improve the processes through which we navigate the world around us. To learn their own meta-structures for the world that we deploy everyday, subconsciously.", "If humanity has never accepted limitations to our abilities, why would we assume that mechanised intelligence will be inherently limited in some way? And with new, unforeseen breakthroughs, the assumption that anyone can predict the long-term future of technology is perhaps untenable at best. The best strategy may be to simply stay as informed as we can and actively engage with the advancements on the horizon.", "With thanks,", "Previous work from the team detailed some of the many advancements within the field of Computer Vision. In practice, research isn\u2019t siloed into isolated fields and, with this in mind, we present a short exploration of an intersection between Computer Vision (CV) and Natural Language Processing (NLP) \u2014 namely, Visual Speech Recognition, also more commonly known as lip reading.", "Similar to the advancements seen in Computer Vision, NLP as a field has seen a comparable influx and adoption of deep learning techniques, especially with the development of techniques such as Word Embeddings", " and Recurrent Neural Networks (RNNs)", ". Moreover, the drive to tackle complex, cross-domain problems using a combination of inputs has spawned much to be excited about. One source of excitement for us comes from seeing the skill of Lip Reading move from human-dominance to machine-dominance in the accuracy rankings. Another still from the method by which this was accomplished.", "It was not so long ago that lip reading was heralded to be a difficult problem, much like the difficulty ascribed to the game of Go; albeit not quite as well-known. In addition to solving this problem, advancements in lip reading may potentially enable several new applications. For instance, dictating messages in a noisy environment, dealing with multiple simultaneous speakers better, and improving the performance of speech recognition systems in general. Conversely, extracting conversations from video alone may be an area of concern in the future.", "Our focus on this niche application, one hopes, is both illustrative and informative. A relatively small body of deep learning work on lip reading was enough to upset the traditional primacy of the expertly-trained lip reader. Meanwhile, the combinatorial nature of AI research and the technologies at the centre of these advancements blend the demarcations between fields in a scintillating way. Where, if ever, such advancements plateau is the question on everyone\u2019s lips.", "The task of predicting innovations and advancements in technologies is notoriously quite difficult, and best reserved for small wagers between colleagues and friends. Where estimates are made, one usually compares a machine\u2019s performance to tasks that humans are already good at, e.g. walking, writing, playing sports, etc. It surprised us to learn two things with regards to lip reading. Firstly, that machines managed to surpass expert-humans recently, and secondly, that expert-humans weren\u2019t that accurate to begin with.", "Irrespective of the bar set by the expert, we think it best to delve into what makes this a tough challenge to master. Visemes, ", ", pose a clear challenge to those who\u2019ve ever attempted to apply them. Namely, that multiple sounds share the same shape. There exists a level of ambiguity between consonants, which cannot be dispensed with \u2014 a problem well documented by Fisher in his extensive study on ", ".", "Since there are only so many shapes that one\u2019s mouth can make in articulation, mapping said shapes accurately to the underlying words is challenging ", ". Especially when much communication relies more on sound than on visual information; vocal communication is sound-dependent. Hence, achieving high accuracy without the context of the speech ", " is extremely difficult \u2014 for people and machines.", "With these limitations it\u2019s not surprising that early studies focused on simplified versions of the problem. Initially, feature engineering produced improvements using facial recognition models which placed bounding boxes around the mouth, and extracted a model of the lips independent from the orientation of the face. Some common features used were the width-height ratio of a bounding box for detecting mouths, the appearance of the tongue (pixel intensity in the red channel of the image) and an approximation of the amount of teeth from the \u2018whiteness\u2019 in the image ", ".", ":", "Extracting Lips as a Feature", "These approaches obtained impressive results (over 70% word accuracy) for tests performed with classifiers trained on ", ". But performance was heavily damaged when trying to lip read from individuals not included in the training set. Lip detection in males with moustaches was also more difficult and, therefore, the performance on such cases was poor. Hence, the feature engineering approaches, while an improvement, ultimately failed to generalise well.", "Following this, using different viseme classification methods with defined language models improved state of the art (SOTA) performance.", "Language models help filter results that are obviously incorrect and improve results by selecting from only plausible options, e.g. \u2019n\u2019 for the 4th character in \u201csoon\u201d rather than \u201csoow\u201d or \u201csoog\u201d. Greater improvements still were made by \u201c", "\u201d the viseme classifier for phoneme classification, which enabled them to deal with multiple possible solutions for words containing the same visemes in similar intervals. This improved accuracy and performed comparatively better than previous approaches.", "These early techniques brought performance to roughly 19% accuracy on an unseen test set, an improvement over the prior best of 17% (+/- 12%) accuracy generated by a sample of hearing-impaired lip readers. A sample group which outperforms the general population on average.", "McGurk and MacDonald argue in their 1976 paper", " that speech is best understood as bimodal, that is taking both visual and audio inputs \u2014 and that comprehension in individuals may be compromised if either of these two domains are absent. Intuitively, many of us can recall mishearing speech while on the phone, or the difficulties one has in pairing sound and lips in a noisy environment. The requirement of bimodal inputs, as well as contextual constraints, hampers the ability of people and machines to read lips with accuracy. This pointed to the need for further studies on the use of these combined information sources. A direction which brings us into the most recent epoch of lip reading approaches.", "It is with this point that we introduce recent work from Assael et al. (2016) \u2014 \u201c", ".\u201d", " \u201c", "\u201d introduces the first approach for an end-to-end lip reading algorithm at sentence level. Earlier work by Wand, Koutn\u00edk and Schmidhuber", " applied LSTMs", " to the task, but only for word classifications. However, their earlier advances, including end-to-end", " trainability, were undoubtedly valuable to the body of work in the space. For those wishing to know more about LSTMs and their variants, Christopher Olah provides an intuitive and detailed explanation of their use ", ".", ":", "LipNet Example at Sentence Level", "On a high level in the architecture, the frames extracted from a video sequence are processed in small sets within a Convolutional Neural Network (CNN),", " while an LSTM-variant runs on the CNN output sequentially to generate output characters. More precisely, a 10-frame sequence is grouped together in a block (width x height x 10), sequence length may vary, but the consecutive nature of these frames creates a ", ".", "Then the output of this LSTM-variant, called a Gated Recurrent Unit (GRU),", " is processed by a multi-layered perceptron (MLP) to output values for the different characters derived from the ", ". Lastly, a Connectionist Temporal Classification (CTC) provides final processing on the sequence outputs to make it more intelligible in terms of precise outputs, i.e. words and sentences. This approach allows information to be passed through the time periods comprising both words and, ultimately, sentences, improving the accuracy of network predictions.", "The authors note that \u2018", "i.e. the variance problems seen in earlier approaches,", "\u2019, originally classed as open problems in Zhou et al. (2014).", ", The approach in LipNet, we feel, is interesting and exciting outside of the narrow confines of accuracy measures alone. The combination of CNNs and RNNs in the network \u2014 itself a hark back to our comments around the lego-like approach of deep learning research \u2014 is, perhaps, more evidence for the soon-to-be-primacy of differential programming. Deep Learning est en train de mourir. Vive Differentiable Programming!", "LipNet also makes use of an additional algorithm typically used in speech recognition systems \u2014 a Connectionist Temporal Classification (CTC) output. After the classification of framewise characters, which in combination with more characters define an output sequence, CTC can group the probabilities of several sequences (e.g. \u201cc__aa_tt\u201d and \u201cccaaaa__t\u201d) into the same word candidates (in this case \u201ccat\u201d) for the final sentence prediction. Thus the algorithm is alignment-free. CTC solves the problem of matching sequences where timing is variable.", ": CTC in Action", "By predicting the alphabet characters and an additional \u201c_\u201d (space) character, it\u2019s possible to generate a word prediction by removing repeated letters and empty spaces, as can be seen in fig. 5 for the classification of the word \u201cplease\u201d. In practical terms this means that elongated pronunciations, variations in emphasis and timings, as well as pauses between syllables and words can still produce consistent predictions using the CTC for outputs.", ": Saliency map of \u201cPlease\u201d", "CTC is a function for output alignment and a loss correction function based on that alignment, and is independent of the CNN and LSTM-variants. One can also think of CTC as similar to a softmax due to converting the raw output of a network (e.g. raw class scores or in our case, characters) into the expected output (e.g. a probability distribution or in this case, words and sentences). CTC makes matching a single character output to word level possible. Awni Hannun provides an excellent dynamic publication that explains CTC operation; available ", ".", "There is a great video which covers some of ", "functionality, as well as a specific use case \u2014 operating within autonomous vehicles. Seeing ", " in operation ties together much of what we\u2019ve discussed about the system so far.", "A hallmark of this method is that the output labels are not conditioned on each other. For example, the letter \u2018a\u2019 in \u2018cat\u2019 is not conditioned on \u2018c\u2019 or \u2018t\u2019. Instead this relation is extracted by three spatio-temporal convolutions, followed by two GRUs which process a set number of the input images. The output from the GRUs then goes through a MLP to compute CTC loss (see fig. 6).", ": ", "Architecture", "The architecture of ", " was deemed an empirical success, achieving a prediction accuracy of 95.2% on sentences from the GRID dataset, an audiovisual sentence corpus for research purposes.", " However, literature on deep speech recognition (Amodei et al., 2015)", " suggested that further performance improvements would inevitably be achieved with more data and larger models. Commentators, reminded of earlier difficulties in generalisability and moustache-handling, expressed concern over the unusual sentences taken from GRID which formed the ", "example video. The limited nature of GRID produced fears of overfitting; but how would ", "fare in the real-world?", ": ", " and other approaches", "Not long after ", ", DeepMind released \u2018", "\u2019, ", " and addressed some of the concerns around ", "generalisability. Taking inspiration from both CNNs for visual feature extraction", " and the use of LSTMs for speech transcription,", " the authors present an innovative approach to the problem of lip reading. By adding individual attention mechanisms for each of the input types, and combining them afterwards to produce character outputs, improvements in both the accuracy and generalisability of the original ", "architecture were realised.", "Attention mechanisms, discussed at length in part two of this piece, refer to a technique for focusing on specific parts of the input or previous layer(s) within neural networks. A somewhat-recent technique, taking inspiration from earlier work but popularised by Alex Graves\u2019s in 2013/2014, it has grown in use partially from his memory-related work: the now-famous sequence generation paper", " along with his work on neural turing machines.", "Attention mechanisms have been an enabler of some the recent success within deep learning; due to more efficient and clever processing of data. It also allows these models to have more interpretability, i.e. if asking why a network thinks a certain image is a dog it is often hard to look at and understand the internals of the network to find out why. Attention allows the network to highlight the salient parts of the image used in its prediction, e.g. a snout and pointed ears. Attention has become such a common technique that it spawned papers like \u201c", "\u201d, which foregoes convolution and recurrence techniques entirely for the problem of machine translation.", "eturning to \u201c", ", Chung et al. (2017) present their ", ". Composed of three main submodules (watch, listen spell) \u2014 with attention sprinkled into the spell module. The system is as follows:", " is a VGG-M", "that extracts a framewise feature representation to be consumed by an LSTM, which generates a state vector and an output signal. The ", " module looks at each frame in the video and extracts the relevant features that the module has learned to look for, i.e. certain lip movements/positions. This is done by a regular VGG-M CNN which outputs a feature representation for each frame.", "This sequence of feature representations are then fed into a regular LSTM which generates a state vector (or cell state) and an output signal. With LSTMs and GRUs there\u2019s an output and a \u201cstate\u201d input to the next LSTM cell. The output is a character prediction (or a probability distribution of predicted character), while state is what encodes \u201cthe past\u201d, i.e. what an LSTM has computed/stored of the past which is used to predict the next output.", ": Watch, Listen, Attend and Spell architecture", "The ", " module uses the Mel-frequency cepstral coefficients (MFCCs)", " as its input. These parameters define a representation of the short-term power spectrum of a sound based on signal transformations. MFCCs ensure transformations are scaled to a frequency which simulates the human hearing range. Following this, independent attention mechanisms in the ", " module for each of the audio and video inputs are combined. These are then in turn passed through the ", " module. With a multi-layered perceptron (MLP) at each time step, the output from the LSTM ends up in a softmax to define the probabilities of the output characters.", "With this, we return to similar themes of progress alluded to in our previous work: data availability and network stack-ability. Neural network-based approaches are typically characterised by heavy data demands. Concomitant to the progress in lip reading is the creation of a unique dataset for training and testing the network. Previously, research in lip reading was hampered by the available datasets and their small vocabularies. One only has to look at the desirable characteristics of Chung et al.\u2019s (2016/2017) datasets, the LRW and the LRS, as expressed by Stafylakis and Tzimiropoulos (2017, p. 2), to understand the value of such data in improving research efforts:", "Chung et al. (2017) created a pipeline to automatically generate the dataset(s)", " from BBC recordings as well as from the contained closed captions, which enabled progress in a data-intensive research area. Their creation is a \u2018Lip Reading Sentences\u2019 (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.\u2019", ": Pipeline to generate LRW/LRS dataset", "The authors also corrupt said datum with storm noises (i.e. weather storms", "), demonstrating the network\u2019s ability to use distorted and low volume data, or to discard audio completely for prediction purposes. Determining whether there\u2019s value to the prediction in listening or not. For those wishing to see more, Joon Son Chung presents a fantastic overview of the authors\u2019 work at CVPR.", "Although movements towards lower data requirements are pressing-on, this paradigm has yet to shift; and it\u2019s likely that it shall remain this way for some time to come. As for stackability, the very nature of the ", " and ", "architectures illustrate the lego-like nature of neural nets \u2014 e.g. CNNs plugged into RNNs with attention techniques.", " While it\u2019s true that this is a ", " oversimplification, as a heuristic we find it increasingly useful in interpreting and understanding the rapid advancements across a lot of existing, and new, AI research.", "Here this last point extends outside of the architecture itself, inscribing the potential stacking of inputs into our heuristic also. A great contribution of these works is the creation of an end-to-end architecture capable of using audio, video, or combinations of both as inputs to generate a text prediction as output: creating a truly multimodal model. Multiple input sequences resulting in a singular output sequence. Solving this multi-modal problem, and others like it, potentially opens new paths to explore in connecting video, audio and language systems.", "urious as to what would follow the approaches detailed previously, we turn our attention to some of the most recent work in this space. Although not exhaustive, here\u2019s a smattering of the best improvements we came across in this domain:", ": Architecture", "\u201c", "\u201d", ": Two new papers from the authors of ", " and ", " respectively.", "Written by"], "postingTime": "2019-08-31T13:31:51.964Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "Alibaba Tech", "articleTile": "Behind the Chat: How E-commerce Robot Assistant AliMe Works", "content": ["The most significant innovation in AI these recent years, smart chatbots, personal assistants, are only a glimpse of what the future holds. Technology companies such as Google, Facebook, Microsoft, Amazon and Apple are at the forefront of personalized interactive products where intelligent human-computer interactions (IHCI) technology will continue to play a central role in automated messaging, task assistance and the Internet of Things. As the market matures, chatbots are becoming more and more specialized according to their specialized intended purposes, such as customer service, entertainment, personal assistance, or education.", "Launched in July 2015, AliMe is an IHCI-based shopping guide and assistant for e-commerce that overhauls traditional services, and improves the online user experience. During 2017\u2019s Double 11 shopping festival, AliMe successfully responded to 9.04 million queries, and accounted for 95% of the customer services rendered by Alibaba\u2019s e-commerce platforms.", "Intelligent human-computer interaction (IHCI) systems are commonly referred to as chatbots or bot systems. Natural language understanding (NLU) is the very foundation of IHCI, a dialogue system that processes users\u2019 questions and generates answers in natural language. This in itself is quite a feat as computers are built on logic-heavy cognitive bases that are not suited for processing dynamic human languages.", "The first step in creating AliMe required setting up abstract frameworks for different fields, strata, and scenarios.", "The majority of intelligent matching processes in use today fall into three main categories- rule-based matching, retrieval, and DL. The technology behind AliMe is based on a combination of all three.", "The dialogue system is thus divided into the following strata:", "This stratum identifies the underlying intention for each message, classifying them and then extracting their attributes. Since intentions determine the subsequent domain identification flow, the intention stratum is a necessary first step in initiating contextual and domain data model processes.", "Questions are matched and identified to generate answers; AliMe\u2019s dialogue system employs three answering strategies according to different intentions:", " FAQs such as \u201cWhat should I do if I\u2019ve forgotten my password?\u201d trigger a query on knowledge graph or retrieval model.", "The knowledge graph is constructed by mining entities and phrases, the relations of which are predefined, from the vast pool of data available. Though knowledge graph-based methods accurately identify answers, they also accrue higher maintenance costs and looser initial data structures AliMe\u2019s Q&A design overcomes this by integrating traditional retrieval models.", "Tasks such as \u201cI\u2019d like to book a one-way flight from New York to Paris for tomorrow\u201d can be solved by the intention commitment + slot filing matching or deep reinforcement learning (DRL) model.", " Chitchatting, such as \u201cI\u2019m in a bad mood\u201d, pulls up a method that marries the retrieval model with deep learning (DL).", "The chitchat domain mainly involves two kinds of models- the retrieval-based model and the deep generative model. The former makes selections from a fixed corpus of answers relevant to a given query, while the latter is more advanced, generating answers without relying on any corpus. The integrated merits of the two models form the core of AliMe\u2019s chat engine. First, the candidate data sets are brought up using the traditional retrieval model; then, candidate sets are re-ranked through the Seq2Seq model; the top answer candidate is chosen when the ranking score is higher than the preset threshold, failing which the seq2seq model is activated to generate an answer.", "AliME\u2019s identification and extraction of intentions is reliant on the classification results. AliME incorporates features of both traditional textual and user behaviors to analyze incomplete user intentions.", "During the process of creating DL-based prediction systems, the team came up with two specific modeling options. The multi-classification model, though faster, required retraining with every new label added to the class family, whereas the binary classification model, a clear underperformer which needed constant dichotomization, allowed for unfettered field expansions on the original platform. It was apparent that both models, with their specific drawbacks and strengths, serve very distinct sets of scenarios.", "AliME\u2019s DL-based intention classification embeds behavioral factors and textual features, and concatenates different vectors before multi-classification or binary classification processing. Textual features can be represented as bag of words or word embedding.", "Intelligent shopping guide systems interact with users to analyze their intentions with the goal of providing a better shopping experience. The interactions serve two main purposes- helping machines understand user intentions, and optimizing recommendation rankings and the interactive process itself.", "Intelligent shopping guide systems are created to deduce what users want, and the attributes of those goods. This brings with it a new set of issues:", " Users tend to express themselves in short sentences, therefore, identifying intentions accurately requires multiple rounds.", " Users often interact inconsistently, detailing or modifying parts of their intentions.", " Shoppers\u2019 intention may not always be semantically correct or accurate.", "Relations between intentions are very complex.", "AliME can accommodate phrasal expressions, intention boundary switches and logical modifications owing to the intention stack and product knowledge graph Due to the vast variety of goods, knowledge graphs are combined with semantic indexes to make identification extremely effective.", "Under intelligent shopping guide scenarios, category management consists of category identification and calculation of category relations.", "AliME\u2019s identification plans are built on knowledge graphs, semantic indexes and DSSM (deep semantic similarity model). The semantic indexes are built on textual information as well as search and click data. Similarities between word segmentations and candidate categories are calculated using word embedding.", "The calculation of category relations addresses intentions arising from the intelligent shopping guide. Two important examples of these relations are hyponymy relations and similarity relations.", "For example, when a user first intends to buy some clothes but later changes their mind to buy a cup, the attributes associated with clothes should not be passed down to the cup. On the other hand, if the user changes his mind and buys a shirt, a hyponym of clothes, the attributes associated with clothes should be passed down to the shirt.", "Hyponymy relations can be calculated through the following two options:", " Knowledge graph-based relation calculation", " Extraction from users\u2019 queries", "Similarity relations can be calculated through the following two options:", " Use of the same hypernym: For example, both Xiaomi and Huawei share the phrase \u2018mobile phone\u2019 as a hypernym", "Semantic similarity based on embedding computation", "Though the technological progress observed in the 21st century is significant, the current phase of AI and its application are definitely nascent. Fields ranging from perception to cognition require vast levels of improvement in order for IHCI to continue enabling industry. Efforts in gathering data and refining knowledge graphs will contribute to IHCI\u2019s development. Task-oriented bots across industrial verticals are poised to provide explosive economic growth; interactive bots targeted at open domains, however, require higher scrutiny and experimentation in the long-term future. Following its successful adoption in computer vision and voice recognition, DL will continue to be applied in the domain of natural language processing (NLP).", "Fortunately, the urgency of development in AI has been met with equal enthusiasm from various stakeholders, from private enterprises to governments, and from academic circles to industrial communities. Given this, we can expect IHCI to fulfill our expectations and visions for the near and long-term future, where even the wildest of science fiction movies and books pale in comparison to the actualized level of technology.", "(Original article by Zhou Wei, Chen Haiqing)", "First hand, detailed, and in-depth information about Alibaba\u2019s latest technology. ", ": ", "[1]: Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using click through data[C]// ACM International Conference on Conference on Information & Knowledge Management. ACM, 2013: 2333\u20132338.", "[2] Minghui Qiu and Feng-Lin Li. MeChat: A Sequence to Sequence and Rerank based Chatbot Engine. ACL 2017", "[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR 2015", "[4] Matthew Henderson. 2015. Machine learning for dialog state tracking: A review. In Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing.", "[5] Mnih V, Badia A P, Mirza M, et al. Asynchronous Methods for Deep Reinforcement Learning[J]. 2016", "[6] Li J, Monroe W, Ritter A, et al. Deep Reinforcement Learning for Dialogue Generation[J]. 2016.", "[7] Sordoni A, Bengio Y, Nie J Y. Learning concept embeddings for query expansion by quantum entropy minimization[C]// Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, 2014: 1586\u20131592.", "Written by"], "postingTime": "2018-03-20T10:02:30.148Z"}
{"nameOfPublication": "ML Review", "nameOfAuthor": "MTank", "articleTile": "Multi-Modal Methods: Image Captioning (From Translation to Attention)", "content": [" If you want robots \ud83e\udd16 in your home, and would like to see that happen ", ", then please take our very short survey. Your responses help guide our simulated environment research and robotics projects \ud83d\udc47\ud83d\udc47\ud83d\udc47", "Give 3 minutes of your time:", "This is the second instalment of our latest publication series looking at some of the intersections between Computer Vision (CV) and Natural Language Processing (NLP). Readers are encouraged to view the piece through our ", " for the best experience: ", "Part Three: Image Captioning (Reinforcement Learning and Beyond)", "Feedback and comments are welcomed, either through medium or directly to ", ".", ":", "Some image captioning examples", "Suppose that we asked you to caption an image; that is to describe the image using a sentence. This, when done by computers, is the goal of image captioning research. To train a network to accurately describe an input image by outputting a natural language sentence.", "It goes without saying that the task of describing any image sits on a continuum of difficulty. Some images, such as a picture of a dog, an empty beach, or a bowl of fruit, may be on the easier end of the spectrum. While describing images of complex scenes which require specific contextual understanding \u2014 ", "\u2014 proves to be a much greater captioning challenge. Providing contextual information to networks has long been both a sticking point, and a clear goal for researchers to strive for.", "Image captioning is interesting to us because it concerns what we understand about perception with respect to machines. The problem setting requires both an understanding of what features (or pixel context) represent which objects, and the creation of a semantic construction \u201cgrounded\u201d to those objects.", "When we speak of grounding we refer to our ability to abstract away from specifics, and instead understand what that object/scene represents on a common level. For example, we may speak to you about a dog, but all of us picture a different dog in our minds, and yet we can ground our conversation in what is common to a dog and progress forward. Establishing this grounding for machines is known as the language grounding problem.", "These ideas also move in step with the explainability of results. If language grounding is achieved, then the network tells me how a decision was reached. In image captioning a network is not only required to classify objects, but instead to describe objects (including people and things) and their relations in a given image. Hence, as we shall see, attention mechanisms and reinforcement learning are at the forefront of the latest advances \u2014 and their success may one day reduce some of the decision-process opacity that harms other areas of artificial intelligence research.", "We thought that the reader may benefit from a description of image captioning applications, of which there are several. Largely, image captioning may benefit the area of retrieval, by allowing us to sort and request pictorial or image-based content in new ways. There are also likely plenty of opportunities to improve quality of life for the visually-impaired with annotations, real-time or otherwise. However, we\u2019re of the opinion that image captioning is far more than the sum of its immediate applications.", "Mapping the space between images and language, in our estimation, may resonate with some deeper vein of progress. Which, once unearthed, could potentially lead to contextually-sophisticated machines. And, as we\u2019ve noted before, providing contextual knowledge to machines may likely be the one of the key pillars that eventually support AI\u2019s ability to understand and reason about the world like humans do.", ": To build networks capable of perceiving contextual subtleties in images, to relate observations to both the scene and the real world, and to output succinct and accurate image descriptions; all tasks that we as people can do almost effortlessly.", "Image captioning research has been around for a number of years, but the efficacy of techniques was limited, and they generally weren\u2019t robust enough to handle the real world. Largely due to the limits of heuristics or approximations for word-object relationships[", "][", "][", "]. However, in 2014 a number of high-profile AI labs began to release new approaches leveraging deep learning to improve performance.", "he first paper, to the best of our knowledge, to apply neural networks to the image captioning problem was Kiros et al. (2014a)[", "], who proposed a multi-layer perceptron (MLP) that uses a group of word representation vectors biased by features from the image, meaning the image itself conditioned the linguistic output. The timeline of this, and other advancements from research labs was so condensed, that looking back it seems like a veritable explosion of interest. These new approaches generally;", ".", "One such evaluation metric is the Bilingual Evaluation Understudy algorithm, or BLEU score. The BLEU score comes from work in machine translation, which is where image captioning takes much of its inspiration; as well as from image ranking/retrieval and action recognition. Understanding the basic BLEU score is quite intuitive.", "A set of high-quality human translations are obtained for a given piece of text, and the machine\u2019s translation is compared against these human baselines, section by section at an n-gram level[", "]. Typically an output score of \u20181\u2019 matches perfectly with the human translations, and a \u20180\u2019 means that the output sentence is completely unrelated to the ground truth. The most representative within Machine Translation and Image Captioning include: BLEU 1-4 (n-gram with n=1-4), CIDEr [", "], ROUGE_L [", "], METEOR[", "]. These approaches are quite similar in that they measure syntactic similarities between two pieces of text, while each evaluation metric is designed to be correlated to some extent with human judgement.", "In image captioning however, ", "are replaced with ", "or ", ". But BLEU scores are still calculated as output against human-annotated reference captions. Hence, network-generated captions are compared against a basket of human-written captions to evaluate performance.", "In the past we\u2019ve noted the huge effect of new datasets on research fields in AI. The arrival of the Common Objects in Context (COCO)[", "] dataset in 2014 ushered in one such shift in image captioning. COCO enabled data-intensive deep neural networks to learn the mapping from images to sentences. And, given a comparatively large dataset of images with multiple human-label descriptions of said images, coupled with new, clever architectures capable of handling image input and language output; it now became possible to train deep neural networks for end-to-end image captioning via techniques like backpropagation.", "In machine translation it is quite common to use \u2018", "\u2019 models[", "]. These models work by generating a representation through a RNN, based on an input sequence, and then feeding that output representation to a second RNN which generates another sequence. This mechanism has been particularly effective with chatbots, enabling them to process the representation of the input query and generate a coherent answer related to the input sequence (sentence).", ": Sequence-to-sequence model", "CNNs can encode abstract features from images. These can then be used for classification, object detection, segmentation, and a litany of other tasks[", "]. Returning to the notion of contemporaneous successes in 2014, Vinyals et al. (2014)[", "], successfully used a sequence-to-sequence model in which the typical encoder LSTM[", "] was replaced by a CNN. In their paper titled, \u201c", " the CNN takes an input image and generates the feature representation which is then fed to the decoder LSTM for generating the output sentence (see fig. 13).", ": CNN encoder to LSTM decoder", "A few more specifics on how the sentence is generated. At every step of the RNN, the probability distribution of the next word is output using a softmax. Depending on the situation, a slightly naive approach would be to take the word with the highest probability at each step after extracting the output from the RNN. However, beam search is another method which represents a better approach for sentence construction. By searching through specific combinations of words, and creating different possible outputs, beam search constructs a whole sentence without relying too heavily on any individual word from the ones which the RNN may generate at any specific time step. ", ": Beam search example", "For example, at the first word prediction output step, a ", " might be outputted overall by choosing ", ". A deeper explanation of beam search for sentence generation, i.e. related to the decoder portion of our example above, may be found ", "[", "].", "Around the time ", " came around, a similar, but distinct, approach was presented by Donahue et al. (2014): Long-term Recurrent Convolutional Networks for Visual Recognition and Description[", "]. Instead of just using an LSTM for encoding a vector, as is typically done in ", " models, the feature representation is outputted by a CNN, in this case VGGNet[", "], and presented to the decoder LSTM. This work was also successfully applied to video captioning, a natural extension of image captioning.", "The main contribution of this work was not only this new connection setting between the CNN encoder and LSTM decoder[", "], but an extensive set of experiments which stacked LSTMs to try different connection patterns. The team also assess beam search against their own random sampling method, as well as using a CNN trained on ImageNet or further fine-tuning the pre-trained network to the specific dataset used[", "].", ", by Fang et al. (2014)[", "], is useful to explain multi-modality of the 2014 breakthroughs. Although distinct from the approaches of Vinyals et al. (2014)[", "] and Donahue et al. (2014)[", "], the paper represents an effective combination of some of these ideas[", "]. For readers, the working flow of the captioning process may bring a new appreciation of the modularity of these approaches.", ": Creating captions from visual concepts", ". Looking closer at how humans would complete the task, they would notice the important objects, parts and semantics of an image and relate them within the global context of the image. All before attempting to put words into a coherent sentence. Similarly, instead of \u201cjust\u201d using the encoded vector representation of the image, we can achieve better results by combining information contained in several regions of the image.", "Using a word detection CNN, which generates bounding boxes similar to what an object detection CNN does, the different regions in the image may receive scores for many individual objects, scenes or characteristics which correspond to words in a predefined dictionary (which includes about 1000 words).", "Next, the likelihood of the matched image descriptors (detections) are analysed according to a statistically predefined language model. E.g. if a region of the image is classified as \u201chorse\u201d, this information can be used as a prior to give a higher likelihood to the action of \u201crunning\u201d over \u201ctalking\u201d for the image captioning output. This combined with beam search produces a set of output sentences that are re-ranked with a Deep Multimodal Similarity Model (DMSM)[", "].", "This is where the ", " comes into play. The DMSM uses two independent networks: a CNN for retrieving a vector representation of the image (VGG) and a CNN architecture with an explicit use. The image encoding network is based on the trained object detector from the previous section, with the addition of a set of fully connected layers to be trained for this re-ranking task. The second CNN is designed to extract a vector representation out of a given natural language sentence, which is the same size as the vector generated by the image encoding CNN. This effectively enables the mapping of language and images to the same feature space.", "Since the image and encoded sentence are both represented as vectors with the same size, both networks are trained to minimise the cosine similarity between the image and ground truth captions for the given image, as well as to increase the difference with a set of irrelevant captions provided.", "During the inference phase, the set of output sentences generated from the language model with beam search are re-ranked with the DMSM networks and compared against each other. The caption with highest cosine similarity is selected as the final prediction.", "Considerable improvements in bounding box detectors, such as RCNN, as well as the success of BiRNNs [", "] in translation, produced another approach theoretically similar to the DMSM for sentence evaluation presented before. Namely, that one can make use of two independent networks, one for text and one for image regions, that create a representation within the same image-text space. An example of such an approach is seen in the work of Karpathy and Fei-Fei (2015)[", "].", "[", "] \u2014 which utilises the aforementioned CNN + RNN approach for caption generation \u2014 is, perhaps, most responsible for popularising image captioning in the media. A large proportion of articles on image captioning tend to borrow from their excellent captioned image examples.", "But more impressive than capturing the public\u2019s attention with their research, were the strides made by Johnson, Karpathy and Fei-Fei later that year \u2014 in ", "[", "].", ": Dense captioning & labelling", "We noted earlier that running a CNN into a RNN allowed the image features, and therefore, its information, to be output in natural language terms. Additionally, the improvements of RCNN inspired ", " to use a region proposal network to create an end-to-end model for captioning, with a forward computation time reduced from 50s to 0.2s using Faster-RCNN[", "].", "With these technical improvements, Johnson et al. (2015) asked the question, ", "The authors introduce a variation to the image captioning task called ", " where the model describes individual parts of the image (denoted by bounding boxes). This approach produces results that may be more relevant, and accurate, when contrasted with captioning an entire image with a single sentence.", "Put simply, the technique resembles object detection, but instead of outputting one word, it outputs a sentence for ", " in a given image. Their model also can be repurposed for image retrieval, e.g. \u201c", "\u201d. In this way we see the connection between image retrieval and image captioning is naturally quite common.", ": Dense captioning in action", "W", "e\u2019ve seen improvements in information flow to the RNN, and the use of multiple bounding boxes and captions. However, if we placed ourselves in the position of captioner, how would we decide on the appropriate caption(s)? What would you ultimately deem important, or disregard, in captioning an image? What would you pay attention to?", "nter \u201c", "\u201d by Xu et al. (2015) [", "] \u2014 the first paper, to our knowledge, that introduced the concept of attention into image captioning. The work takes inspiration from attention\u2019s application in other sequence and image recognition problems. Building on seminal work from Kiros et al. (2014a; 2014b)[", "][", "], which incorporated the first neural networks into image captioning approaches, the impressive research team of Xu et al. (2015)[", "] implement hard and soft attention for the first time in image captioning.", "Attention as a technique, in this context, refers to the ability ", ". Broadly, it can be understood as a tool to direct the allocation of available processing resources towards the most informative parts of the input signal. Rather than summing up the image as a whole, with attention the network can add more weight to the \u2018", "\u2019 parts of the image. Additionally, for each outputted word the network can recompute its attention to focus on a different part of the image.", "There are multiple ways to implement attention, but Xu et al. (2015) divide the image into a grid of regions after the CNN feature extraction, and produce one feature vector for each. These features are used in different ways for soft and hard attention:", "Training is instead completed using the final loss/reward (obtained from the sampled trajectory of chosen regions) as an approximation of the expected reward to be obtained from the MLP which, most importantly, can then be used to calculate the gradients. The same MLP is again used to calculate these probabilities[", "]. The idea of sampling an attention ", " as an estimation was taken from a Reinforcement Learning algorithm called REINFORCE[88]. The next part of this publication will deal with Reinforcement Learning applied to image captioning in different ways and with greater detail.", ": Attention in action", "ncorporating attention allows the decoder to focus on specific parts of the input representation for each of the outputted words. Meaning, that in converting aspects of the image to captions, the network can choose ", " and ", " to focus in relation to specific words outputted during sentence generation. Such techniques not only improve network performance, but also aid interpretability; we have a better understanding how the network determined its answer. As we shall see, attention mechanisms have grown in popularity since their inception.", "Attention and its variants come in many forms: Semantic attention, spatial attention and multi-layer attention. Hard, soft, bottom-up, top-down, spatial, adaptive, visual, text-guided, and so on. We feel that attention, while a newer technique for handling multi-modal problems, has the potential to be somewhat revolutionary.", "Such techniques not only allow neural networks to tackle previously insurmountable problems, but also aid network interpretability; a key area of interest as AI permeates our societies. For those wishing to know more about attention, beyond the limited areas that we touch upon, there is an excellent distil article from Olah and Carter (2016)[", "] available from ", ", and another by Denny Britz (2016)[", "] available ", ".", "Attention can enable our inspection and debugging of networks. It can provide functional insights, i.e. which parts of the image the network is ", "\u2019. Each form of attention, as we\u2019ll see, has its own unique characteristics.", "Semantic attention refers to the technique of focusing on ", " concepts, i.e. objects or actions which are integral to constructing an accurate image caption. In spatial attention the focus is placed on regions of interest; but semantic attention relates attention to the keywords used in the caption as it\u2019s generated.", "There are several important differences, by the authors\u2019 own admission, between their use of semantic attention and previous use-cases in image captioning. Comparing this work to Xu et al. (2015)[", "], their attention algorithm learns to attend to the specific word concepts found within an image ", " words defined from specific spatial locations. It is important to note that some concepts or words may not be directly related to a specific region, e.g. the word \u201cexciting\u201d which may encompass the entire image. This is the case even with concepts that are not directly seen in the image, and can be expanded by \u2018", "\u2019[", "].", ": Semantic attention framework", " is classified as a latent representation of what the decoder already knows. As an extension of a spatial attention model, it determines whether the model must attend to predict the next word. We mentioned that words like \u2018a\u2019, \u2018it\u2019 and \u2018of\u2019 may be seen as not worth attending to; but words like \u2018ball\u2019, \u2018man\u2019 and \u2018giraffe\u2019 are not only worth attending at a point in time (sentinel), but also in a particular part of the image (spatial).", "\u201c", "\u201d[", "].", ": Visualisation of caption generation", "Attention is usually applied spatially to the final layer outputted by the encoder CNN, treating all channels the same to calculate where the attention should focus on, i.e. the usual attention model generates output sentences by only attending to specific spatial areas within the final convolutional layer.", "\u201c", "\u201d[", "].", "\u2018", "\u2019 and the authors\u2019 take full advantage of this natural design \u2014 applying attention to multiple layers within the CNN and to the individual channels within each layer.", "Their approach was applied to the usual datasets of Flick8k, Flickr30k and COCO, and a thorough analysis of the different attention variants was undertaken. The authors note improvements in metrics both through combinations of attention variants, or with a single type, e.g. Spatial vs Channel vs Spatial + Channel. They also vary how many final layers the network should attend to (1\u20133), and extend this to different feature extractors, e.g. a VGG network (with attended layers being chosen from the \u201c", "\u201d convolution layers) or a ResNet.", "The TencentVision team are leading the COCO captioning leaderboard at present. [", "] According to the leaderboard, their entry description reads \u201c", "\u201d. When contrasted with the original paper, one must conclude that an approach which incorporates Reinforcement Learning techniques constitutes a variation on the original approach. However, we could not find a publication detailing these additions as of yet[", "].", "Following from this definition, bottom-up attention is applied to a set of specific spatial locations which are generated by an object detection CNN. These salient spatial regions are typically defined by a grid on the image, but here they compute bottom-up attention over all bounding boxes where the detection network finds a region of interest. Specifically, each region of interest is weighted differently by a scaling/alpha factor and these are summed into a new vector which is passed into the language model LSTM[", "].", "On the other hand, top-down attention uses an LSTM with visual information [", "], as well as task-specific context input, to generate its own weighted value of these features. The previously generated word, the hidden state from the language model LSTM, and the image features averaged across all objects are used to generate the top-down attention output.", "Using the same attention methodology, Anderson et al. (2018) managed to make strides in two different tasks, i.e. both image captioning and VQA [", "]. Their approach is currently second on the COCO captioning leaderboard [", "], achieving SOTA scores on the MSCOCO test server with CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively[", "].", "\u201c", "\u201d[", "].", "Although attention and its variants represent quite a large body of impressive work, finally we turn ours, limited by space, to our two favourite pieces of research to date:", "\u201c", "\u201d[", "].", "\u201c", "\u201d[", "].", "Written by"], "postingTime": "2019-07-29T20:36:11.355Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "Exploratory v4.2 Released! \u2014 Dashboard, New Analytics, New Charts, and more!", "content": ["Happy New Year everyone!", "And, I\u2019m super excited that the first thing I can do in 2018 is to announce Exploratory v4.2! \ud83c\udf89", "After almost three months of hard and sleepless work, finally, we are releasing v4.2 today. There are a lot of new features and enhancements (and of course, bug fixes!) based on our Exploratory users\u2019 feedback. Thanks to those who gave us such feedback and have been waiting for this day!", "Among many other improvements, there are four main areas we have focused on improving with this release. They are:", "Let me walk you through one by one.", "Finally, we\u2019re adding Dashboard along with Note and Slides as part of the communication and reporting capability of Exploratory.", "It\u2019s often better to establish what metrics are most important for your business or team at the very beginning of your data science projects. Then, you can start analyzing what are influencing the metrics and how you can increase (or decrease) them by using various data science methods.", "And Dashboard is the best way to monitor such business metrics periodically and share with the entire team so that everyone is working towards to the same business goals.", "You can create Dashboard with all the charts you have created by a click of a button.", "You can set the order of the charts by drag and drop.", "And you can publish your Dashboard to ", " to share with your team and schedule it every hour or day.", "You can also export it in a HTML format as well.", "There will be more enhancements coming in this area, so stay tuned. And, we would love to hear your feedback!", " is often used as a method to reduce the dimensionality of the data. This is useful especially when you are building machine learning models based on a data with a large set of variables (columns). The PCA algorithm creates a fewer set of variables that capture the most variance in the data and are mutually un-correlated, hence the newly created variables can still represent the original data without losing much information.", "But even without a need of building machine learning models, it turned out that PCA can be super useful at exploratory data analysis phase because it can help us understand the relationship between the variables and can be used to cluster the underlying data.", "With v4.2, you can quickly access ", " under Analytics view.", "Let\u2019s take a look at it with this 2016 California Election result data with which we can observe how each county in California voted for the ballot measures such as \u2018Marijuana Legalization\u2019, \u2018Ban on Single Use Plastic Bag\u2019, etc.", "Under Analytics view, we can select \u2018Principal Component Analysis\u2019 and select all the measures as the variables, then run it.", "The first tab called \u2018Component Importance\u2019 shows how much of the variance of the data can be explained by each of the newly created component (variable/dimension).", "Here, we can see that the first component can explain 72.88% of the variance.", "Going to \u2018Biplot\u2019 tab, you can see the relationship between the variables. If the variables are heading in the same direction and similar length those are considered close.", "Each dot represents county and its position is determined based on how it voted for all the measures. I\u2019ve assigned Party_Name column to Color so that each dot can be Blue if it voted for the Democratic candidate (Hilary Clinton) and Red if it voted for the Republican candidate (Donald Trump).", "Many of the measures are heading towards the left and the blue counties are closer to those measures, which means these measures are the ones that the Democratic counties tend to vote for.", "On the other hand, the measures like \u2018Adult Film Condom Requirements\u2019 and \u2018legislative_procedure_requirements\u2019 are heading towards the bottom and there is almost no county around them. This means that these don\u2019t make the counties vote for or against based on whether they are Democratic or Republican.", "By assigning \u2018County Name\u2019 column to Label and checking \u2018Show on Plot\u2019 in the property, you can see the relationship among the counties based on how they voted.", "We have added ", " under Regression Analysis analytics type. Logistic Regression is ", " among the data scientists surveyed by ", ". The reason why this good old algorithm is so popular is that it can provide the insight that is useful and relatively simple to understand. The typical use case is, you build prediction models with Logistic Regression algorithm to predict binary outcomes (e.g. TRUE or FALSE) and then you look at the coefficients or odds ratios for the predictor variables to see how they would impact on the outcome.", "For example, here\u2019s a U.S. baby birth data that is collected by ", " (Center for Disease Control) and it includes how many months the babies are in the womb, the weights of the babies at birth, the mother and father\u2019s demographics, etc.", "Now, let\u2019s say we want to know what makes some babies born prematurely. Each baby has a column \u2018is_premature\u2019 that indicates whether he/she was born prematurely (TRUE) or not (FALSE).", "This is when Logistic Regression can be useful. We can select \u2018is_premature\u2019 column to \u2018What to Predict\u2019 and select all the columns we are interested in finding how they effect the outcome. By clicking on Run button it will produce a chart like below.", "Each bar is an error bar, the dot at the center is Odds Ratio that is to show how much a given variable would impact the probability of outcome. The blue color indicates the positive direction for the odds ratio and the red color indicates the negative direction. The gray color indicates that a given variable is not considered statistically significant to the outcome. I know some arguments around P-Value especially with the historically recommended threshold value (0.05), but we are still using P-Value being 0.05 as the threshold to show it gray or not.", "Anyway, you can see some races of the mothers tend to have the premature babies than the other races. Also, the bar for plurality (is the baby twin, triplet, etc.) variable shows that one value increase in the plurality (single to twin, twin to triplet, and so on) will increase the chance of being premature 2.5 times more.", "Even when you discovered a great insight from the Coefficient chart or table, if the quality of the model is not good, then those insights are useless. That\u2019s when you want to open Model Summary tab where you can see a list of the metrics such as AUC, Recall, Precision, P-Value, Log Likelihood, etc. to understand the quality of the model.", "One information useful under this tab is the base levels for the categorical variables. As mentioned above, the chance of having premature babies for some races of the mothers are higher, but what does \u2018higher\u2019 mean? Compared to what? This is when you want to check the base level of this \u2018mother_race_name\u2019 column. As you can see below, it\u2019s \u2018White\u2019.", "Therefore, we can understand that Filipino mothers have 5.8 times more chance of having premature babies compared to White mothers.", "You can also check what type of prediction this model can perform better under Prediction Matrix tab and compare the predicted values by the model against the actual values (answers).", "By the way, if you are familiar with R, this uses \u2018glm\u2019 function with the family being set as \u201cbinomial(link=\u2019logit\u2019)\u201d.", " under Analytics tab uses Random Forest algorithm to build prediction models and extract the variable importance information from the model.", "Having this information that tells us which variables have more power in influencing the outcome we want to predict is useful. But the obvious question with this information is, how do they influence the outcome?", "This has been hard especially for ensemble models like Random Forest which builds a bunch of decision trees based on random sample data and predict the result based on the majority vote. And there have been many great works being done to improve the explainability of such models. One amazing work we have found recently is called \u2018", "\u2019 from ", " and others. The promise is to provide how each variable influence the result by taking sample data and having the model predict the outcome. As the name suggests, it is designed specifically for addressing the explainability challenge of Random Forest, and we are finding it the most practically useful and easiest to understand the result compared to other works in this area.", "So, with v4.2, we are adding this functionality under Effects tab of Variable Importance.", "As I have shown above with Logistic Regression example, I\u2019m trying to understand what influences whether the babies are prematurely born or not. With this new \u2018Effect\u2019 tab with the information from ", " package, we can see that the odds of having premature baby increases as Plurality increases from single to twin, by the use of Cigarette, and as father age increases from 40 to 60, etc.", "By the way, I\u2019m expecting a baby in a few weeks, and this is why I\u2019m obsessed with \u2018premature\u2019 baby prediction! :)", "We have seen two examples of binary classification above, one is Logistic Regression and another is Random Forest, both of which tried to predict binary nature of the outcome (TRUE or FALSE). But one of the challenges for building the binary classification models is something called \u2018imbalanced data\u2019. With the example of the above baby data, premature babies are much less than non-premature babies. So if we just build a prediction model we might end up getting a model that predicts FALSE most of the times because that can simply increase the odds of being right.", "Here is how it looks when we build a prediction model with Random Forest for Variable Importance under Analytics view.", "As you can see, the model is predicting FALSE for most of the data. While this helps to increase the metrics like accuracy rate, but it won\u2019t help the metrics like AUC to measure the overall quality of the model.", "To address this problem, one of the most practically useful solutions is called ", " (Synthetic Minority Over-sampling Technique), which balances data by increasing the minority class, in this case, that is premature babies by artificially populating sample data that are similar to the neighboring data and reducing the majority class, in this case, that is non-premature babies by downsampling.", "And we have added this functionality as part of Variable Importance and Logistic Regression. Also, we are exposing it as a standalone data wrangling step, which you can access from the Plus button.", "Internally, we are using an R packages called \u2018", "\u2019 from Andrea Dal Pozzolo and others, which we find it practically useful.", "Anyway, with the above example, when you set \u2018Yes\u2019 to \u2018Fix Imbalanced Data\u2019 we can see that the model is performing better for predicting a case of premature babies (TRUE).", "And as the result, we can also see some of the metrics such as AUC, F Score, etc. have improved.", "We have added a few new chart types, but more importantly, we have made massive enhancements around existing Chart, Map, and Pivot Table.", "Yep, finally. And I don\u2019t think this one requires additional explanations. ;)", "This is a variation of Scatterplot. You can assign a column to Group By to decide how many bubbles you want to show. And this drives all the aggregation of the data for X / Y Axis, Size, and Color.", "This is to show a single aggregated value. Using this by itself under Viz tab is not useful at all, but it becomes useful when you have metrics you want to keep track and include them inside your Dashboard.", "We have added a few useful interactive actions for charts. First, by clicking on a data point on the chart you can see the summary information in a pop-up.", "You will see three action menus at the top.", "By selecting either \u2018Keep Only\u2019 or \u2018Exclude\u2019 menu you can create the chart level filter quickly. Here, I\u2019ve selected DL (Delta) and AA (American) carriers and clicking on \u2018Exclude\u2019 menu.", "This will create a chart level filter to exclude DL and AA from the chart data.", "By clicking on \u2018Show Detail\u2019 menu, you can see the related rows of data with all columns at this step.", "You can select multiple points by pressing Command (Mac) or Control (Windows) key.", "This is similar to the new Bubble chart mentioned above. By assigning a column to ", " you can see as many circles on the map as the number of values of the column. And this drives how the size and the color of each circle are calculated with each aggregation function assigned respectively.", "Here, I\u2019m assigning \u2018country\u2019 column to ", " to show a circle on each country.", "By changing from Country column to Continent column for ", ", now we can see only 5 circles that represent the 5 continents.", "Trend Line support was there since v3.1, but it was only for Scatterplot. With v4.2, you can show the trend line with Line chart as well.", "And as an added bonus, we now show the model metrics like P-Value, R Squared when mouse over on the trend line. This is for both Line and Scatter charts.", "It supports Linear Regression, GAM and Loess models for calculating the trend line, and you can quickly switch between them from the property dialog.", "We have added Reference Line for Bar, Line, and Scatter charts. There are various ways to calculate the values for the reference lines and you can format the lines with various colors and styles.", "Here, I\u2019m showing the average departure delay time as a Red color solid line.", "You can also show the reference lines for both X and Y Axis. Here, I\u2019m showing the average ages of father and mother by mother\u2019s race. By showing the reference lines of average age for all the races for father (Y-Axis) and mother (X-Axis) it\u2019s easier to see what races tend to get married at earlier ages or later.", "You can also calculate the values for the reference line by X-Axis data points. This is useful when you want to show a reference line that is a series of the aggregated value for each data point. Here, I\u2019m showing the average weekly temperatures of six different cities as red color to emphasize which cities are hotter or colder.", "Another useful example is to show the total values that combine all the lines. Here, I\u2019m showing a total revenue trend as Red Dotted line and revenue trend lines for three segments.", "Sometimes you want to show two measures with different scales or units on the same chart. The problem is that when they have very different scales or units it becomes harder to see one of the measures.", "This is when you want to assign them to different Y-Axises, either Y1 (left side) or Y2 (right side).", "This often makes the visualization a bit more confusing, so it might not be an effective option for your exploratory data analysis, but it\u2019s still useful for reporting with Dashboard, Note, and Slides.", "We have added a few additional Window Calculation types.", "First, there is \u2018Sum Ratio\u2019 under Cumulative calculation type. This is useful when you want to see how much of the values are captured accumulatively. The example below shows that the first three countries of United States, Japan, and United Kingdom capture about 80% of the users.", "We have revisited \u2018% of\u2019 type Window Calculation. First, \u2018% of Total\u2019 was there even before, and this is useful when you want to see the ratio of each member.", "You might be more familiar with this \u2018% of Total\u2019 option when a column is assigned to Color and the calculation direction is \u2018Color\u2019 instead of \u2018X-Axis\u2019.", "But this \u2018% of Sum (Total)\u2019 is useful even when there is no column assigned to Color. For example, with the chart like below, you can see what is the percentage of the users coming from each country.", "Now, if we change the \u2018Summarize Values\u2019 option to \u2018First\u2019, we can see what is the percentage of the users for each country compared to the number of the users in the first column, in this case, that is United States.", "Now you can set the skip (or increment) values and the data range (Min and Max values) at Y-Axis Label for Bar, Line, and Area chart. You can set them for both X and Y-Axis for Scatter chart.", "There are two useful enhancements for Pivot Table.", "Now the column level sorting sorts the data for each group. In the example below, you want to sort the countries based on the values under \u2018darwin\u2019 column.", "By clicking on the column header the countries are sorted only within each group of the first column, in this case, that is \u20182016\u201311\u201301\u2019, \u20182016\u201312\u201301\u2019, etc.", "Clicking on the column header one more time will sort the data in a descending order.", "We have added all the ", " that have been supported for charts to Pivot Table. Supporting them for Pivot Table was a bit tricky because it has Row and Column, and the Row can have multiple levels, so the way we would expect the window calculations to be done is slightly different than other charts like Bar, Line, etc.", "So we are introducing the following 5 different types of the directions which each of the window calculations would follow.", "Let\u2019s quickly take a look at each option with a Cumulative Sum (Running Total) example.", "With \u2018", "\u2019 direction, you can see that the values are getting accumulated for each row as it moves towards to the right.", "With \u2018", "\u2019 option, you can see that the values are getting accumulated for each column as it moves from the top to the bottom.", "With \u2018", "\u2019 option, the values are getting accumulated as it moves from the top to the bottom direction, however, the accumulation stops at the end of each group (e.g. AA) and restart with a new group (e.g. DL).", "With \u2018", "\u2019 direction, the values are getting accumulated from the top to the bottom of each group, just like the previous option. However, it doesn\u2019t stop the accumulation at the end of the group, instead it keeps going by moving towards to the right (e.g. from AZ column to CA, then CO, then FL, and so on.)", "Finally, with \u2018", "\u2019 direction, the values are getting accumulated from the top to the bottom ignoring the groups, just like the \u2018Column (Down)\u2019 direction. However, it doesn\u2019t stop the accumulation at the bottom, instead it keeps going by moving towards to the right (e.g. from AZ column to CA, then CO, then FL, and so on.)", "There can be other directions such as \u2018Row (Across then Down)\u2019, and we will keep adding more of such options in the future releases, so stay tuned!", "Sometimes you want to run the same data transformation not just on one column, but also on multiple columns. For example, you might want to convert the data type from character to date for multiple columns. Or, you might want to normalize (scale) numeric values for multiple columns. You can do such one by one, but obviously that\u2019s cumbersome and it will create a maintenance nightmare later.", "This is why dplyr has a set of functions called \u2018mutate_at\u2019, \u2018mutate_if\u2019, \u2018mutate_all\u2019, in \u2018mutate\u2019 family. I\u2019ve written a post for more details on each function, and you could type such commands with Custom Command input in Exploratory even before.", "But with v4.2, we are introducing a new UI to make this type of multiple columns operation easier. You can access it by selecting multiple columns by pressing Command key (Mac) or Control key (Windows) or Shift key (both Mac and Windows) and selecting either \u2018Change Data Type\u2019 or \u2018Work with \u2026\u2019 menu from the column header menu.", "Let\u2019s say, you want to normalize (scale) numeric values for multiple columns like below. You can select \u2018normalize\u2019 function under \u2018Work with Numeric Function\u2019.", "This will normalize (or scale) these columns at once with \u2018", "\u2019 function from \u2018exploratory\u2019 package.", "Or, let\u2019s say, you want to convert the data type from characters to Date for multiple columns like below. You can select those columns and select \u2018Convert to Date / Time\u2019, then select \u2018Year, Month, Day, Hour, Minute, Second\u2019 from the column header menu.", "This will convert these columns to POSIXct (Date/Time) with \u2018", "\u2019 function from \u2018lubridate\u2019 package.", "When you select those operations from the column header menu you will get a dialog like below.", "You can simply click on \u2018Run\u2019 button to execute right away, but additionally, you can change the way you select the columns at the left side of the dialog and change the operations at the right side of the dialog.", "With v4.2, you can move the steps at the right hand side by simply dragging a step and dropping it wherever you want! This would be useful especially when you want to move the steps like Mutate, Filter, Join, etc.", "We have added more filter options for Date and POSIXct (Date/Time) data type columns. There are two main areas of improvements. One is to support \u2018relative\u2019 date types. For example, you might want to filter the data for the previous year like below.", "This would generate a filter command like below.", "Another area of the enhancement is the capability of filtering the data by using Date/Time function like \u2018year\u2019, \u2018month\u2019, etc. internally. For example, you might want to filter the data just for 2017. Then, you can select \u2018Year\u2019 for Value Type parameter and simply type \u20182017\u2019.", "This would generate a filter command like below.", "Most of the data analysis work can be very experimental especially when you are at exploratory data analysis mode. And in that mode, you want each data wrangling and visualization operation to be performed as fast as speed of your thoughts, but obviously, the larger the data is, the slower the operation can become.", "With v4.2, we are introducing \u2018Sample Data\u2019 mode. You can now click a button called \u2018Sample\u2019 at the top, which will reduce down the data size by sampling the original data that is imported, and continue on your data exploration. After you know what you want, then you can click the button again to get out of the sampling mode and apply all the steps you have added at the right hand side of Data Wrangling pane to the whole data.", "When you have missing values or NAs in your data, sometimes the solution is to just copy the previous values (carry forward) or the next values. In R, there is this function called \u2018", "\u2019 from \u2018", "\u2019 package that does this job easily. With v4.2, we have added an UI for it so that you can access it from the column header menu.", "Here\u2019s a historical US beer tax rate data, and let\u2019s say we want to fill \u2018RATE\u2019 column below by copying the previous values.", "We can select \u2018Replace / Convert Data\u2019 and \u2018Fill NA with Previous / Next Value\u2019 from the column header menu.", "And this will fill the NAs with the previous non-NA values.", "When you want to create Bins (or Categories) for numeric columns, you can select \u2018Create Bin (Category)\u2019 menu from the column header menu. Now you can type the label text for each bin when creating bins with \u2018Manual\u2019 option.", "We have added a new data source called \u2018Text File (Raw Lines \u2014 Not Delimited)\u2019 to read a text file by line by line to import the data.", "Now we support scheduling Chart, Note, Slides, and Dashboard with data that is coming from Google BigQuery as well.", "In the previous release, we had a bug that it failed to save the Note (or Slides) content appropriately and ended up overriding the existing with a wrong information. This can be extremely frustrating (ok, upsetting actually!) especially after you spend hours of writing or editing. Of course, we have fixed the bug in v4.1 patch and added more test scenarios around this area since then. But, to be 100% confident that our users won\u2019t waste their times by a similar problem again in future, we have added \u2018", "\u2019 feature for Note and Slides. With this feature, now you can restore from any state of the past history of your Note or Slides.", "Click on \u2018History\u2019 button.", "Select a state in the history and click \u2018Restore\u2019 button.", "Each \u2018Restore\u2019 operation itself becomes a new state (or version), so you can switch between different versions of Note.", "This is a bit of technical information about saving Note and Slides only for those who are interested. Note and Slides are automatically saved every time you type or insert charts. However, this automatically saved contents are once stored inside the application temporally. When you move away from Note or Slides such as moving to a data frame, closing the project, etc., then it will save the content onto the file system where your Exploratory\u2019s repository is located. This is called \u2018persistent\u2019 save, and it didn\u2019t happen before as long as you kept opening Note or Slides. But with this v4.2, the \u2018persistent\u2019 save also happens automatically every 10 minutes. By the way, this \u2018persistent\u2019 save is done via Git, the source control system that you are asked about when you install Exploratory Desktop for the first time. And this is why you can restore any older versions with a click of the button. \ud83d\udcaa And you should expect this feature to be expanded to cover other areas in future releases as well!", "Phew! There are A LOT!", "It might take some time for you to digest all the features, but we think this release is going to make your exploratory data analysis and reporting much more productive and effective. Hope you will like it as much as we do once you upgrade and start using it!", "Make sure to check ", " for all the enhancements and bug fixes and ", " from our ", " page to start exploring it today!", "If you don\u2019t have an account yet, sign up for a 30 days absolutely free trial! If you happen to be a current student or teacher at schools, it\u2019s free!", "Written by"], "postingTime": "2018-02-20T03:26:35.885Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Sam DeBrule", "articleTile": "Machine Learnings \u2014 How to develop an AI product - Machine Learnings", "content": ["\u201cThe internet is riddled with websites set up for the sole purpose of stealing a user\u2019s information or installing malware on a victim\u2019s machine. Antivirus companies blacklist them as fast as they can, but with new sites launched every day, it\u2019s a Sisyphean effort to keep up. A new [machine learning] system called URLNet uses neural networks that look at character-level and word-level combinations in \u2014 you guessed it \u2014 the site\u2019s URL to detect a site\u2019s risk. URLs contain clues to whether a site is malicious, like length and misspelled domain names. \u201c \u2014 Jackie Snow, Editor ", "\u201cAI challenges global security because it lowers the cost of conducting many existing attacks, creates new threats and vulnerabilities, and further complicates the attribution of specific attacks\u2026such as: persuasive ads generated by AI systems being used to target the administrator of a security systems; cybercriminals using neural networks and \u201cfuzzing\u201d techniques to create computer viruses with automatic exploit generation capabilities; malicious actors hacking a cleaning robot so that it delivers an explosives payload to a VIP; and rogue states using omniprescent AI-augmented surveillance systems to pre-emptively arrest people who fit a predictive risk profile.\u201d \u2014 ", "1/ The top minds in AI are getting together to \u201cforecast how malicious actors could misuse AI technology, and potential ways we can prevent and mitigate these threats.\u201d ", "2/ In many cases, conversational AI bots aren\u2019t improving at the rate we\u2019d expect since people aren\u2019t willing to stick around and keep using them when their responses feel off. ", "3/ As New York City Mayor de Blasio announces a task force to hold the city\u2019s agencies responsible for how they use algorithms to make decisions (like putting people in prison), a research organization recommends the framework it hopes to be the \u201cfoundation [for] defining meaningful algorithmic accountability.\u201d ", "4/ If we continue to put our blind faith in black box AI algorithms we run the risk of perpetuating inequality and locking disadvantaged communities in people in place. ", "5/ While the largest Tech companies salivate over the prospect of winning the self-driving car market, a battle is being waged that will determine which maps these cars will use to \u201csee.\u201d ", "6/ If we\u2019re unwilling to admit that AI can (and will) have many negative impacts on the future workforce, we won\u2019t make the changes necessary to the education and job training ecosystem we\u2019ll need to ensure a future that doesn\u2019t totally suck. ", "7/ To understand the priorities of a large tech company, all you need to do is take a quick glance at the seating chart. It shouldn\u2019t then come as a surprise that AI researchers are taking over the most coveted seats in the office: right next to the CEO. ", "Knowing that 81% of IT leaders are currently investing in or planning to invest in Artificial Intelligence (AI) is no surprise [1]. This is a technology that is going to change the way we work and live, so companies are trying to keep up.", "Launching products with AI as its core technology is not simple \u2014 the talent pool is limited, the landscape very competitive, and the technology is still a challenge, despite all the progress that was made in the last couple of years. But there is one additional challenge: Product Management in the context of AI.", "Being an area that got popular in the last years, the overall experience in the field is limited and the information available to plan and decide about AI applications is also limited. We are all learning as we do things, but at the same time we have very high expectations.", "Let me tell you a very common story of what can be the relationship between a manager and a team in charge of the next product or feature involving AI.", "The manager promised that the new AI-enabled feature would be ready by the end of the quarter and is working with the team to bring it to life. Since competitors and pretty much everyone on the internet brag about the latest AI-based innovation, both the stakeholders and the manager believe this is no rocket science and things will work the way they should.", "On the developer side, everything is a challenge: the data needs to be prepared, they still didn\u2019t figure out the best algorithm, the architecture is complex, technology is not there yet. What some people believe to be an \u201ceasy task\u201d is not.", "As a result, everyone is frustrated: the team cannot make things work as expected, managers don\u2019t understand why things don\u2019t work and the stakeholders assume everyone is incompetent. The truth is, nothing looks good in this scenario.", "The lack of experience developing uncertain things \u2014 AI is a very good example given how recent the commercial deployment of AI is \u2014 leads to serious estimation errors and broken expectations everywhere. Shit happens when you make commitments without having enough information to decide.", "This wouldn\u2019t be a challenge if companies were truly flexible, willing to run experiments to gather more information and quick at iterating. In reality, even the ones that claim to be flexible are only typically doing it at the delivery phase, by employing agile methodologies\u2026", "\u201cPredicting ICO returns with machine learning\u201d by ", ". ", "\u201cHaving access to \u201cinfinite compute\u201d is absolutely not a necessary factor for doing good research\u2026.\u201d submitted by ", ". ", "\u201c\u2018Automating Inequality\u2019: Algorithms In Public Services Often Fail The Most Vulnerable\u201d submitted by ", ". ", "Written by"], "postingTime": "2018-02-26T20:23:02.623Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Sam DeBrule", "articleTile": "Machines for creative enablement - Machine Learnings", "content": ["A little while back, ", " of ", ", wrote a piece about machines being used for creative enablement. I think it\u2019s important enough to warrant re-surfacing. Read it in the \u201cWhere we\u2019re going.\u201d section below!", "\u201cAI is probably the most important thing humanity has ever worked on. I think of it as something more profound than electricity or fire\u2026I think a lot of things will play out in more positive ways than people think. But the risks are important. Any time you work with technology, you need to learn to harness the benefits while minimizing the downsides.\u201d ", " ", "\u201cChina\u2019s advantage over the West in the biotech, artificial intelligence, and machine learning race: They\u2019re not pausing to have key ethics debates, where we take years. This is a fight between democracies and techno-authoritarians.\u201d ", "1/ Concerns around AI algorithms\u2019 ability to explain how they make decisions have slowed their introduction into fields ranging from medicine to military systems \u2014 and one investor argues we shouldn\u2019t worry about this \u2018black box.\u201d ", "2/ Another investor, speaking about his experience as a Google Now user thinks just the opposite \u2014 \u201cwhat I want\u2026everywhere that machine learning touches me, is a \u201cwhy\u201d button I can push (or speak) to know why I got that recommendation.\u201d ", "3/ As we embed more machine learning algorithms in our technology experiences, we need to optimize for \u201caugmenting human capability,\u201d not just making the algorithm smarter. ", "4/ If you live in the US, and someone uses an AI app that doctors your face onto an actor in a pornographic video, you have no ground for legal recourse against them. ", "5/ For the foreseeable future, we should expect machine learning to reduce costs rather than increase revenues since \u201cwe haven\u2019t found a way, yet, to have computers do [unpredictable and novel] things for us\u2026\u201d ", "6/ Now that closed circuit TVs can use machine learning to analyze endless streams of video footage without human intervention, we as a society will have a new brand of privacy issues to grapple with. ", "7/ Facebook makes major changes to its AI leadership, as Yann LeCun steps down as head of their research organization, and Jerome Pesenti takes over. ", "We think the future workplace will be one that\u2019s led by people who excel at finding and acting quickly with their information.", "But today, our work is fractured. Millions of unnecessary steps stand between us and the information we need to do our jobs.", "Think of Journal as a companion app that connects your tools. ", ".", "\u201cHow should brands use future technologies?\u201d by ", ". ", "\u201cHow to solve 90% of NLP problems: a step-by-step guide\u201d submitted by Genevi\u00e8ve Smith (", "). ", "\u201cAlgorithms are making American inequality worse\u201d submitted by ", ". ", "\u201cEngineers design artificial synapse for \u201cbrain-on-a-chip\u201d hardware\u201d submitted by ", " (", "). ", "\u201cThe accuracy, fairness, and limits of predicting recidivism\u201d submitted by ", ". ", "\u201cEp 42 \u2014 Deep learning, autonomous weapons, and the future of AI with Stuart Russell\u201d submitted by ", ". ", "The information we publish on the web pales in comparison to the totality of information inside our heads. Companies like Jelly and Quora have jumped on the opportunity to expose that knowledge in a people-powered search engine, but if we take a step back, there\u2019s a more fundamental question to be answered: what about making tools that help us better harness the stuff of our own minds?", "Can we use technology to fundamentally think and express more powerfully, and not just to make our lives marginally more convenient (one pizza at a time)?", "The rise of software and the web has opened up many more potential dimensions for idea conception and expression. Yet the narrow digital channels we have today \u2014 whether self-expiring video or 140 character snippets of text \u2014 barely scratch the surface of what\u2019s possible. As far as pushing the limits of what web-connected software can do (for us), we\u2019re still splashing about in the kiddie section of the pool.", "This is not to deny the profound emotional significance in the act of experiencing the world through another person\u2019s eyes, or in the value of realtime access to global events. But it would be comically absurd to replace high school literature and writing with training courses for how to express oneself on Twitter, Facebook or Snapchat \u2014 because we\u2019d be foregoing opportunities to grow the depth of our thinking.", "Yet this sort of deprivation is exactly what happens when we only use software to think about and communicate simple things faster and further. In your pocket and on your desk sit devices already brimming with the latent potential for interactive, multidimensional idea expression, but we\u2019ve instead turned them into buckets full of single-purpose, push-button appliances\u2026", " ", "Co-Founder and CEO of ", "Written by"], "postingTime": "2018-01-29T22:25:40.287Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Sam DeBrule", "articleTile": "Machine Learnings \u2014 Facebook\u2019s big algorithm change", "content": ["\u201cDeath prediction is an emerging field in medical technology and artificial intelligence. [A] company, Aspire Health, claims its algorithms can save families thousands by predicting the point at which elderly patients should shift from hospitals, where they receive ongoing treatment, to in-home palliative care, where they\u2019re made comfortable but aren\u2019t expected to recover. The families save money by foregoing expensive procedures that won\u2019t buy much time.\u201d", "\u2014 ", " ", "\u201cIn 2015, A black software developer embarrassed Google by tweeting that the company\u2019s Photos service had labeled photos of him with a black friend as \u201cgorillas.\u201d \u2026More than two years later, one of those fixes is erasing gorillas, and some other primates, from the service\u2019s lexicon. The awkward workaround illustrates\u2026 a shortcoming of existing machine-learning technology.", "With enough data and computing power, software can be trained to categorize images or transcribe speech to a high level of accuracy. But it can\u2019t easily go beyond the experience of that training. And even the very best algorithms lack the ability to use common sense, or abstract concepts, to refine their interpretation of the world as humans do.\u201d ", "1/ In response to public outcry and research that points to psychological harm caused by passive consumption of information on social media, Zuckerberg urges Facebook\u2019s product managers change their algorithms to prioritize \u201cmeaningful interaction.\u201d ", "2/ For many of us, our sense purpose is tied to the work we do. Over the coming decades as automation transforms the nature of our work, our sense of self-worth will transform too. ", "3/ Investors are increasingly turning to machine learning programs to enhance the way they manage their funds, and a firm\u2019s power will hinge upon their ability to \u201charvest\u201d data. ", "4/ Companies have risen and fallen on the back of the Facebook algorithm. This most recent change to their News Feed algorithm will hurt many media companies, but it may come at the benefit of many more individuals. ", "5/ In a move that will set a precedent for the global Tech ecosystem, the French government broadens its powers to potentially block foreign acquisitions of AI companies. ", "6/ You won\u2019t be able to use the technology for many years, but scientists have created the first algorithm \u201cto interpret \u2014 and accurately reproduce \u2014 images seen or imagined by another person.\u201d ", "7/ A team of 100 people runs a 24-hour drive-a-thon on with its self-driving car to learn what it will take to keep a ", "of autonomous vehicles running at \u201cmaximum capacity.\u201d ", "Whether you need to reference a piece of information in a document you\u2019re writing or answer a colleague\u2019s question, Journal helps you find information that\u2019s shared across your work apps so you can act quickly.", ".", "\u201cTurning Design Mockups Into Code With Deep Learning\u201d submitted by Avi Eisenberger (", "). ", "\u201cAccountability, Generalizability, and Rigor in Finance Research: Machine Learning in Markets (Part II)\u201d submitted by Samiur Rahman (", "). ", "Join 30,000+ people who read the weekly ", " newsletter to understand how AI will impact the way they work and live.", "Written by"], "postingTime": "2018-01-15T18:44:47.837Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Sam DeBrule", "articleTile": "YouTube\u2019s algorithm is back in the news for all the wrong reasons", "content": ["\u201c\u2026.Alonso Martinez [technical director at Pixar Animation Studios] pointed out that we have an extremely accurate technological approach to pain quantification in the form of MRI scanning, but that this is expensive and highly invasive. We can also imagine it being unnecessary in an era where machines can parse vast troves of data to build predictive models of once unquantifiable things, such as suffering reflected in the microexpressions of a face\u2026\u201d ", " ", "\u201cThe current language in the killer robot debate suggests that those weapons are capable of acting without meaningful human control, and that their creation and use is somehow distinct from other sorts of collective actions. It also suggests that potential harm arising from that creation and use may be morally unattributable to those who create and use them. This is not the sort of moral detachment we should foster in our technology and military communities, especially in relation to what is perhaps the gravest and most consequential of all human activities: war.\u201d ", "1/ YouTube\u2019s recommendation algorithm \u201cdoes not appear to be [optimizing] for what is truthful, or balanced, or healthy for democracy,\u201d and the impact it has on our political system could be hugely underestimated. ", "2/ For all the progress that has been made in the field of AI lately, it still feels like we\u2019re far from some major breakthroughs \u2014 like ones that will save humans from doing work we can\u2019t stand, and helping us do a better job of the things that we\u2019re not so good at. ", "3/ There\u2019s a counter-intuitive narrative emerging that autonomous trucks will create more trucking jobs than they eliminate. However, it\u2019s unclear whether or not this narrative is just wishful thinking from a company that has a lot riding on this narrative becoming true. ", "4/ Many regulators would argue that in order to prevent injustices caused by AI systems, we must make sure that algorithms can \u201cexplain themselves.\u201d The problem is, keeping algorithms simple enough to be explicable means slowing down massive potential progress. ", "5/ Amazon shakes up the business world when it announces it\u2019s partnership with Berkshire Hathaway and JP Morgan to tackle healthcare \u2014 but it turns out China\u2019s giant companies may have a major AI advantage in the space. ", "6/ Much has been made of the machine learning algorithms that power Google Translate replacing the need for human translators, but there\u2019s probably a long way off. ", "7/ For every fleet of autonomous vehicles on the car, you should expect a remote human operator in some faraway office trying to make sure that everything moves smoothly. ", "We think the future workplace will be one that\u2019s led by people who excel at finding and acting quickly with their information.", "But today, our work is fractured. Millions of unnecessary steps stand between us and the information we need to do our jobs.", "That\u2019s why we\u2019re building Journal, a companion app that connects your work apps. ", ".", "\u201cMeet the Company Trying to Democratize Clinical Trials with AI\u201d submitted by Avi Eisenberger (", "). ", "\u201cAndrew Ng officially launches his $175M AI Fund\u201d submitted by Samiur Rahman (", "). ", "\u201cNeva is now Astound. Here\u2019s why.\u201d submitted by Dan Turchin (", "). ", "Written by"], "postingTime": "2018-02-05T20:19:48.522Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Sam DeBrule", "articleTile": "Machine Learning won\u2019t reach its potential without the human element", "content": ["We\u2019re excited for ", " to join us this week! Sarah is an investor at ", ".", "Sara and I spoke this week on topics ranging from machine learning\u2019s impact on social media to the role it plays within companies.", "Sarah! Thanks so much for your time. I\u2019m pumped to talk. Can you start by shedding light on your background and the types of companies you invest in?", " Of course. I recently joined ", ", a seed stage VC that invests in technical founders solving technical problems. We focus primarily on Machine Intelligence, Analytics, and Enterprise Infrastructure. At a high level, my career has been focused on leveraging computational models to better understand complex data sets.", "A common refrain amongst VC\u2019s is, \u201call startups will eventually be machine learning startups.\u201d What do you think?", " I think it\u2019s reasonable to say ", " because there is a broad enough swath of problems to which machine learning can apply.", "But, not all problems are suited for machine learning. It should and will be a tool in everyone\u2019s toolkit, but ML is best suited to a subset of problems where you have high volume, highly dimensional data and a well defined objective. There are many other problems, for example when only small datasets are available, where other methodologies may be more appropriate.", " Do you have any opinions about machine learning that would be unpopular amongst the VC crowd?", " I don\u2019t think that adoption of AI will hit a tipping point until we have more effective mechanisms and interfaces for consuming machine learning results. These mechanisms and interfaces must enable us to interpret and reason about uncertainty associated with our models. In other words, they should show what an algorithm based a decision on and how confident it is in its decision.", "For example, people get excited about academic research on medical diagnoses, then jump to the conclusion that these methods will be adopted by physicians. What people fail to recognize is the output of machine learning models must be used by people, employed by organizations, with responsibilities and liabilities. A physician, who faces malpractice risk, won\u2019t accept a diagnosis just because your model is \u201cmore precise.\u201d", "He/she is going to want to know how the model arrived at the diagnosis, why it might be wrong, and who is most likely to be affected if it is wrong.", "We don\u2019t yet trust AI and I don\u2019t think we will or can trust it until it can \u201cadmit\u201d what it doesn\u2019t know.", "For example, consider a system that recommends certain drug treatments based on medical records. The model underlying this system was developed on a training dataset. If the system encounters a patient who is unlike the patients profiled in the training set \u2014 that is, who lies outside its data distribution \u2014 it may make an unreasonable recommendation about a treatment plan. In this case, we would want to know that the system is not confident in its recommendation.", "Likewise, think about an autonomous vehicle, which is trained to identify trucks, motorcycles, and cars. If this vehicle encountered a new object, say a scooter, which it could not classify with certainty, we might want it to alert the user to take control of steering. People won\u2019t use self-driving cars or accept treatment from doctors aided by black-box algorithms if they can\u2019t ask questions like \u201chow do you know this\u201d or \u201cwhy are you sure?\u201d", "Medicine and autonomous vehicles are extreme examples, but there are other use cases like real-time bidding or inventory management. Businesses want to understand why and when their model might be wrong so they can do scenario planning and think sensibly about possible futures. If they don\u2019t their decisions can have very negative, cascading effects.", "What happens when we can interpret the algorithms?", " By enabling interpretability, we can better understand and even change our own behavior. For example, interpretable machine learning could clarify bias in our datasets (and therefore in our society), which we didn\u2019t know existed. Also, by understanding what features machines use, we could improve our own learning experiences.", "For example, Marco Tulio Ribeiro and other researchers at the University of Washington are finding that with interpretable machine learning models of Chinese characters, they can provide better guidance to students learning to read Chinese.", "So, it\u2019s not just about super negative repercussions if we can\u2019t explain models. But if we can understand them, we can enable and improve human behaviors.", " Allow me to pull you back to super negative repercussions :). There\u2019s been so much talk recently of the research coming out about the negative impacts of social media. How does machine learning make this worse?", "Machine learning on these platforms determines what we see and pay attention to. Even things as simple as the order in which news articles are displayed can impact what we read first, which can impact what we pay the most attention to, and could even affect our behavior that day. For example, an article that is prioritized in a news feed may elicit a stronger reaction. If we\u2019re made to feel strongly enough, we might decide to write to our congressman, which could ultimately impact policy.", "There\u2019s a huge power mismatch associated with data. On one hand, algorithms are trained using data collected by tech behemoths about our preferences, social networks, buying patterns. However, we have no transparency into when and how those algorithms are applied. So perhaps there\u2019s a case to be made for interpretability here too.", "For example, how might I respond to news article ranking if I knew it was generated algorithmically? Would I respond differently if I knew that the author\u2019s ethnicity was used as a feature in the ranking algorithm? Why not allow me to specify the features that are applied to my ranking? If we\u2019re more transparent about our algorithms, we can give more agency back to the consumer.", "How then do we make sure that machine learning is built ethically within organizations?", " As machine learning gets commercialized, it won\u2019t be the sole responsibility of machine learning engineers and mathematicians to ensure AI safety but of all people who build products \u2014 from UX designers to product managers to sales and marketing.", "While I don\u2019t think that everyone needs to be able to implement machine learning, everyone in the company should understand it and should become more data literate so that they can reason about the consequences of machine learning. We have to hold everyone in an organization responsible for AI safety, not just the machine learning engineers.", "Truth be told, machine learning is broken in a lot of organizations. Too often, machine learning researchers are isolated from the rest of the organization. As a former Data Product Manager, I think the way product managers interact with machine learning developers is often broken.", "They think of machine learning as a silver bullet that produces a completely clean wound. Product needs to get smarter about how machine learning is wielded and about its hidden costs; PMs need to provide more direction about how its output will be consumed.", "It\u2019s right to hold machine learning developers responsible for optimization metrics (although even they need to think beyond precision and recall), but product managers need to understand how certain design decisions can impact the user perception, including of precision and recall since these perceptions actually affect how the user responds.", "Machine learning has been a bit too isolated. It needs to be brought into the fold, and results need to be tied back to organizational directives.", "What excites you most about the future of machine learning?", "Honestly, I\u2019m most excited by how we use machine learning to understand and improve ourselves, including through augmented intelligence. Machine learning can help us better understand our own biology and psychology and if applied responsibly, it can also help us behave better and make more conscientious and less biased decisions.", "Machine learning can help us personalize and accelerate education and could even one day facilitate business conversations and negotiations. I know it sounds idealistic, but I really want us to use machine learning to become better people.", "Today, there is a lot of paranoia about AI and automation replacing jobs. I think, and perhaps hope, people will start to react to this fear, by considering augmented intelligence. How can we use machine intelligence to make ourselves more efficient workers (so that we won\u2019t lose our jobs to robots and so that we can have more productive economies)?", " That is an interesting question indeed! Thanks again for your time, Sarah. Looking forward to doing this again.", " Talk soon!", "Join ", " who read the weekly", "newsletter to understand how AI will impact the way they work and live.", "Written by"], "postingTime": "2018-01-08T16:28:52.706Z"}
{"nameOfPublication": "Machine Learnings", "nameOfAuthor": "Michael Dempsey", "articleTile": "Moving from a default trust to default skeptic society", "content": ["I\u2019ve been spending a lot of time thinking about how new forms of AI and machine learning will shape society over the next decade. One area I\u2019ve gone deep on is how ", " will manifest itself in our daily lives.", "My main takeaway in my research thus far is this:", "Or put another way, while today humans largely trust what they hear/see, tomorrow they will not.", "In my lifetime I\u2019ve seen this shift towards skepticism take place. ~10 years ago, people believed what they could see in relative high fidelity and what they were told in major media outlets. Photoshop made people skeptical of incomprehensible images. Smartphones, social media, and internet publishing has given to the rise of clickbait and \u201cfake news\u201d making society in 2018 increasingly more skeptical about internet content (and ", ") than in 2017.", "This started with the rise of creative editing tools, has moved to a data-driven understanding of human beings and how they consume information, and ends with how machine learning upgrades the scalability in creating and manipulating ", ".", "But today seeing is still believing and we trust certain sources to continue to report the truth and we default believe them.", "What happens once the first person/group displays the power of scalable and believable fake audio or video as a weapon? We recently saw an early sign of this with ", ", a technology used to superimpose celebrities\u2019 faces in porn (", ").", "The result? The internet collectively freaked out and multiple platforms took an uncharacteristically censorship-heavy approach, quickly banning the content. And while deepfakes often manipulate something as sacred as sex acts, the targeting of celebrities in our sex-desensitized world makes me believe that the potential impact on society technologies like this could have will not be realized by similar manipulations, but instead with what could happen next.", "DeepFake and whatever follows it will plant a seed of doubt and skepticism in people\u2019s minds and destroy the idea that you \u201chave to see it to believe it\u201d. But only once false assets are used for something as meaningful as setting off some sort of widespread trigger across financial markets or government agencies, or when they change the outcome of something as large as an election (again?), will that seed blossom into a mindset shift for society.", "Eventually this could trickle down from \u201cimportant to verify\u201d facts to day-to-day conversations. As a generation that records everything we do in order to create social proof, will our existing digital footprints become worthless as quickly as our feeds became \u201cvaluable\u201d? Will the rebirth of trust in these types of assets come from a return to analog mediums?", " when our worlds are already being overrun by multiple realities and we are hopping between physical and digital worlds.", "These are all interesting exercises to think through, but a lot of how I look at the world as an investor as well as a human opining on the future is through ", ". While I\u2019m not confident in what will be the catalysts for this particular case, I\u2019m fairly confident that we are fast approaching an inflection point in how humans internally compute all new information. From there we have to figure out whether the power of shared beliefs and incentives can keep us a default trust society, or if a new wave of technology will make us default skeptics.", "Written by"], "postingTime": "2018-02-16T20:58:05.333Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "A Beginner\u2019s Guide to EDA with Linear Regression \u2014 Part 2", "content": ["This is the second post in a series of \u201cA beginner\u2019s guide to EDA with Linear Regression\u201d, continued from ", ". If you haven\u2019t read it yet, I\u2019d strongly recommend you start with the first post.", "So far, we have investigated if ", " and ", " were impacting ", ", and we know that both Father Age and Mother Age influence the changes in Gestation Week. But since we have done the investigation separately, one for Father Age\u2019s influence on Gestation Week and another for Mother\u2019s Age\u2019s influence on Gestation Week, we still don\u2019t know which of Father Age and Mother Age is the direct cause of the influence.", "In this post, I\u2019m going to investigate further to find this out.", "So far, we know that the increases in ", " would make ", "shorter. And, the increases in ", " would also make ", "shorter.", "But wait for a second\u2026", "Aren't younger mothers tend to have babies with younger fathers? And the same goes for older mothers, aren\u2019t older mothers tend to have babies with older fathers? I mean, aren\u2019t Father Age and Mother Age in this data positively correlated?", "Take a look at this Scatter chart that has ", "at X-Axis and ", "at Y-Axis below.", " and ", " are ", ", which means that when Father Age increases Mother Age also increases.", "By using Correlation analysis under Analytics view in ", ", we can see that the correlation between them is 0.75, which means they are moderately positively correlated.", "Now, this brings up a question.", "When we say that ", " would influence the changes in ", ", is Father Age alone really influencing the changes?", "Let\u2019s give it a fictional example.", "This older father had a baby recently, and the baby came out earlier than the average (Shorter Gestation Week).", "Now, is this because the father is older?", "Or, maybe his wife or partner happened to be also older and could that be actually the direct influence of having the baby earlier than the average?", "So instead of ", " directly influencing ", ", maybe ", " is influencing ", " to be younger or older, and then ", " ends up directly influencing Gestation Week to be shorter or longer.", "To answer this question we can bring Father Age and Mother Age together as the predictor variables and re-build the regression model.", "Why?", "Well, the answer is in how we interpret ", ".", "When we have multiple predictor variables we can interpret each coefficient of the variables as \u201cone point increase in a given predictor variable would increase or decrease a certain amount in the target variable ", ".", "This last part of the sentence is very important. It means that we can say if one year increase in ", " would increase or decrease some amount of ", " when Mother age stays the same, for example. So we can basically separate the potential influence of ", " from the influence of ", ". This goes the other way around, too. We can evaluate the Mother Age\u2019s influence alone without a potential influence of Father Age by having these two variables as predictors.", "Let\u2019s take a look.", "I have brought both ", " and ", " as the predictors and re-built the Linear Regression model.", "Here is the coefficient chart.", "Notice that ", " is now gray color, which means its P-value is greater than 0.05. As we have seen how we interpret P-value in the ", ", we can\u2019t conclude that Father Age\u2019s influence on Gestation Week is statistically significant.", "But\u2026 the P value is 0.06.", "Again, the P value is really saying that the probability of getting the similar changes in ", " is about 6% when assuming ", " and ", " have no relationship. And this probability of \u20186%\u2019 can be considered as pretty low for some situations. On top of that, is there much difference between the well-known threshold of 5% and 6%, just one percent difference!", "But that\u2019s not the point here. The point I want to make here is that P-value for ", " variable has increased a lot and came to a point where we are not so sure if we should be confident that ", " is really influencing on ", "when ", ".", "Let\u2019s take a look at ", " variable. Its P-value is much smaller compared to the one for ", ".", "So the probability of getting the similar changes in ", " without any influence of ", " when ", " stays the same is very low. So if anything, ", " can be more reliable to explain the changes in ", "than ", ". And this means that we can interpret that ", " is probably the one that is more of a direct influence on ", ". And the reason ", " looked influencing on ", " in the previous analysis could be because it happened to be correlated well with ", ".", "So what we are finding is something like this.", "The coefficient estimate of Mother Age is -0.02, which means that one year increase in ", "would make ", " 0.02 weeks shorter when ", " stays the same.", "Now that we are finding that ", " is most likely the one influencing the changes in ", " more than ", ", does this mean that ", " doesn\u2019t have any influence at all on ", "?", "To answer this question, we can take a look at Model Summary.", "Here, ", " is now showing 0.0015, this is much higher than the model with only Mother Age, which was 0.00066. The value went up almost double. And this difference can be considered as a range of the variability of ", " that Mother Age alone can\u2019t explain but adding Father Age can.", "Now, one thing we need to be reminded about ", ".", "R Squared almost always goes up when you add more variables regardless whether they can really explain the changes in the target variable or not. To address this problem, we have a variation of R Squared called ", ", which penalizes for adding more variables, therefore, it can go down when you add variables that don\u2019t contribute in explaining the variability of the target variable.", "So, if ", " was not helpful at all then ", " could have decreased or stayed the same. It was 0.00066 with the model with only ", " before, and now it is 0.00149 with ", ". So this Adjusted R Squared also has more than doubled! So we can interpret that adding Father Age has some contributions in explaining the variability of ", ".", "Another metric for measuring the contribution of the added variables is AIC (Akaike Information Criterion), which I\u2019m going to talk about when we start getting into the variable selection topic later in this series.", "One more thing with the model quality.", "The ", " for this model with ", " and ", " is much smaller than the model with just Mother Age. This means that this model has become more reliable in terms of explaining the changes in ", ".", "Based on the investigation we have made so far, we have learned that ", " is more reliable than ", " in order to explain the changes in ", ". This leads us to believe that ", " would be the one directly influencing the changes in ", " compared to ", ".", "But, ", " also seems to contribute some degrees in explaining the changes in ", ".", "Now, I\u2019m still not 100% sure there is nothing else behind Mother Age and Father Age. When I built a model with only Father Age in the previous post I once thought Father Age had some influences on Gestation Week, but it turned out that majority of the influence was carried over from Mother Age.", "This makes me want to investigate further on other variables like Mother Race, Father Race, etc. Conveniently, we can bring those variables together and re-build the Linear Regression model again to see the coefficients, P values, etc. for these variables", "But, here is one thing.", "So far, we have focused on investigating Father Age and Mother Age, which happen to be both numerical data type. And, understanding the coefficient estimate of numeric data type variable is intuitive. For example, we can say something like \u201cOne year increase in ", " tends to make ", " 0.5 weeks shorter.\u201d", "But, how about categorical variables like Mother Race whose values can be White, Black, Japanese, etc.? What does it mean when we say \u2018One value change or increase\u2019 for the categorical variables?", "And, that is the topic for the Part 3.", "If you want to try this out quickly, you can download the data from ", ", import it in Exploratory Desktop, and follow the steps.", "If you don\u2019t have Exploratory Desktop yet, you can sign up from ", " for 30 days free trial!", "If you are interested in learning various powerful Data Science methods ranging from Machine Learning, Statistics, Data Visualization, and Data Wrangling without programming, go visit our ", " and enroll today!", "Written by"], "postingTime": "2018-03-07T14:13:08.410Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "Exploratory\u2019s Weekly Update Vol.10 \u2014 What AI Can/Can\u2019t Do, Emerging Role in Data Science, & more.", "content": ["Hi there!", "It\u2019s Kan from ", ".", "Before starting this week\u2019s Exploratory\u2019s Weekly Update, there are two things.", "First, my personal update. We\u2019ve just had our baby a week ago. It\u2019s been super hectic, but awesome, so much love. There is something about the newborn. ;)", "Second, we have rescheduled our online ", " to this coming April due to the fact that we needed a more time to prepare the contents and the next release of Exploratory v4.3. Thanks to those who have kindly accommodated the new schedule! The entry is still open, and we have ", ". If you are interested in learning Data Science without programming, sign up today!", "Now, let\u2019s start this week\u2019s updates!", " \u2014 ", "When you start AI / Machine Learning projects you will most likely hit the following 5 challenges.", "- Need large data set for training models", "- Need to label the training data ", "- Don\u2019t know what\u2019s happening inside AI / ML models \u2014 Blackbox", "- Hard to generalize AI models", "- Bias in AI models", "Labeling the data is a part of data preparation, which is the most critical task for building better models given that the AI models are heavily depending on the quality of the data. This is why Data Scientists spend their 80% of the time on Data Wrangling.", " \u2014 ", "Companies hire more data scientists and expect them to find amazing insights magically, but this is one of the reasons why many Data Science projects fail at big companies. Data Scientists don\u2019t understand Business and Business leaders don\u2019t understand Data Science. To address this communication problem, a new role called \u2018Translator\u2019 in Data Analysis is emerging.", "Personally, I don\u2019t like creating another role and have business leaders throw everything on this new role. For the next generation of leaders and managers need to be able to employ some of Data Science methods just like they use Excel today. But, if it helps to make Data Science projects more success, then why not? It might be one of the roles we need in a transitional period.", " \u2014 ", "While China has a plan to spend $150 billion in AI till 2030 and becomes the world\u2019s leader in AI, the current US government is cutting 15% in Science and Technology research funding in 2018.", "Sean is the inventor of Prophet \u2014 Time Series Forecasting algorithm, which you can use in Exploratory. I\u2019d strongly recommend you read the whole thread if you are serious about data analysis. ", " \u2014 ", "LinkedIn publishes a list of top 10 skills desired by companies every year. Someone collected it from 2013 and 2017 and merged them together. Statistical Analysis & Data Mining is a skill that ranks as number 2, which is no surprise given the popularity of Data Science these days.", "(Introduced by \u2018", "\u2019.)", " \u2014 ", "They used Amazon\u2019s Mechanical Turk (crowdsourcing), asked people to write moments when they felt happy in the last 24 hrs, 1 week, and 1 month, and published the data as a series of CSV files. You can import the files into Exploratory, if you have, then do some text analysis to find some interesting insights. Here\u2019s a ", " for a quick text analysis in Exploratory, though it\u2019s a bit old.", "(Introduced by \u2018", "\u2019.)", "We are adding Statistical Test capabilities into Analytics View to make it easier to access them and use them for the next release, v4.3, One of them is Chi-Squared Test.", "Also, by visualizing the pairs of Category A and B that contribute increasing Chi-Squared you can spot some unusual trends quickly.", "As mentioned at the beginning, we have re-scheduled our ", " to April 9th \u2014 13th. There is also ", "! If you are interested in learning Data Science without programming, sign up today!", "That\u2019s it for this week.", "Have a wonderful week!", "CEO/", "Written by"], "postingTime": "2018-02-20T21:13:56.130Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "Exploratory\u2019s Weekly Update Vol. 11 - learn data science", "content": ["Hi there!", "It\u2019s Kan from Exploratory.", "I always thought this American vacation called \u2018Ski Week\u2019 was such a cool idea. Basically, the schools are off during the whole week so that families with kids can go skiing. What I didn\u2019t realize until last week was that it also meant that I had to have a baby crying at left-hand side and a pre-school boy begging for playing at right-hand side while I was trying to work on Exploratory. ;)", "Anyway, before starting this week\u2019s update, our ", " enrollment is still open. We have a student discount (50% off). If you are interested in learning Data Science without programming, sign up today!", "Now, here\u2019s this week\u2019s update!", "Explainability / Interpretability of Machine Learning models is one of the most challenging roadblocks for making Machine Learning be useful for many. We, as consumers of the applications, don\u2019t know if something we read, see, listen, etc. are created by AI and we don\u2019t really know how they are even created. And it turned out that the ones who create them also don\u2019t fully understand the limitations of the algorithm they use and what bias they have in their training data.", "This is more of the reason why we need to emphasize more on data analysis side of Machine Learning (or Statistical Learning), which can help us understand the relationships and patterns in data, rather than just getting the result of the prediction. This is not a \u2018cool\u2019 message many of us want to hear when we want all the problems to be solved automatically by AI, but it\u2019s an important one. ;)", "Once we go beyond this AI automation hype though, people will understand AI is just an extension of our toolkit. It\u2019s more like Augmented Intelligence rather than Artificial Intelligence. And the discussion will become more like, how can we use AI to make ourselves more productive or improve our decision making, rather than worrying about if AI will take over our jobs to improve our productivity.", "Data savvy tech companies like Google, Amazon, Facebook, etc. are increasingly becoming the existential threat to many businesses. And this makes many companies wanting to build their own data-driven cultures. The problem is that these AI first tech companies didn\u2019t become Data savvy overnight, they started the journey from their very early stage. And this is why it\u2019s called \u2018culture\u2019.", "But this could be a problem. This recent AI hype might end up making Data Science projects being just another one of those IT driven projects, rather than being a business driven project that would changes the way business folks make their business decisions using data. Creating positions like Chief Data Officer, Chief Analytics Officer, etc. makes people feel good in a short term. But the cultural change takes times and that\u2019s what most of the companies need for the long term.", "But, we can also argue that, if only Data savvy companies can be competitive then all companies will have data-driven culture eventually thanks to \u2018survival of the fittest\u2019. Companies without such culture will die if all up to the market.", "If anyone wants to know what is the difference between Machine Learning and Statistics, this is it. It\u2019s not a new blog post from last week, but I have just found it last week and wanted to share.", "Peter Thiel, Paypal founder, Venture capitalist.", "Peter Thiel (Paypal founder, Venture capitalist)and Reid Hoffman (LinkedIn founder, Venture capitalist) discussed tech, politics, etc. One of the topics was AI. They have both very interesting views but differs as a libertarian and a liberal. Strongly recommend watching this recording. ", "Office of Foreign Assets Control in the Treasury Department publishes lists of the organizations and individuals who are the target of U.S. economic sanctions.", "(Introduced by \u2018", "\u2019.)", " \u2014 ", "Committee to Protect Journalists collects and publishes data about journalists who have been killed around the world since 1992. It contains the detail data like their nationality, where they were killed, which media outlets they worked for, etc.", "(Introduced by \u2018", "\u2019.)", "Linear Regression is nothing new and nothing fancy in this day and age, but it is still one of the most frequently used Statistical Learning (or Machine Learning) algorithm among Data Scientists / Statisticians for analyzing data due to its simplicity, which makes it easier to work with various problems and also makes it easier to interpret the insights it produces. However, this powerful and useful algorithm is still a bit of mystery for those who are new to the world of Data Science, so I\u2019m starting a series called \u2018A Practical Guide of Exploratory Data Analysis with Linear Regression\u2019, and here is the first post.", "Hope this will help many more folks use Linear Regression for their daily data analysis and get more values out of their data.", "As I mentioned last week, we are adding more Statistical Test capabilities into Analytics View. One of them is Normality Test, which evaluates if a given variable\u2019s data is normally distributed. Some of the statistical algorithms like T-Test, Pearson Correlation, etc. assume the underlying data is normally distributed. Now you can simply select a set of variables and quickly find which variables are normally distributed or not.", "For example, I have the following 8 variables whose data distribution can be shown as Histogram.", "Normality Test (Shapiro-Wilk test) will tell us which variables are most likely normally distributed and which are not.", "We can also use something called QQ Plot to visualize how the underlying data is close or far from the ideal form of the normal distribution.", "As mentioned at the beginning, our ", " enrollment is still open. We have a student discount (50% off). If you are interested in learning Data Science without programming, sign up today!", "That\u2019s it for this week.", "Have a wonderful week!", "CEO/", "Written by"], "postingTime": "2018-02-27T20:04:51.475Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "A Beginner\u2019s Guide to Exploratory Data Analysis with Linear Regression \u2014 Part 1", "content": ["We at ", " always focus on, as the name suggests, making ", " easier. EDA is a practice of iteratively asking a series of questions about the data at your hand and trying to build hypotheses based on the insights you gain from the data.", "At this EDA phase, one of the algorithms we often use is Linear Regression.", "Linear Regression is an algorithm to draw an optimized straight line between two or more variables. Being able to draw such a straight line helps us not only predict the unknown but also understand the relationship between the variables better.", "Though it has been there for a long time, it is still the most often used algorithm among many data scientists thanks to its simplicity and explainability.", "With the recent popularity of Machine Learning algorithms, there has been a lot of attention focused on the prediction side of things. But what I\u2019m finding the most useful for this type of Statistical algorithm is its ability to help us investigate the relationship between the variables.", "So, I\u2019m starting a series called \u201c", "\u201d to demonstrate how Linear Regression is so useful to produce useful insights and help us build good hypotheses effectively at Exploratory Data Analysis (EDA) phase.", "Here is a list of the episodes I\u2019m going to discuss.", "I\u2019m going to use ", " (UI for R) to demonstrate.", "Let\u2019s begin.", "Let\u2019s say we have a website that sells books, and we have past data about how much time our customers spent on the website and how much dollars they spent on purchasing books. If I draw a chart to visualize each customer\u2019s spending time at X-Axis and spending dollars at Y-Axis it would look something like below.", "Each dot represents each customer.", "Now, here is a new customer who has just spent 28 minutes on our website, and we want to know how much she is going to spend on purchasing books.", "We can draw a line that goes between the dots \u2018nicely\u2019, then find a point that hits 28 minutes on X-axis.", "Then, the value on Y-Axis for this point is the amount we would expect this customer to spend.", "Linear Regression algorithm would draw this blue line above in the most optimized way by making the distance between the line and all the dots to be the least.", "In order to draw such straight line, it will define a formula like below.", "The ", " is the slope of the line, which indicates how much the Y-Axis value goes up when X-Axis value increases one. And ", " is called \u2018intercept\u2019 which is the value when X-Axis value is 0.", "We can extend the model by adding more variables (or columns in the table or data frame).", "For example, you might want to predict how much your customers will be spending based on their demographical information like gender, age, employer, nationality, income, where they live, etc.", "Now, here is the thing.", "Linear Regression algorithm is not just for predicting the future. It is actually super useful for gaining useful insights about the relationships among the variables in data.", "And, thanks to its simplicity, it is easier to understand and explain such insights from the model to other people in a human language, not in a mathematical or scientific language.", "Let\u2019s explore with real data.", "Here, I have queried ", " from Google\u2019s hosted public data repository at BigQuery. It is about baby births in the US.", "If you are interested, you can download this data directly from ", " as CSV.", "At the very first of Exploratory Data Analysis, we want to start understanding the data quickly.", "In this data set, we have 12 columns and almost 2 million rows.", "Let\u2019s take a look at some of the columns from this Summary view quickly.", " column has each baby\u2019s weight at birth, which is ranging from 0.5 pounds to 18 pounds.", " column indicates whether a given baby is single or multiple births (e.g. twins, triplets, etc.),", " column indicates where they were born. California is the most frequent, the next is Texas, and so on.", " and ", " columns indicate what races the babies\u2019 parents are.", " and ", " columns indicate how old babies\u2019 parents were at the births. By the way, there is a weird data in Father\u2019s age at 99. People often use 99 or 999 as NA, so I\u2019m going to need to take care of this later.", " indicates how many weeks the babies were in mother\u2019s belly before they were born.", "Now, I just recently had a baby. And of course, I had been super anxious about when he was going to be born. But I know that predicting \u2018when\u2019 is not as simple as I would hope.", "\u201cit is difficult to make predictions, particularly about the future.\u201d", "by Mark Twain", "There are a lot of things that can cause (or stimulate) the delivery timing, and the basic data like we have here is most likely not enough for me to build a prediction model that I can confidently rely on.", "Instead, what I want to know is this.", "\u201cWhat might make the ", " shorter or longer?\u201d", "For example, if there is any difference between the father races (I\u2019m Japanese) or between mother races (my wife is White). Or, if a father\u2019s age has anything to do with whether the baby will be born early or late? (I\u2019m not young, whatever that means ;))", "So let\u2019s dig into the data and try to find if there are any trends that help me answer these questions.", "First, let\u2019s explore the data by using charts to see if there are any trends that would help understand the relationship between ", " variable (column) and the other variables (columns).", "How about the relationship between ", " and ", "?", "But before investigating on that, I need to take care of something about Father Age. As mentioned before, it has values of \u201899\u2019 in this column but it looks weird because there isn\u2019t any value close to it.", "It\u2019s a very common practice in some fields to use \u201899\u2019 or \u2018999\u2019 for NA and this one seems to be the case. So I\u2019m going to replace \u201899\u2019 with NA by using \u2018", "\u2019 function from \u2018", "\u2019 R package.", "In Exploratory, we can construct the syntax from the column header menu. Select \u2018", "\u2019 under \u2018", ".", "This will produce the syntax like below.", "After running the step, we can see \u201899\u2019 is gone, and those originally tagged as \u201899\u2019 are now showing up as NA (red color) like below.", "Having taken care of the \u201899\u2019, we can visualize the relationship between ", " and ", "by using Scatter chart assigning Father Age at X-Axis and Gestation Week at Y-Axis.", "We can\u2019t really see an obvious correlation between the two variables. I\u2019ve enabled a trend line (Linear Regression) inside the chart, and the line looks almost flat.", "How about if we sliced into each Father Race category?", "Here, I\u2019m showing the same scatter chart for each Father Race.", "Still not obvious, but some of the trend lines are showing upward or downward trends more than before. For example, the trend line for Japanese seems to be showing a highly positive correlation, which means that as the father\u2019s age gets older the gestation week becomes longer. However, there seems to be not much data there, so I\u2019m not sure how much we can count on this trend line at this point.", "Let\u2019s do the same for Mother Age.", "This time, I\u2019m assigning ", " to X-Axis.", "Again, we don\u2019t see an obvious correlation. The trend line is going from left to right horizontally and it looks almost flat.", "Now, I\u2019m assigning ", " to ", " to see if there is any trend in each race category.", "It looks that there is a slight downward trend for American Indian, Filipino, and Japanese, which means that as the mother\u2019s age gets older the gestation week becomes shorter.", "How about the relationship between ", " and ", ". Here, I\u2019m using a Bar chart to show the average ", "periods by each ", ". The red dotted line shows the overall average.", "It\u2019s hard to see the difference, so I\u2019m zooming in to the area around the overall average age, which is 38.69.", "Here, we can see some differences. Some races like Chinese and Japanese tend to have the gestation week longer than the average. On the other hand, Black tends to have the gestation week shorter.", "However, we need to be reminded that the differences here are very small, They are all within less than a week.", "Let\u2019s look at ", " against ", " the same way we did for ", " above.", "We can see a similar subtle trend here as well. Chinese and Japanese are longer than the others, and Black and Filipino are shorter than the others.", "How about the relationship between ", " and ", "? Are twin or triplet (or even more!) babies are born earlier than single babies?", "Here, I\u2019m using a Bar chart assigning ", " to X-Axis and ", " to Y-Axis, and also showing a reference line (Red dotted line) to show the overall average of Gestation Week.", "This one is actually easier to spot the trend. As the plurality increases the average of the gestation week becomes shorter.", "Instead of comparing the ", " of Gestation Week by using a ", " chart, we can see how the values of Gestation Week are ", "for each Plurality number by using ", " chart like below.", "Y-Axis shows ", " and X-Axis shows ", ". Each box represents the range between 25 percentile and 75 percentile of Gestation Week and the center line inside each box indicates the median value of Gestation Week.", "This Boxplot chart helps us to compare the distribution of Gestation Week among Plurality categories. We can see a trend that Gestation Week values tend to go down as the number of Plurality goes up, though there are some overlaps between them.", "This leads us to think that there seems to be a ", " between Gestation Week and Plurality.", "So far, I\u2019ve got a few hypotheses based on the observations we have made so far.", "Now, let\u2019s evaluate these hypotheses by building ", " models.", "Let\u2019s start evaluating the above hypotheses one by one by building Linear Regression models.", "Before moving further, I want to emphasize one thing.", "By performing the regression analysis with Linear Regression algorithm we can understand the relationships between the variables better. And we might find that some of the variables might be able to explain a large portion of the changes in ", ".", "But the relationships between the variables we are talking about here are Correlation, which means that the changes in one variable can be observed at the same pace as the changes occur in another variable.", "Therefore, as I continue my analysis I might say something like \u201cthe changes in Father Age have an effect on the changes in Gestation Week\u201d, but this does ", " necessarily mean that the changes in Father Age are ", " Gestation Week shorter or longer.", "But, even if we don\u2019t know if ", " is really causing the changes in ", ", just knowing that there is a correlation between them helps us estimate how much Gestation Week would change when we observe a certain amount of changes in Father Age.", "Ok, having said that, let\u2019s continue.", "In Exploratory, I can start the regression analysis with Linear Regression under Analytics View, and assign ", " to \u2018Target Variable\u2019 and ", " to \u2018Predictor Variable\u2019, and click Run button.", "By the way, this is an equivalent of running the linear regression function \u2018", "\u2019 in R like below.", "Anyway, once the model is built, we would get a chart like below under Coefficient tab in Exploratory.", "The dot at the middle of the red line shows ", ".", "The line color is red because the coefficient value is negative (less than 0). It would have been blue if the value was positive, and it would have been gray if its ", " was less than a threshold value (the default is 0.05).", "Now, we start talking about two very important metrics. One is ", " and another is ", ".", "Let\u2019s unpack these two metrics.", "Start with ", ".", "P-value in this context is a probability of getting a similar change in ", " even when there is no relationship between ", " and ", ". And if the probability is small enough (a very well known threshold is 5% but it can be higher or lower depending on the nature of the data.), then we would think that there has to be some degree of ", "effect on the changes in ", ".", "Here, the P-value is 0.000013, and it\u2019s quite a small number. This means that if we assume that ", " doesn\u2019t have any effect on the changes in ", ", the chance of getting a similar change in ", " is only 0.0013% (0.000013 / 100). Given that we are observing such a rare thing with this data, it doesn\u2019t make sense to believe that ", " doesn\u2019t have anything to do with the changes in ", ". Therefore, we can conclude that this ", " should have something to do with the changes in ", ".", "Now let\u2019s look at the other metric.", "The coefficient estimate here can be interpreted as, how much of the change in ", " can be explained (or influenced) by one value increase of ", ".", "It is -0,0075. This means that as the father age becomes one year older the gestation week would become 0.0075 weeks shorter.", "We have focused on the effect of Father Age variable to Gestation Week so far. But behind the scene, we have actually built a Linear Regression model. Let\u2019s take a look at the model itself further.", "We can check the model summary to understand the quality of this Linear Regression model that has produced the insight above.", "Under the Summary tab, we can see a list of the metrics.", "These are all useful metrics, but here I want to introduce the two most useful ones.", "R Squared is to measure how much of the variability of ", " this model can explain compared to a \u2018dumb\u2019 model.", "What is the \u2018dumb\u2019 model?", "In this case, it is the model that would simply calculate the average of Gestation Week and always return that number. That\u2019s why I call it \u2018dumb\u2019 model. \ud83d\ude09", "The values of R Squared vary between 0 and 1.", "1 is the highest, which means that the model can explain 100% of the variability of a target variable, in this case, that is ", ".", "The R Squared is 0.0004 here, meaning that it\u2019s better than the \u2018dumb\u2019 model, but at the same time, it doesn\u2019t really explain the variability of Gestation Week.", "So, what is the ", " anyway?", "Let me use Scatter chart to explain this better.", "Here, I\u2019m showing ", " at X-Axis and ", " at Y-Axis, and each dot represents each baby.", "Also, I\u2019m showing a Trend Line (Linear Regression Model) as Blue Line to show the trend of the relationship between ", " and ", " as a straight line.", "And lastly, there is a Reference Line as Red Line showing the average of Gestation Week.", "Now, as you recall, the R-Squared for this model is 0.0004.", "Well, this 0.0005 actually is a byproduct of the difference between the Blue and the Red lines.", "Compared to the Red colored average reference line, the Blue colored trend line (Linear Regression model covers a bit more of the variability of Gestation Week. But, it still stays within a range of 38 and 39 of Gestation Week and is not explaining the whole range of Gestation Week (mainly from 25 to 45) very well.", "Just to give you some idea, this is how it would look when R Squared is high.", "This case, R Squared is 0.96 meaning that this model that draws this Linear Regression line (Blue) can explain 96% of the variability of Y-Axis variable compared to the red average line. The blue line is very close to all the dots and we can intuitively see that it is much better to predict whatever the Y-Axis variable values compared to the red average line.", "So, all in all, the R Squared here is not that great. I mean, it looks pretty bad!", "But this doesn\u2019t mean that the coefficient estimate of ", " is not reliable. Whether we can conclude if there is any meaningful linear relationship between ", " and ", " is entirely up to the P-value, which is another metric under the model summary.", "We have already looked at P-value for Father Age variable under Coefficient Estimate tab. But, we have P-value for the model itself, and this can be found under the Summary tab.", "Since we have only one predictor variable, this model\u2019s P-value happens to be the same value of P-value for Father Race variable. But when we start adding more predictor variables the model, this model level P-value will become different from the P-value of each variable.", "Anyway, P-value here is ", ", very small number. This leads us to conclude that this model has something to do with the changes in Gestation Week.", "Another useful metric is ", ". This is to show the average difference between the actual values and the values this model would predict \u2014 Predicted Values.", "Here, it is showing as about 2.4.", "This means that the values predicted by this model make an error (difference) of 2.4 weeks on average. Note that the unit is the same for the target variable, in this case, that is a week (Gestation Week).", "Now, let\u2019s sum up what we have learned so far.", "We can conclude that ", " has an effect on ", " and we know that ", " because the P-Value is quite small. And one year increase of Father Age would ", ".", "But, this prediction model, which is built only with ", " can explain only ", ". And, if we use it to predict the gestation week you would need to ", "s (difference from the actual values) on average.", "Given that one year increase in Father Age would make Gestation Week only ", " weeks shorter, ", " weeks error looks pretty big!", "We have looked at the influence of ", " on ", ", but most of you must be thinking that shouldn\u2019t ", " be the one having an effect on", " more than ", ".", "I mean, mothers are the ones carrying the babies throughout the whole pregnancy, right?", "Then, we have two set of questions here.", "First, what is the relationship between Mather Age and Gestation Week? Is there a significant relationship? How about comparing to Father Age?", "Second, is Mother Age the one causing Gestation Week longer or shorter? Or Father Age?", "In the next episode, I\u2019m going to investigate further to answer these questions by using the same Linear Regression model.", "If you want to try this out quickly, you can download the data from ", ", import it in Exploratory Desktop, and follow the steps.", "If you don\u2019t have Exploratory Desktop yet, you can sign up from ", " for 30 days free trial!", "If you are interested in learning various powerful Data Science methods ranging from Machine Learning, Statistics, Data Visualization, and Data Wrangling without programming go visit our ", " and enroll today!", "Written by"], "postingTime": "2018-12-21T22:51:43.045Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Shuhei Yoshida", "articleTile": "Exploratory v4.2 \u30ea\u30ea\u30fc\u30b9\uff01 \u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3001\u65b0\u3057\u3044\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u3084\u30c1\u30e3\u30fc\u30c8\u30bf\u30a4\u30d7\u306a\u3069\u306e\u65b0\u6a5f\u80fd\u3082", "content": ["\u4e09\u304b\u6708\u9593\u306b\u308f\u305f\u3063\u3066\u4e0d\u7720\u4e0d\u4f11\u3067\u958b\u767a\u306b\u53d6\u7d44\u307f\u3001\u4eca\u56dev4.2\u3092\u3064\u3044\u306b\u30ea\u30ea\u30fc\u30b9\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3057\u305f\u3002Exploratory\u30e6\u30fc\u30b6\u30fc\u306e\u7686\u69d8\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306b\u57fa\u3065\u304d\u3001\u591a\u304f\u306e\u65b0\u6a5f\u80fd\u3092\u8ffd\u52a0\uff08\u30d0\u30b0\u4fee\u6b63\u3092\u542b\u3080\uff09\u3057\u307e\u3057\u305f\u3002\u3054\u610f\u898b\u3092\u304f\u3060\u3055\u3063\u305f\u7686\u69d8\u306b\u5fc3\u304b\u3089\u611f\u8b1d\u3044\u305f\u3057\u307e\u3059\uff01", "\u4eca\u56de\u306e\u30ea\u30ea\u30fc\u30b9\u3067\u7279\u306b\u6ce8\u529b\u3057\u305f\u306e\u306f", "\u306e\uff14\u3064\u3067\u3059\u3002\u305d\u308c\u3067\u306f\u3001\u3072\u3068\u3064\u305a\u3064\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01", "\u30ce\u30fc\u30c8\u3084\u30b9\u30e9\u30a4\u30c9\u3068\u3044\u3063\u305f\u5f93\u6765\u306e\u30ec\u30dd\u30fc\u30c6\u30a3\u30f3\u30b0\u6a5f\u80fd\u306b\u52a0\u3048\u3001\u7686\u69d8\u304b\u3089\u7279\u306b\u8981\u671b\u306e\u591a\u304b\u3063\u305f\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u30c7\u30fc\u30bf\u30fb\u30b5\u30a4\u30a8\u30f3\u30b9\u30fb\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u521d\u671f\u6bb5\u968e\u306b\u304a\u3044\u3066\u306f\u3001\u30d3\u30b8\u30cd\u30b9\u3084\u30c1\u30fc\u30e0\u306b\u3068\u3063\u3066\u91cd\u8981\u306b\u306a\u308b\u6307\u6a19\u3092\u660e\u78ba\u306b\u3057\u3066\u304a\u304f\u3068\u3088\u3044\u3067\u3057\u3087\u3046\u3002\u305d\u3057\u3066\u3001\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u624b\u6cd5\u3092\u4f7f\u3044\u306a\u304c\u3089\u3001\u4f55\u304c\u305d\u306e\u6307\u6a19\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u306e\u304b\u3001\u307e\u305f\u305d\u306e\u6307\u6a19\u304c\u4e0a\u304c\u3063\u305f\u308a\u4e0b\u304c\u3063\u305f\u308a\u3059\u308b\u539f\u56e0\u306f\u4f55\u3067\u3042\u308b\u306e\u304b\u306a\u3069\u3092\u5206\u6790\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u307e\u3055\u306b\u3053\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u4f7f\u3063\u3066\u3001\u30c1\u30fc\u30e0\u306e\u30d3\u30b8\u30cd\u30b9\u6307\u6a19\u3092\u5b9a\u671f\u7684\u306b\u78ba\u8a8d/\u5171\u6709\u3059\u308b\u3053\u3068\u3067\u3001\u30e1\u30f3\u30d0\u30fc\u5168\u54e1\u304c\u5171\u901a\u306e\u76ee\u6a19\u306b\u5411\u304b\u3063\u3066\u9032\u3093\u3067\u3044\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u30af\u30ea\u30c3\u30af\u64cd\u4f5c\u3072\u3068\u3064\u3067\u3001\u3053\u308c\u307e\u3067\u306b\u4f5c\u6210\u3057\u305f\u30c1\u30e3\u30fc\u30c8\u3092\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u8868\u793a\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u30c1\u30e3\u30fc\u30c8\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u8a2d\u5b9a\u306f\u30c9\u30e9\u30c3\u30b0\u30a2\u30f3\u30c9\u30c9\u30ed\u30c3\u30d7\u3067OK", "\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092", "\u306b\u30d1\u30d6\u30ea\u30c3\u30b7\u30e5\u3059\u308c\u3070\u3001\u5b9a\u671f\u7684\u306b\u30c7\u30fc\u30bf\u3092\u30ea\u30d5\u30ec\u30c3\u30b7\u30e5\u3057\u3066\u3001\u6700\u65b0\u306e\u72b6\u6cc1\u3092\u30c1\u30fc\u30e0\u3067\u5171\u6709\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u3082\u3061\u308d\u3093HTML\u306b\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002", "\u4eca\u5f8c\u3082\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306e\u9032\u5316\u306f\u7d9a\u304d\u307e\u3059\u306e\u3067\u3054\u671f\u5f85\u304f\u3060\u3055\u3044\u3002\u7686\u69d8\u304b\u3089\u306e\u7a4d\u6975\u7684\u306a\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u304a\u5f85\u3061\u3057\u3066\u3044\u307e\u3059\uff01", "\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u6e1b\u3089\u3059\u65b9\u6cd5\u3068\u3057\u3066\u3088\u304f\u4f7f\u308f\u308c\u307e\u3059\u3002 \u7279\u306b\u3001\u591a\u6570\u306e\u5909\u6570\uff08\u5217\uff09\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3092\u3082\u3068\u306b\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u3063\u3066\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u5834\u5408\u306b\u4fbf\u5229\u3067\u3059\u3002 PCA\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u3082\u5927\u304d\u304f\u3001\u76f8\u4e92\u306b\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u304c\u5c11\u306a\u304f\u306a\u308b\u3088\u3046\u306b\u4f5c\u6210\u3059\u308b\u305f\u3081\u3001\u65b0\u3057\u304f\u4f5c\u6210\u3055\u308c\u305f\u5909\u6570\u306f\u5143\u306e\u30c7\u30fc\u30bf\u304c\u3082\u3064\u610f\u5473\u3092\u5931\u3046\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002", "PCA\u306f\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u628a\u63e1\u3057\u3066\u3001\u3082\u3068\u3068\u306a\u308b\u30c7\u30fc\u30bf\u3092\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u6642\u306a\u3069\u306b\u3082\u4f7f\u308f\u308c\u308b\u3053\u3068\u304b\u3089\u3001\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u5fc5\u8981\u304c\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u63a2\u7d22\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u3092\u3059\u308b\u3068\u304d\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002", "v4.2\u3067\u306f\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u30fb\u30d3\u30e5\u30fc\u304b\u3089\u7c21\u5358\u306b\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "2016\u5e74\u306e\u30ab\u30ea\u30d5\u30a9\u30eb\u30cb\u30a2\u5dde\u9078\u6319\u306e\u7d50\u679c\u30c7\u30fc\u30bf\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u30ab\u30ea\u30d5\u30a9\u30eb\u30cb\u30a2\u5dde\u306e\u5404\u90e1\u304c\u300c\u30de\u30ea\u30d5\u30a1\u30ca\u306e\u5408\u6cd5\u5316\u300d\u3001\u300c\u4f7f\u3044\u6368\u3066\u306e\u30d3\u30cb\u30fc\u30eb\u888b\u306e\u4f7f\u7528\u7981\u6b62\u300d\u306a\u3069\u306e\u6cd5\u6848\u306b\u3069\u306e\u3088\u3046\u306b\u6295\u7968\u3057\u305f\u304b\u306e\u30c7\u30fc\u30bf\u304c\u3053\u3061\u3089\u306b\u3042\u308a\u307e\u3059\u3002", "\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u30d3\u30e5\u30fc\u304b\u3089\u4e3b\u6210\u5206\u5206\u6790\u3092\u9078\u3073\u3001\u5168\u3066\u306e\u5217\u3092\u5909\u6570\u3068\u3057\u3066\u9078\u629e\u3057\u3066\u5b9f\u884c\u3057\u307e\u3059\u3002", "\u306f\u3058\u3081\u306e\u2018Component Importance\u2019\u30bf\u30d6\u3067\u306f\u3001\u65b0\u3057\u304f\u4f5c\u3089\u308c\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\uff08\u5909\u6570\u3082\u3057\u304f\u306f\u6b21\u5143\uff09\u304c\u3001\u3069\u306e\u7a0b\u5ea6\u306e\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u8aac\u660e\u3067\u304d\u308b\u304b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002", "\u3053\u3053\u3067\u306f\u3001\u3072\u3068\u3064\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c72.88%\u306e\u30c7\u30fc\u30bf\u5206\u6563\u3092\u8aac\u660e\u3067\u304d\u3066\u3044\u308b\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059", "\u6b21\u306b \u2018Biplot\u2019\u30bf\u30d6\u3067\u5909\u6570\u3069\u3046\u3057\u306e\u95a2\u4fc2\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u3082\u3057\u5909\u6570\u306e\u76f4\u7dda\u304c\u540c\u3058\u65b9\u5411\u3001\u540c\u3058\u9577\u3055\u3067\u3042\u308c\u3070\u3001\u305d\u308c\u3089\u306e\u5909\u6570\u306f\u8fd1\u3044\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002", "\u305d\u308c\u305e\u308c\u306e\u70b9\u306f\u90e1\u3092\u8868\u3057\u3066\u3044\u3066\u3001\u305d\u306e\u4f4d\u7f6e\u306b\u3088\u3063\u3066\u5404\u6cd5\u6848\u306b\u3069\u306e\u3088\u3046\u306b\u6295\u7968\u3057\u305f\u304b\u304c\u308f\u304b\u308a\u307e\u3059\u3002Party_Name\u306e\u5217\u3092\u8272\u306b\u5272\u308a\u5f53\u3066\u3066\u653f\u515a\u540d\u306b\u3088\u3063\u3066\u8272\u5206\u3051\u3092\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\u3068\u3001\u6c11\u4e3b\u515a\uff08\u30d2\u30e9\u30ea\u30fc\u30fb\u30af\u30ea\u30f3\u30c8\u30f3\uff09\u306b\u6295\u7968\u3057\u305f\u70b9\u304c\u9752\u304f\u3001\u5171\u548c\u515a\uff08\u30c9\u30ca\u30eb\u30c9\u30fb\u30c8\u30e9\u30f3\u30d7\uff09\u306b\u6295\u7968\u3057\u305f\u70b9\u304c\u8d64\u304f\u306a\u308a\u307e\u3059\u3002", "\u591a\u304f\u306e\u6cd5\u6848\u304c\u5de6\u5074\u306b\u5411\u304b\u3063\u3066\u4f38\u3073\u3066\u304a\u308a\u3001\u9752\u8272\u306e\u90e1\u306f\u3053\u308c\u3089\u306e\u6cd5\u6848\u306b\u8fd1\u3044\u3068\u3053\u308d\u306b\u3042\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3053\u308c\u3089\u306e\u6cd5\u6848\u306b\u306f\u6c11\u4e3b\u515a\u306e\u90e1\u304c\u6295\u7968\u3059\u308b\u50be\u5411\u306b\u3042\u308b\u3068\u3044\u3048\u307e\u3059\u3002", "\u4e00\u65b9\u3001\u2018Adult Film Condom Requirements\u2019\u3084 \u2018legislative_procedure_requirements\u2019 \u306e\u7dda\u306f\u4e0b\u306e\u65b9\u306b\u5411\u304b\u3063\u3066\u4f38\u3073\u3066\u304a\u308a\u3001\u6c11\u4e3b\u515a\u304b\u5171\u548c\u515a\u304b\u306b\u3088\u308b\u5f71\u97ff\u304c\u3042\u307e\u308a\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002", "\u2018County Name\u2019\u5217\u3092Label\u306b\u5272\u308a\u5f53\u3066\u3001\u30c1\u30e3\u30fc\u30c8\u4e0a\u306b\u8868\u793a\u306b\u30c1\u30a7\u30c3\u30af\u3092\u5165\u308c\u308b\u3053\u3068\u3067\u3001\u6295\u7968\u7d50\u679c\u306b\u3082\u3068\u3065\u3044\u305f\u90e1\u3069\u3046\u3057\u306e\u95a2\u4fc2\u3092\u307f\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u56de\u5e30\u5206\u6790\u306e\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u30bf\u30a4\u30d7\u306e\u4e0b\u306b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306fKaggle\u304c\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u3068\u3063\u305f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306e\u9593\u30672017\u5e74\u306b\u6700\u3082\u4eba\u6c17\u306e\u3042\u308b\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3059\u3002 \u3053\u306e\u53e4\u304d\u826f\u304d\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u666e\u53ca\u3057\u3066\u3044\u308b\u7406\u7531\u306f\u3001\u305d\u308c\u304c\u30b7\u30f3\u30d7\u30eb\u3067\u6bd4\u8f03\u7684\u7406\u89e3\u3057\u3084\u3059\u3044\u30a4\u30f3\u30b5\u30a4\u30c8\u3092\u63d0\u4f9b\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002 \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\u3057\u3066\u30d0\u30a4\u30ca\u30ea\u7d50\u679c\uff08TRUE\u307e\u305f\u306fFALSE\uff09\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3001\u4e88\u6e2c\u5909\u6570\u306e\u4fc2\u6570\u307e\u305f\u306f\u30aa\u30c3\u30ba\u6bd4\u3092\u8abf\u3079\u3066\u7d50\u679c\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3068\u3044\u3063\u305f\u5834\u5408\u306b\u4f7f\u308f\u308c\u307e\u3059\u3002", "\u305f\u3068\u3048\u3070\u3001\u3053\u3053\u306bCDC\uff08\u7c73\u56fd\u75be\u75c5\u7ba1\u7406\u30bb\u30f3\u30bf\u30fc\uff09\u306b\u3088\u3063\u3066\u53ce\u96c6\u3055\u308c\u305f\u7c73\u56fd\u306e\u4e73\u5150\u306e\u51fa\u751f\u30c7\u30fc\u30bf\u304c\u3042\u308a\u3001\u5b50\u3069\u3082\u304c\u5b50\u5bae\u306b\u4f55\u30f6\u6708\u9593\u3044\u308b\u304b\u3001\u751f\u307e\u308c\u305f\u3068\u304d\u306e\u8d64\u3061\u3083\u3093\u306e\u4f53\u91cd\u3001\u6bcd\u89aa\u3068\u7236\u89aa\u306e\u60c5\u5831\u306a\u3069\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u3002", "\u3053\u3053\u3067\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u3053\u3068\u304c\u539f\u56e0\u3067\u65e9\u7523\u306e\u8d64\u3061\u3083\u3093\u304c\u751f\u307e\u308c\u3066\u304f\u308b\u306e\u304b\u3092\u63a2\u3063\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u3087\u3046\u3002 \u305d\u308c\u305e\u308c\u306e\u8d64\u3061\u3083\u3093\u306e\u30c7\u30fc\u30bf\u306b\u306f\u3001\u65e9\u7523\u304b\uff08TRUE\uff09\u3001\u305d\u3046\u3067\u306a\u3044\u304b\uff08FALSE\uff09\u3092\u793a\u3059\u5217\u300cis_premature\u300d\u304c\u3042\u308a\u307e\u3059\u3002", "\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u304c\u6709\u7528\u3067\u3059\u3002 \u300cWhat to Predict\u300d\u306b", "\u5217\u3092\u9078\u629e\u3057\u3001\u300cVariable Columns\u300d\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u8abf\u3079\u305f\u3044\u3059\u3079\u3066\u306e\u5217\u3092\u9078\u629e\u3057\u307e\u3059\u3002 \u305d\u3057\u3066\u5b9f\u884c\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b0\u30e9\u30d5\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002", "\u5404\u30d0\u30fc\u306f\u30a8\u30e9\u30fc\u30d0\u30fc\u3067\u3042\u308a\u3001\u4e2d\u592e\u306e\u70b9\u306f\u30aa\u30c3\u30ba\u6bd4\u3067\u3059\u3002\u3053\u308c\u306f\u3001\u4e0e\u3048\u3089\u308c\u305f\u5909\u6570\u304c\u7d50\u679c\u306e\u78ba\u7387\u306b\u3069\u308c\u3060\u3051\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u3092\u793a\u3059\u305f\u3081\u306e\u3082\u306e\u3067\u3059\u3002 \u9752\u8272\u306f\u30aa\u30c3\u30ba\u6bd4\u306e\u6b63\u306e\u65b9\u5411\u3092\u793a\u3057\u3001\u8d64\u8272\u306f\u8ca0\u306e\u65b9\u5411\u3092\u793a\u3057\u307e\u3059\u3002 \u30b0\u30ec\u30fc\u306f\u3001\u6240\u4e0e\u306e\u5909\u6570\u304c\u7d50\u679c\u306b\u5bfe\u3057\u3066\u7d71\u8a08\u7684\u306b\u6709\u610f\u3067\u3042\u308b\u3068\u306f\u307f\u306a\u3055\u308c\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "\u3055\u3066\u3001\u3053\u306e\u30b0\u30e9\u30d5\u304b\u3089\u3001\u6bcd\u89aa\u306e\u3044\u304f\u3064\u304b\u306e\u4eba\u7a2e\u304c\u4ed6\u306e\u4eba\u7a2e\u3088\u308a\u3082\u65e9\u7523\u306e\u8d64\u3061\u3083\u3093\u3092\u6301\u3064\u50be\u5411\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u591a\u80ce\u5150\uff08\u53cc\u5b50\u3001\u4e09\u3064\u5b50\u306a\u3069\uff09\u306e\u5909\u6570\u306f\u3001\u5024\u304c1\u3064\u5897\u52a0\u3059\u308b\uff08\u3072\u3068\u308a\u304b\u3089\u53cc\u5b50\u3001\u53cc\u5b50\u304b\u3089\u4e09\u3064\u5b50\u306a\u3069\uff09\u3053\u3068\u306b\u3088\u308a\u3001\u65e9\u7523\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c2.5\u500d\u9ad8\u304f\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "Coefficient\u306e\u30bf\u30d6\u306b\u3042\u308b\u30b0\u30e9\u30d5\u3084\u8868\u304b\u3089\u4f55\u304b\u5927\u304d\u306a\u30a4\u30f3\u30b5\u30a4\u30c8\u3092\u898b\u51fa\u3057\u305f\u3068\u3057\u3066\u3082\u3001\u30e2\u30c7\u30eb\u306e\u54c1\u8cea\u304c\u826f\u304f\u306a\u3051\u308c\u3070\u3001\u305d\u306e\u30a4\u30f3\u30b5\u30a4\u30c8\u306f\u5f79\u306b\u306f\u7acb\u3061\u307e\u305b\u3093\u3002 Model Summary\u30bf\u30d6\u304b\u3089\u3001\u30e2\u30c7\u30eb\u306e\u54c1\u8cea\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u3001AUC\u3001Recall\u3001Precision\u3001P-Value\u3001Log Likelihood\u306a\u3069\u3044\u304f\u3064\u304b\u306e\u6307\u6a19\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u3053\u306e\u30bf\u30d6\u3067\u5f79\u306b\u7acb\u3064\u60c5\u5831\u306e1\u3064\u306b\u3001\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u30d9\u30fc\u30b9\u30ec\u30d9\u30eb\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002 \u4e0a\u8a18\u306e\u3088\u3046\u306b\u3001\u6bcd\u89aa\u306e\u4e00\u90e8\u306e\u4eba\u7a2e\u3067\u65e9\u7523\u306e\u8d64\u3061\u3083\u3093\u304c\u751f\u307e\u308c\u308b\u53ef\u80fd\u6027\u306f\u9ad8\u304f\u306a\u308a\u307e\u3059\u304c\u3001\u300c\u9ad8\u3044\u300d\u3068\u306f\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u3082\u3064\u306e\u3067\u3057\u3087\u3046\uff1f\u307e\u305f \u4f55\u306b\u6bd4\u3079\u3066\u300c\u9ad8\u3044\u300d\u3068\u3044\u3048\u308b\u306e\u3067\u3057\u3087\u3046\uff1f \u305d\u308c\u306f\u3001\u3053\u306e \u2018mother_race_name\u2019\u5217\u306e\u30d9\u30fc\u30b9\u30ec\u30d9\u30eb\u3092\u78ba\u8a8d\u3059\u308b\u3068\u308f\u304b\u308a\u307e\u3059\u3002\u4e0b\u306e\u753b\u9762\u306e\u201dBase Level of mother_race_name\u201d \u306b \u201dWhite\u201d\u3068\u3042\u308b\u3053\u3068\u304b\u3089\u3001\u300c\u767d\u4eba\u3068\u6bd4\u3079\u3066\u9ad8\u3044\u300d\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002", "\u3053\u306e\u3053\u3068\u304b\u3089\u3001\u30d5\u30a3\u30ea\u30d4\u30f3\u306e\u6bcd\u89aa\u306f\u3001\u767d\u4eba\u306e\u6bcd\u89aa\u306b\u6bd4\u3079\u3066\u672a\u719f\u5150\u3068\u306a\u308b\u78ba\u7387\u304c5.8\u500d\u591a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002", "Prediction Matrix\u306e\u30bf\u30d6\u3067\u306f\u3001\u3053\u306e\u30e2\u30c7\u30eb\u304c\u3069\u306e\u3088\u3046\u306a\u4e88\u6e2c\u3092\u3088\u308a\u9ad8\u3044\u78ba\u7387\u3067\u5c0e\u304d\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u3001\u30e2\u30c7\u30eb\u306b\u3088\u308b\u4e88\u6e2c\u5024\u3068\u5b9f\u969b\u306e\u5024\uff08\u56de\u7b54\uff09\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002", "\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u30bf\u30d6\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\u3057\u3066\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u5909\u6570\u91cd\u8981\u5ea6\u60c5\u5831\u3092\u62bd\u51fa\u3057\u3066\u3044\u307e\u3059\u3002", "\u3069\u306e\u5909\u6570\u304c\u4e88\u6e2c\u3057\u305f\u3044\u7d50\u679c\u306b\u3088\u308a\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u3092\u77e5\u308b\u3053\u3068\u306f\u6709\u7528\u306a\u3053\u3068\u3067\u3059\u3002 \u3057\u304b\u3057\u3001\u3053\u3053\u3067\u660e\u3089\u304b\u306b\u3057\u3066\u304a\u304d\u305f\u3044\u306e\u306f\u3001\u305d\u308c\u3089\u304c\u7d50\u679c\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3057\u3066\u3044\u308b\u306e\u304b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002", "\u3057\u304b\u3057\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u3088\u3046\u306a\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u305f\u304f\u3055\u3093\u306e\u6c7a\u5b9a\u6728\u3092\u4f5c\u6210\u3057\u3001\u591a\u6570\u6c7a\u3067\u7d50\u679c\u3092\u4e88\u6e2c\u3059\u308b\u3068\u3044\u3046\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u5b66\u7fd2\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u3001\u305d\u308c\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u306e\u306f\u3068\u3066\u3082\u96e3\u3057\u3044\u306e\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u305d\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306e\u8aac\u660e\u80fd\u529b\u3092\u5411\u4e0a\u3055\u305b\u308b\u305f\u3081\u306b\u591a\u304f\u306e\u7d20\u6674\u3089\u3057\u3044\u624b\u6cd5\u304c\u958b\u767a\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u305d\u3046\u3057\u305f\u624b\u6cd5\u306e\u3072\u3068\u3064\u306b\u3001Zachary M. Jones\u3089\u306e\u201d", "\uff08EDARF\uff09\u201d\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u304c\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u53d6\u3063\u3066\u7d50\u679c\u3092\u4e88\u6e2c\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u7d50\u679c\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u3092\u5c0e\u304d\u51fa\u3057\u3066\u304f\u308c\u307e\u3059\u3002\u540d\u524d\u304c\u793a\u3059\u3088\u3046\u306b\u3001\u305d\u308c\u306f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u8aac\u660e\u80fd\u529b\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u3082\u306e\u3067\u3001\u3053\u306e\u5206\u91ce\u306b\u304a\u3051\u308b\u4ed6\u306e\u53d6\u7d44\u307f\u3068\u6bd4\u8f03\u3057\u3066\u3001\u6700\u3082\u5b9f\u7528\u7684\u304b\u3064\u7c21\u5358\u306b\u7d50\u679c\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u305d\u3053\u3067v4.2\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u5206\u6790\u306eEffects\u30bf\u30d6\u306b\u3053\u306e\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u4e0a\u8a18\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4f8b\u3067\u4f7f\u3063\u305f\u540c\u3058\u30c7\u30fc\u30bf\u3067\u3001\u3069\u306e\u3088\u3046\u306a\u3053\u3068\u304c\u65e9\u7523\u3067\u7523\u307e\u308c\u308b\u304b\u3069\u3046\u304b\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3057\u3088\u3046\u3068\u3057\u305f\u3068\u3057\u307e\u3059\u3002\u65b0\u3057\u3044Effects\u30bf\u30d6\u3067\u306f\u3001\u8d64\u3061\u3083\u3093\u304c\u3072\u3068\u308a\u304b\u3089\u53cc\u5b50\u3001\u4e09\u3064\u5b50\u3068\u5897\u3048\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u65e9\u7523\u306e\u53ef\u80fd\u6027\u304c\u9ad8\u307e\u308b\u3053\u3068\u3084\u3001\u30bf\u30d0\u30b3\u306e\u4f7f\u7528\u3001\u305d\u3057\u3066\u7236\u306e\u5e74\u9f62\u304c40\u6b73\u304b\u308960\u6b73\u306b\u5897\u52a0\u3059\u308b\u306a\u3069\u306e\u8981\u7d20\u3082\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059 \u3002", "\u3053\u3053\u307e\u3067\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u304a\u3088\u3073\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u3075\u305f\u3064\u306e\u30d0\u30a4\u30ca\u30ea\u5206\u985e\uff08\u4e8c\u9805\u5206\u985e\uff09\u306e\u4f8b\u3092\u3054\u7d39\u4ecb\u3057\u3066\u304d\u307e\u3057\u305f\u3002\u3069\u3061\u3089\u3082\u7d50\u679c\u306e\u30d0\u30a4\u30ca\u30ea\u7279\u6027\u3092\u4e88\u6e2c\u3057\u3088\u3046\u3068\u3059\u308b\u3082\u306e\u3067\u3057\u305f\uff08TRUE\u307e\u305f\u306fFALSE\uff09\u3002 \u3057\u304b\u3057\u3001\u30d0\u30a4\u30ca\u30ea\u5206\u985e\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u3068\u304d\u306e\u8ab2\u984c\u306e1\u3064\u306f\u3001\u300c\u4e0d\u5747\u8861\u30c7\u30fc\u30bf\u300d\u3078\u306e\u5bfe\u7b56\u3067\u3059\u3002 \u4e0a\u8a18\u306e\u4e73\u5150\u30c7\u30fc\u30bf\u306e\u4f8b\u3067\u306f\u3001\u672a\u719f\u5150\u304c\u7523\u307e\u308c\u308b\u30b1\u30fc\u30b9\u306f\u306f\u308b\u304b\u306b\u5c11\u306a\u304f\u3001\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3068\u3001\u305f\u3044\u3066\u3044\u306e\u5834\u5408FALSE\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3068\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u5358\u7d14\u306b\u4e88\u6e2c\u304c\u6b63\u3057\u304f\u306a\u308b\u78ba\u7387\u304c\u9ad8\u304f\u306a\u308b\u305f\u3081\u3067\u3059\u3002", "\u3053\u3053\u3067\u306f\u3001\u30a2\u30ca\u30ea\u30c6\u30a3\u30af\u30b9\u30d3\u30e5\u30fc\u306e\u4e0b\u3067\u5909\u6570\u91cd\u8981\u5ea6\u306e\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u4f7f\u7528\u3057\u3066\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002", "\u3054\u89a7\u306e\u3088\u3046\u306b\u3001\u3053\u306e\u30e2\u30c7\u30eb\u306f\u5927\u90e8\u5206\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066FALSE\u3092\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u6b63\u7b54\u7387\u306e\u89b3\u70b9\u3067\u306f\u826f\u3044\u7d50\u679c\u3092\u51fa\u3057\u3066\u3044\u308b\u3068\u307f\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\u3001AUC\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306e\u5168\u4f53\u7684\u306a\u54c1\u8cea\u3092\u6e2c\u5b9a\u3059\u308b\u6307\u6a19\u3092\u307f\u308b\u3068\u3044\u3044\u3068\u306f\u3044\u3048\u307e\u305b\u3093\u3002", "\u3053\u306e\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306e\u5b9f\u7528\u7684\u306a\u5bfe\u7b56\u306e1\u3064\u304cSMOTE\uff08Synthetic Minority Over-Sampling Technique\uff09\u3068\u547c\u3070\u308c\u308b\u3082\u306e\u3067\u3001\u5c11\u6570\u6d3e\u306e\u30c7\u30fc\u30bf\u3092\u5897\u3084\u3059\u3053\u3068\u306b\u3088\u3063\u3066\u30d0\u30e9\u30f3\u30b9\u3092\u3068\u308d\u3046\u3068\u3057\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u65e9\u7523\u3068\u306a\u308b\u8d64\u3061\u3083\u3093\u306e\u985e\u4f3c\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u3064\u304f\u308a\u3060\u3057\u3001\u591a\u6570\u6d3e\u3067\u3042\u308b\u975e\u65e9\u7523\u306e\u8d64\u3061\u3083\u3093\u306e\u30c7\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002", "\u3053\u306e\u65b0\u6a5f\u80fd\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u304a\u3088\u3073\u30ed\u30b8\u30b9\u30c6\u30a3\u30af\u30b9\u56de\u5e30\u6a5f\u80fd\u306b\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\u307e\u305f\u3001\u72ec\u7acb\u3057\u305f\u30c7\u30fc\u30bf\u30e9\u30f3\u30b0\u30ea\u30f3\u30b0\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u30d7\u30e9\u30b9\u30dc\u30bf\u30f3\u304b\u3089\u5b9f\u884c\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002", "\u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u3001\u300c\u30c7\u30fc\u30bf\u306e\u4e0d\u5747\u8861\u3092\u89e3\u6d88\u300d\u3092\u300cYes\u300d\u3068\u3059\u308b\u3053\u3068\u3067\u3001\u65e9\u7523\u306e\u8d64\u3061\u3083\u3093(TRUE)\u3092\u3088\u308a\u9ad8\u3044\u7cbe\u5ea6\u3067\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u3064\u304f\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001\u7d50\u679c\u3068\u3057\u3066AUC\u3084F Score\u3068\u3044\u3063\u305f\u6307\u6a19\u3092\u6539\u5584\u3055\u305b\u308b\u3053\u3068\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002", "\u3061\u306a\u307f\u306b\u3053\u306e\u6a5f\u80fd\u306f\u3001\u5185\u90e8\u7684\u306bAndrea Dal Pozzolo\u3089\u306b\u3088\u3063\u3066\u516c\u958b\u3055\u308c\u305f\u2018", "\u2019 \u3068\u3044\u3046R\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002", "\u30fb\u5909\u6570\u91cd\u8981\u5ea6\u3001\u56de\u5e30\u5206\u6790\u3001\u6642\u7cfb\u5217\u4e88\u6e2c\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "\u30fb\u7570\u5e38\u5024\u691c\u77e5\u5206\u6790\u306b\u3001\u201cAnomaly with Expected\u201d\u30d3\u30e5\u30fc\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f", "\u65b0\u3057\u3044\u30c1\u30e3\u30fc\u30c8\u3092\u52a0\u3048\u305f\u307b\u304b\u3001\u65e2\u5b58\u306e\u30c1\u30e3\u30fc\u30c8\u3084\u5730\u56f3\u3001\u30d4\u30dc\u30c3\u30c8\u30c6\u30fc\u30d6\u30eb\u306b\u6570\u591a\u304f\u306e\u65b0\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u306f\u3044\u3001\u3064\u3044\u306b\u3001\u3001\u3001\u3053\u308c\u4ee5\u4e0a\u306e\u8aac\u660e\u306f\u4e0d\u8981\u3067\u3059\u3088\u306d :)", "\u30d0\u30d6\u30eb\u30c1\u30e3\u30fc\u30c8\u306f\u6563\u5e03\u56f3\u3092\u30d9\u30fc\u30b9\u3068\u3057\u305f\u30c1\u30e3\u30fc\u30c8\u306b\u306a\u308a\u307e\u3059\u3002\u30b0\u30eb\u30fc\u30d7\u5316\u306e\u3068\u3053\u308d\u3067\u5217\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u8868\u793a\u3059\u308b\u30d0\u30d6\u30eb\u306e\u6570\u3092\u6c7a\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001X/Y\u8ef8\u3001\u30b5\u30a4\u30ba\u3001\u8272\u306a\u3069\u3067\u30c7\u30fc\u30bf\u96c6\u8a08\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002", "\u30b7\u30f3\u30b0\u30eb\u30d0\u30ea\u30e5\u30fc\u306f\u3072\u3068\u3064\u306e\u96c6\u8a08\u5024\u3092\u8868\u793a\u3057\u307e\u3059\u3002\u5358\u72ec\u306e\u6a5f\u80fd\u3068\u3057\u3066\u306f\u3042\u307e\u308a\u610f\u5473\u304c\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u3053\u306e\u6570\u5024\u3092\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u8868\u793a\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u6307\u6a19\u3068\u306a\u308b\u5024\u3092\u7d99\u7d9a\u7684\u306b\u8ffd\u8de1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u30c1\u30e3\u30fc\u30c8\u306e\u8868\u793a\u306b\u3044\u304f\u3064\u304b\u306e\u4fbf\u5229\u306a\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\u307e\u305a\u3001\u30c1\u30e3\u30fc\u30c8\u4e0a\u306e\u3072\u3068\u3064\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30c7\u30fc\u30bf\u306e\u6982\u8981\u304c\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u3067\u8868\u793a\u3055\u308c\u308b\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "\u305d\u3057\u3066\u305d\u306e\u4e0a\u90e8\u306b\u4ee5\u4e0b\u306e3\u3064\u306e\u30a2\u30af\u30b7\u30e7\u30f3\u30e1\u30cb\u30e5\u30fc\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002", "\u2018Keep Only\u2019 \u307e\u305f\u306f \u2018Exclude\u2019 \u3092\u9078\u629e\u3059\u308b\u3068\u3001\u30c1\u30e3\u30fc\u30c8\u5185\u3067\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u304b\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001DL\uff08\u30c7\u30eb\u30bf\u822a\u7a7a\uff09\u3068AA\uff08\u30a2\u30e1\u30ea\u30ab\u30f3\u822a\u7a7a\uff09\u3092\u9078\u629e\u3057\u3066\u2019Exclude\u2019\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002", "\u3059\u308b\u3068\u3001\u30c1\u30e3\u30fc\u30c8\u5185\u3067DL\u3068AA\u304c\u9664\u5916\u3055\u308c\u3001\u30c1\u30e3\u30fc\u30c8\u5185\u306e\u30c7\u30fc\u30bf\u306b\u30d5\u30a3\u30eb\u30bf\u30fc\u304c\u304b\u3051\u3089\u308c\u305f\u3053\u3068\u304c\u5206\u304b\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002", "\u2018Show Detail\u2019 \u30e1\u30cb\u30e5\u30fc\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u30c7\u30fc\u30bf\u306e\u5bfe\u8c61\u3068\u306a\u308b\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002", "\u307e\u305f\u3001Mac\u3067\u306fCommand\u30ad\u30fc\u3092\u3001Windows\u3067\u306fControl\u30ad\u30fc\u3092\u304a\u3057\u306a\u304c\u3089\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067\u3001\u8907\u6570\u306e\u9805\u76ee\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u3053\u3061\u3089\u3082\u4e0a\u8a18\u306e\u30d0\u30d6\u30eb\u30c1\u30e3\u30fc\u30c8\u306e\u62e1\u5f35\u306b\u985e\u4f3c\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u30b0\u30eb\u30fc\u30d7\u5316\u306e\u5217\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u30de\u30c3\u30d7\u4e0a\u306e\u5186\u306e\u6570\u3092\u6307\u5b9a\u3057\u305f\u5217\u306e\u5024\u306b\u5408\u308f\u305b\u3066\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u305d\u3057\u3066\u3001\u96c6\u8a08\u3055\u308c\u305f\u5024\u304c\u305d\u308c\u305e\u308c\u306e\u5186\u306e\u5927\u304d\u3055\u3084\u8272\u3067\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002", "\u3053\u3053\u3067\u306f\u3001\u2019Country\u2019\u306e\u5217\u3092\u30b0\u30eb\u30fc\u30d7\u5316\u306b\u6307\u5b9a\u3057\u3066\u3001\u300c\u56fd\u300d\u306e\u5217\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002", "\u30b0\u30eb\u30fc\u30d7\u5316\u306b\u6307\u5b9a\u3059\u308b\u5217\u3092\u2019Country\u2019\u304b\u3089\u2019Continent\u2019\u306b\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3001\u4e94\u5927\u9678\u3068\u3057\u30665\u3064\u306e\u5186\u3060\u3051\u304c\u8868\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002", "\u30c8\u30ec\u30f3\u30c9\u30e9\u30a4\u30f3\u306fv3.1\u304b\u3089\u6563\u5e03\u56f3\u306b\u306e\u307f\u8868\u793a\u3067\u304d\u307e\u3057\u305f\u304c\u3001v4.2\u304b\u3089\u306f\u30e9\u30a4\u30f3\u30c1\u30e3\u30fc\u30c8\u306b\u3082\u8868\u793a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "\u3055\u3089\u306b\u3001\u30c8\u30ec\u30f3\u30c9\u30e9\u30a4\u30f3\u4e0a\u306b\u30de\u30a6\u30b9\u3092\u7f6e\u304f\u3053\u3068\u3067\u3001P Value\u3084R Squared\u3068\u3044\u3063\u305f\u6307\u6a19\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u3061\u3089\u306f\u3001\u6563\u5e03\u56f3\u3001\u30e9\u30a4\u30f3\u30c1\u30e3\u30fc\u30c8\u3069\u3061\u3089\u3067\u3082\u53ef\u80fd\u3067\u3059\u3002", "\u30c8\u30ec\u30f3\u30c9\u30e9\u30a4\u30f3\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u7dda\u5f62\u56de\u5e30\u3001GAM\u3001\u304a\u3088\u3073Loess models\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304a\u308a\u3001\u30d7\u30ed\u30d1\u30c6\u30a3\u30c0\u30a4\u30a2\u30ed\u30b0\u304b\u3089\u305d\u308c\u3089\u3092\u5207\u308a\u66ff\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u68d2\u30b0\u30e9\u30d5\u3001\u7dda\u30b0\u30e9\u30d5\u3001\u304a\u3088\u3073\u6563\u5e03\u56f3\u306b\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30e9\u30a4\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30e9\u30a4\u30f3\u306e\u5024\u306f\u8272\u3005\u306a\u65b9\u6cd5\u3067\u8a08\u7b97\u3067\u304d\u3001\u307e\u305f\u3001\u7dda\u306e\u8272\u3084\u30b9\u30bf\u30a4\u30eb\u3082\u66f8\u5f0f\u8a2d\u5b9a\u3067\u5909\u66f4\u3067\u304d\u307e\u3059\u3002", "\u3053\u3053\u3067\u306f\u3001\u5e73\u5747\u51fa\u767a\u9045\u5ef6\u6642\u9593\u3092\u8d64\u8272\u306e\u5b9f\u7dda\u3067\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "X\u8ef8\u3068Y\u8ef8\u306e\u53cc\u65b9\u306b\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30e9\u30a4\u30f3\u3092\u8868\u793a\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u4eba\u7a2e\u3054\u3068\u306b\u7236\u3068\u6bcd\u306e\u5e73\u5747\u5e74\u9f62\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002 \u7236\u89aa\uff08Y\u8ef8\uff09\u3068\u6bcd\u89aa\uff08X\u8ef8\uff09\u306e\u5e73\u5747\u5e74\u9f62\u3092\u305d\u308c\u305e\u308c\u306e\u4eba\u7a2e\u3054\u3068\u306b\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30e9\u30a4\u30f3\u3067\u8868\u793a\u3059\u308b\u3053\u3068\u3067\u3001\u3069\u306e\u4eba\u7a2e\u306e\u4eba\u304c\u82e5\u304f\u3001\u3082\u3057\u304f\u306f\u5e74\u9f62\u3092\u91cd\u306d\u3066\u304b\u3089\u7d50\u5a5a\u3059\u308b\u304b\u3092\u7c21\u5358\u306b\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "X\u8ef8\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306b\u3088\u3063\u3066\u57fa\u6e96\u7dda\u306e\u5024\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002 \u3053\u308c\u306f\u3001\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u4e00\u9023\u306e\u96c6\u8a08\u5024\u3092\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30e9\u30a4\u30f3\u3068\u3057\u3066\u8868\u793a\u3059\u308b\u5834\u5408\u306b\u4fbf\u5229\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u3069\u306e\u90fd\u5e02\u304c\u6691\u304f\u3066\u3001\u3069\u306e\u90fd\u5e02\u304c\u5bd2\u3044\u304b\u4e00\u76ee\u3067\u308f\u304b\u308b\u3088\u3046\u306b\u30016\u3064\u306e\u7570\u306a\u308b\u90fd\u5e02\u306e\u9031\u5e73\u5747\u6c17\u6e29\u3092\u8d64\u8272\u3067\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "\u3082\u30461\u3064\u306e\u4fbf\u5229\u306a\u4f7f\u7528\u4f8b\u306f\u3001\u3059\u3079\u3066\u306e\u30e9\u30a4\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u5408\u8a08\u5024\u3092\u8868\u793a\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\uff13\u3064\u306e\u30bb\u30b0\u30e1\u30f3\u30c8\u306e\u58f2\u4e0a\u3068\u3001\u305d\u308c\u3089\u306e\u5408\u8a08\u3067\u3042\u308b\u7dcf\u58f2\u4e0a\u9ad8\u3068\u8d64\u70b9\u7dda\u3067\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "\u7570\u306a\u308b\u5c3a\u5ea6\u3084\u5358\u4f4d\u306e2\u3064\u306e\u30c7\u30fc\u30bf\u3092\u540c\u4e00\u306e\u30b0\u30e9\u30d5\u4e0a\u306b\u8868\u793a\u3057\u305f\u3044\u3053\u3068\u304c\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u5c3a\u5ea6\u3084\u5358\u4f4d\u304c\u5927\u304d\u304f\u7570\u306a\u308b\u5834\u5408\u3001\u7247\u65b9\u306e\u30c7\u30fc\u30bf\u306e\u8868\u793a\u304c\u5c0f\u3055\u304f\u306a\u308a\u3059\u304e\u3066\u898b\u306b\u304f\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002", "\u305d\u3053\u3067\u3001Y\u8ef8\u306e\u3072\u3068\u3064\u3092\u5de6\u5074\u306b\u3001\u3075\u305f\u3064\u3081\u3092\u53f3\u5074\u306b\u7570\u306a\u308b\u5c3a\u5ea6\u3067\u8868\u793a\u3059\u308b\u3053\u3068\u3067\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002", "\u3053\u306e\u3088\u3046\u306a\u8868\u793a\u65b9\u6cd5\u306f\u63a2\u7d22\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u3092\u884c\u3046\u3068\u304d\u306b\u306f\u3001\u5c11\u3057\u6df7\u4e71\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u306e\u3067\u9069\u3055\u306a\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u3001Dashboard\u3001Note\u3001Slides\u3067\u306e\u30ec\u30dd\u30fc\u30c8\u4f5c\u6210\u306b\u306f\u5f79\u306b\u7acb\u3064\u3067\u3057\u3087\u3046\u3002", "\u3044\u304f\u3064\u304b\u306e\u65b0\u3057\u3044\u8868\u8a08\u7b97\u6a5f\u80fd\u30bf\u30a4\u30d7\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u7d2f\u8a08\u306e\u8a08\u7b97\u30bf\u30a4\u30d7\u306b\u300c\u5408\u8a08\u306e\u5272\u5408\u300d\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u3053\u308c\u306f\u3001\u30c7\u30fc\u30bf\u306e\u7d2f\u7a4d\u5024\u3092\u78ba\u8a8d\u3059\u308b\u5834\u5408\u306b\u4fbf\u5229\u3067\u3059\u3002 \u4e0b\u306e\u4f8b\u306f\u3001\u7c73\u56fd\u3001\u65e5\u672c\u3001\u30a4\u30ae\u30ea\u30b9\u306e\u6700\u521d\u306e3\u30ab\u56fd\u3067\u30e6\u30fc\u30b6\u30fc\u306e\u7d0480\uff05\u3092\u7372\u5f97\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002", "\u8868\u8a08\u7b97\u306e\u5272\u5408\u306e\u8a08\u7b97\u30bf\u30a4\u30d7\u3092\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\u4ee5\u524d\u304b\u3089\u5408\u8a08\u306e\u5272\u5408\u3092\u8a08\u7b97\u3059\u308b\u6a5f\u80fd\u306f\u3042\u308a\u307e\u3057\u305f\u3057\u3001\u9805\u76ee\u3054\u3068\u306e\u5168\u4f53\u306b\u5360\u3081\u308b\u5272\u5408\u3092\u78ba\u8a8d\u3059\u308b\u6642\u306b\u4fbf\u5229\u3067\u3057\u305f\u3002", "\u5217\u3092\u8272\u3067\u5206\u5272\u3057\u3066\u3001\u304b\u3064\u8a08\u7b97\u65b9\u5411\u3092 \u2018X\u8ef8\u2019\u3068\u3059\u308b\u4ee3\u308f\u308a\u306b \u8272\u3092\u9078\u629e\u3057\u305f\u5834\u5408\u3001\u3053\u306e\u5408\u8a08\u306e\u5272\u5408\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u306a\u3058\u307f\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002", "\u3057\u304b\u3057\u3001\u5408\u8a08\u306e\u5272\u5408\u306e\u8a08\u7b97\u306f\u5217\u3092\u8272\u3067\u5206\u5272\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u3067\u3082\u4fbf\u5229\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4e0b\u306e\u30c1\u30e3\u30fc\u30c8\u3067\u306f\u305d\u308c\u305e\u308c\u306e\u56fd\u304b\u3089\u4f55\u30d1\u30fc\u30bb\u30f3\u30c8\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u304d\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u6b21\u306b\u5024\u306e\u96c6\u8a08\u65b9\u6cd5\u3092\u6700\u521d\u306e\u5024\u3068\u3059\u308b\u3068\u3001\u3072\u3068\u3064\u3081\u306e\u5217\u306e\u56fd\u306e\u5024\uff08\u3053\u306e\u4f8b\u3067\u306f\u30a2\u30e1\u30ea\u30ab\u306b\u306a\u308a\u307e\u3059\uff09\u306b\u5bfe\u3057\u3066\u3001\u4ed6\u306e\u56fd\u306e\u5024\u304c\u4f55\u30d1\u30fc\u30bb\u30f3\u30c8\u3092\u5360\u3081\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u30d0\u30fc\u3001\u30e9\u30a4\u30f3\u3001\u304a\u3088\u3073\u30a8\u30ea\u30a2\u30c1\u30e3\u30fc\u30c8\u306eY\u8ef8\u30e9\u30d9\u30eb\u3067\u3001\u30b9\u30ad\u30c3\u30d7\uff08\u307e\u305f\u306f\u5897\u5206\uff09\u5024\u3068\u30c7\u30fc\u30bf\u7bc4\u56f2\uff08\u6700\u5c0f\u5024\u3068\u6700\u5927\u5024\uff09\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002 \u6563\u5e03\u56f3\u3067\u306fX\u8ef8\u3068Y\u8ef8\u306e\u4e21\u65b9\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002", "\u30d4\u30dc\u30c3\u30c8\u30c6\u30fc\u30d6\u30eb\u306b\u95a2\u3057\u3066\u3075\u305f\u3064\u306e\u4fbf\u5229\u306a\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u5217\u30ec\u30d9\u30eb\u306e\u30bd\u30fc\u30c8\u306f\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306b\u30c7\u30fc\u30bf\u3092\u30bd\u30fc\u30c8\u3057\u307e\u3059\u3002 \u4e0b\u306e\u4f8b\u3067\u306f\u3001 \u2018darwin\u2019\u30ab\u30e9\u30e0\u306e\u5024\u306b\u57fa\u3065\u3044\u3066\u56fd\u3092\u30bd\u30fc\u30c8\u3059\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002", "\u5217\u30d8\u30c3\u30c0\u30fc\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u6700\u521d\u306e\u5217\uff08\u3053\u306e\u5834\u5408\u306f \u20182016\u201311\u201301\u2019\u3001\u20192016\u201312\u201301 \u2018\u306a\u3069\uff09\u306e\u5404\u30b0\u30eb\u30fc\u30d7\u5185\u3067\u306e\u307f\u56fd\u304c\u30bd\u30fc\u30c8\u3055\u308c\u307e\u3059\u3002", "\u518d\u5ea6\u5217\u30d8\u30c3\u30c0\u30fc\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067\u3001\u9006\u306e\u9806\u5e8f\u3067\u30bd\u30fc\u30c8\u3055\u308c\u307e\u3059\u3002", "\u8868\u8a08\u7b97\u6a5f\u80fd", "\u30c1\u30e3\u30fc\u30c8\u3067\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u3059\u3079\u3066\u306e\u8868\u8a08\u7b97\u6a5f\u80fd\u3092\u30d4\u30dc\u30c3\u30c8\u30c6\u30fc\u30d6\u30eb\u306b\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u884c\u3068\u5217\u304c\u3042\u308a\u3001\u3055\u3089\u306b\u884c\u306b\u306f\u8907\u6570\u306e\u30ec\u30d9\u30eb\u304c\u3042\u308b\u305f\u3081\u3001\u30d4\u30dc\u30c3\u30c8\u30c6\u30fc\u30d6\u30eb\u3092\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u306e\u306f\u5c11\u3057\u8907\u96d1\u3067\u3001\u8868\u8a08\u7b97\u3092\u884c\u3046\u65b9\u6cd5\u306f\u30d0\u30fc\u3001\u30e9\u30a4\u30f3\u30c1\u30e3\u30fc\u30c8\u306a\u3069\u3068\u5c11\u3057\u7570\u306a\u308a\u307e\u3059\u3002", "\u305d\u3053\u3067\u3001\u305d\u308c\u305e\u308c\u306e\u8868\u8a08\u7b97\u6a5f\u80fd\u306e5\u3064\u306e\u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u8a08\u7b97\u306e\u65b9\u5411\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002", "\u7d2f\u7a4d\u5408\u8a08\uff08\u5408\u8a08\uff09\u306e\u4f8b\u3092\u4f7f\u3063\u3066\u5404\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3059\u3050\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002", "\u884c\uff08\u5de6\u304b\u3089\u53f3\uff09\u3092\u9078\u629e\u3059\u308b\u3068\u3001\u30c7\u30fc\u30bf\u306f\u5404\u5217\u306e\u5024\u3092\u5de6\u304b\u3089\u53f3\u3078\u7d2f\u7a4d\u3057\u3066\u3044\u304d\u307e\u3059\u3002", "\u5217\uff08\u4e0a\u304b\u3089\u4e0b\uff09\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u5404\u5217\u306e\u5024\u304c\u4e0a\u304b\u3089\u4e0b\u306b\u79fb\u52d5\u3059\u308b\u306b\u3064\u308c\u3066\u5024\u304c\u7d2f\u7a4d\u3055\u308c\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002", "\u5217\uff08\u30b0\u30eb\u30fc\u30d7\u5185\u3067\u4e0a\u304b\u3089\u4e0b\uff09\u3067\u306f\u3001\u5024\u304c\u4e0a\u304b\u3089\u4e0b\u306b\u5411\u304b\u3063\u3066\u7d2f\u7a4d\u3055\u308c\u307e\u3059\u304c\u3001\u7d2f\u7a4d\u306f\u5404\u30b0\u30eb\u30fc\u30d7\u306e\u6700\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u505c\u6b62\u3057\u3001\u65b0\u3057\u3044\u30b0\u30eb\u30fc\u30d7\u3067\u518d\u958b\u3057\u307e\u3059\u3002", "\u5217\uff08\u30b0\u30eb\u30fc\u30d7\u5185\u3067\u4e0a\u304b\u3089\u4e0b\u3001\u5de6\u304b\u3089\u53f3\uff09\u306e\u65b9\u5411\u3067\u306f\u3001\u524d\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3068\u540c\u69d8\u306b\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u306e\u4e0a\u304b\u3089\u4e0b\u306b\u5024\u304c\u7d2f\u7a4d\u3055\u308c\u307e\u3059\u3002 \u305f\u3060\u3057\u3001\u30b0\u30eb\u30fc\u30d7\u306e\u7d42\u308f\u308a\u3067\u505c\u6b62\u305b\u305a\u3001\u53f3\u306b\u5411\u304b\u3063\u3066\u7d2f\u7a4d\u3057\u7d9a\u3051\u307e\u3059\uff08\u4f8b\uff1aAZ\u5217\u304b\u3089CA\u3001\u6b21\u306bCO\u3001FL\u306a\u3069\uff09\u3002", "\u6700\u5f8c\u306b\u3001\u5168\u4f53(\u4e0a\u304b\u3089\u4e0b\u3001\u5de6\u304b\u3089\u53f3\uff09\u306e\u65b9\u5411\u3067\u306f\u3001\u5024\u306f\u5217\u306e\u4e0b\u65b9\u5411\u3068\u540c\u69d8\u306b\u30b0\u30eb\u30fc\u30d7\u3092\u7121\u8996\u3057\u3066\u4e0a\u304b\u3089\u4e0b\u306b\u7d2f\u7a4d\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u308c\u306f\u5e95\u90e8\u3067\u7d2f\u7a4d\u3092\u505c\u6b62\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u53f3\u306b\u5411\u304b\u3063\u3066\u7d2f\u7a4d\u3057\u7d9a\u3051\u307e\u3059\uff08\u305f\u3068\u3048\u3070\u3001AZ\u5217\u304b\u3089CA\u3001\u6b21\u306bCO\u3001FL\u306a\u3069\uff09\u3002", "\u5217\uff08\u5de6\u304b\u3089\u53f3\u3001\u4e0a\u304b\u3089\u4e0b\uff09\u306e\u3088\u3046\u306a\u4ed6\u306e\u7d2f\u7a4d\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u3002\u4eca\u5f8c\u306e\u30ea\u30ea\u30fc\u30b9\u3067\u3053\u306e\u3088\u3046\u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3055\u3089\u306b\u8ffd\u52a0\u3057\u3066\u3044\u304f\u4e88\u5b9a\u3067\u3059\u306e\u3067\u3001\u304a\u697d\u3057\u307f\u306b\uff01", "1\u3064\u306e\u5217\u3060\u3051\u3067\u306a\u304f\u3001\u8907\u6570\u306e\u5217\u306b\u5bfe\u3057\u3066\u540c\u69d8\u306e\u30c7\u30fc\u30bf\u5909\u63db\u3092\u884c\u3044\u305f\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001\u8907\u6570\u306e\u5217\u306e\u30c7\u30fc\u30bf\u578b\u3092character\u304b\u3089date\u306b\u5909\u63db\u3057\u305f\u308a\u3001 \u307e\u305f\u306f\u3001\u8907\u6570\u306e\u5217\u306enumeric\u306e\u5024\u3092\u6b63\u898f\u5316\uff08\u30b9\u30b1\u30fc\u30eb\uff09\u3057\u305f\u3044\u5834\u5408\u306a\u3069\u3067\u3059\u3002 \u305d\u308c\u3089\u3092\u3001\u3072\u3068\u3064\u3072\u3068\u3064\u884c\u3046\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u3068\u3066\u3082\u9762\u5012\u306a\u4f5c\u696d\u3067\u3059\u3057\u3001\u30c7\u30fc\u30bf\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\u5730\u7344\u306b\u9665\u308a\u307e\u3059\u3002", "\u3053\u306e\u305f\u3081\u3001dplyr\u3068\u3044\u3046\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f \u2018mutate_at\u2019\u3001 \u2018mutate_if\u2019\u3001 \u2018mutate_all\u2019\u3068\u3044\u3046\u4e00\u9023\u306e\u95a2\u6570\u304c \u2018mutate\u2019\u30d5\u30a1\u30df\u30ea\u3068\u3057\u3066\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u79c1\u306f\u5404\u6a5f\u80fd\u306e\u8a73\u7d30\u306b\u3064\u3044\u3066\u5225\u9014\u8a18\u4e8b\u3092\u6295\u7a3f\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u307e\u3067\u3082Exploratory\u306b\u30ab\u30b9\u30bf\u30e0\u30b3\u30de\u30f3\u30c9\u5165\u529b\u3067\u3053\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002", "v4.2\u3067\u306f\u3001\u3053\u306e\u3088\u3046\u306a\u8907\u6570\u5217\u64cd\u4f5c\u3092\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\u306e\u65b0\u3057\u3044UI\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002 Command\u30ad\u30fc\uff08Mac\uff09\u307e\u305f\u306fControl\u30ad\u30fc\uff08Windows\uff09\u307e\u305f\u306fShift\u30ad\u30fc\uff08Mac\u3068Windows\u306e\u4e21\u65b9\uff09\u3092\u62bc\u3057\u306a\u304c\u3089\u8907\u6570\u306e\u5217\u3092\u9078\u629e\u3057\u3001\u5217\u30d8\u30c3\u30c0\u30fc\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u3092\u5909\u63db\u300d\u307e\u305f\u306f\u300c\u6570\u5024\u95a2\u6570\u3092\u64cd\u4f5c\u300d\u306a\u3069\u306e\u30e1\u30cb\u30e5\u30fc\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002", "\u305f\u3068\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8907\u6570\u306e\u5217\u306e\u6570\u5024\u3092\u6b63\u898f\u5316\uff08\u30b9\u30b1\u30fc\u30eb\uff09\u3057\u305f\u3044\u3068\u3057\u307e\u3057\u3087\u3046\u3002 \u300c\u6570\u5024\u95a2\u6570\u3092\u64cd\u4f5c\u300d\u3067normalize\u95a2\u6570\u3092\u9078\u629e\u3057\u307e\u3059\u3002", "\u3053\u308c\u306f\u3001 \u2018exploratory\u2019\u30d1\u30c3\u30b1\u30fc\u30b8\u306e \u2018normalize\u2019\u95a2\u6570\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u3053\u308c\u3089\u306e\u5217\u3092\u4e00\u5ea6\u306b\u6b63\u898f\u5316\uff08\u307e\u305f\u306f\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff09\u3057\u307e\u3059\u3002", "\u3042\u308b\u3044\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u8907\u6570\u306e\u5217\u306e\u30c7\u30fc\u30bf\u578b\u3092character\u304b\u3089date\u306b\u5909\u63db\u3057\u305f\u3044\u3068\u3057\u307e\u3057\u3087\u3046\u3002 \u3053\u306e\u5834\u5408\u306f\u3001\u3053\u308c\u3089\u306e\u5bfe\u8c61\u5217\u3092\u9078\u629e\u3057\u3066\u300c\u65e5\u4ed8/\u6642\u523b\u306b\u5909\u63db\u300d\u3092\u9078\u629e\u3057\u3001\u5217\u30d8\u30c3\u30c0\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300cYear, Month, Day, Hour, Minute, Second\u300d\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002", "\u3053\u308c\u3089\u306e\u5217\u306f\u3001 \u2018lubridate\u2019\u30d1\u30c3\u30b1\u30fc\u30b8\u306e \u2018ymd_hms\u2019\u30d5\u30a1\u30f3\u30af\u30b7\u30e7\u30f3\u3067POSIXct\uff08\u65e5\u4ed8/\u6642\u523b\uff09\u306b\u5909\u63db\u3055\u308c\u3066\u3044\u307e\u3059\u3002", "\u5217\u30d8\u30c3\u30c0\u30fc\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u3053\u306e\u3088\u3046\u306a\u64cd\u4f5c\u3092\u5b9f\u884c\u3057\u3088\u3046\u3068\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30c0\u30a4\u30a2\u30ed\u30b0\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002", "\u305d\u306e\u307e\u307e\u5b9f\u884c\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u3059\u3050\u306b\u5b9f\u884c\u3067\u304d\u307e\u3059\u304c\u3001\u30c0\u30a4\u30a2\u30ed\u30b0\u306e\u5de6\u5074\u3067\u9078\u629e\u3059\u308b\u5217\u3092\u5909\u66f4\u3057\u305f\u308a\u3001\u53f3\u5074\u3067\u5b9f\u884c\u3059\u308b\u5185\u5bb9\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002", "v4.2\u3067\u306f\u3001\u753b\u9762\u53f3\u5074\u306b\u8868\u793a\u3055\u308c\u3066\u3044\u308b\u30b9\u30c6\u30c3\u30d7\u3092\u30c9\u30e9\u30c3\u30b0\u3067\u4efb\u610f\u306e\u5834\u6240\u306b\u79fb\u52d5\u3067\u304d\u307e\u3059\u3002 \u3053\u308c\u306f\u3001\u7279\u306b\u3001\u8a08\u7b97\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u3001\u7d50\u5408\u306a\u3069\u306e\u624b\u9806\u3092\u79fb\u52d5\u3059\u308b\u5834\u5408\u306b\u4fbf\u5229\u3067\u3059\u3002", "Date\u304a\u3088\u3073POSIXct\uff08\u65e5\u4ed8/\u6642\u523b\uff09\u30c7\u30fc\u30bf\u578b\u306e\u5217\u306b\u30d5\u30a3\u30eb\u30bf\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f\u3002 \u4e3b\u306a\u6539\u5584\u70b9\u306f2\u3064\u3067\u3059\u3002\u3072\u3068\u3064\u306f\u3001\u300c\u76f8\u5bfe\u7684\u306a\u300d\u65e5\u4ed8\u30bf\u30a4\u30d7\u3092\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u305f\u3068\u3048\u3070\u3001\u524d\u5e74\u306e\u30c7\u30fc\u30bf\u3092\u6b21\u306e\u3088\u3046\u306b\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u3053\u306e\u64cd\u4f5c\u306f\u5185\u90e8\u7684\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092\u767a\u884c\u3057\u307e\u3059\u3002", "\u3082\u3046\u3072\u3068\u3064\u306f\u3001\u5185\u90e8\u7684\u306b \u2018\u5e74\u2019\u3001 \u2018\u6708\u2019\u306a\u3069\u306e\u65e5\u4ed8/\u6642\u523b\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\u6a5f\u80fd\u3067\u3059\u3002 \u305f\u3068\u3048\u3070\u30012017\u306e\u30c7\u30fc\u30bf\u3060\u3051\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\u3059\u308b\u5834\u5408\u306b\u306f\u3001Value Type\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u300cYear\u300d\u3092\u9078\u629e\u3057\u3001\u300c2017\u300d\u3068\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002", "\u3053\u306e\u64cd\u4f5c\u306f\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u767a\u884c\u3057\u3066\u3044\u307e\u3059\u3002", "\u63a2\u7d22\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u3092\u884c\u3046\u969b\u306b\u306f\u3001\u69d8\u3005\u306a\u4f5c\u696d\u3092\u5b9f\u9a13\u7684\u306b\u3084\u3063\u3066\u307f\u308b\u3068\u3044\u3046\u3053\u3068\u304c\u591a\u3005\u3042\u308b\u3067\u3057\u3087\u3046\u3002 \u307e\u305f\u3001\u305d\u306e\u3088\u3046\u306a\u72b6\u6cc1\u4e0b\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30e9\u30f3\u30b0\u30ea\u30f3\u30b0\u3084\u53ef\u8996\u5316\u3092\u3088\u308a\u30b9\u30d4\u30fc\u30c7\u30a3\u306b\u884c\u3044\u305f\u3044\u306f\u305a\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u30c7\u30fc\u30bf\u91cf\u304c\u591a\u3051\u308c\u3070\u591a\u3044\u307b\u3069\u3001\u305d\u306e\u30b9\u30d4\u30fc\u30c9\u304c\u9045\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002", "v4.2\u3067\u306f\u3001\u300c\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u300d\u30e2\u30fc\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u4e0a\u90e8\u306e\u300c\u30b5\u30f3\u30d7\u30eb\u300d\u3068\u3044\u3046\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u30a4\u30f3\u30dd\u30fc\u30c8\u3055\u308c\u305f\u5143\u306e\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3092\u7e2e\u5c0f\u3057\u3001\u30c7\u30fc\u30bf\u306e\u63a2\u7d22\u3092\u7d9a\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u5f90\u3005\u306b\u65b9\u5411\u6027\u304c\u898b\u3048\u3066\u304d\u305f\u3068\u3053\u308d\u3067\u3001\u3082\u3046\u4e00\u5ea6\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30e2\u30fc\u30c9\u3092\u30aa\u30d5\u306b\u3057\u3001\u53f3\u5074\u306b\u8ffd\u52a0\u3057\u305f\u3059\u3079\u3066\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u30c7\u30fc\u30bf\u5168\u4f53\u306b\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002", "\u30c7\u30fc\u30bf\u306b\u6b20\u640d\u5024\u3084NA\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u3001\u3072\u3068\u3064\u524d\u306e\u5024\uff08\u6301\u3061\u8d8a\u3057\uff09\u307e\u305f\u306f\u6b21\u306e\u5024\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3053\u3068\u3067\u5bfe\u5fdc\u3057\u307e\u3059\u3002 R\u306b\u306f\u3001\u3053\u306e\u4f5c\u696d\u3092\u7c21\u5358\u306b\u884c\u3046\u305f\u3081\u306e \u2018fill\u2019\u3068\u3044\u3046\u30d5\u30a1\u30f3\u30af\u30b7\u30e7\u30f3\u304c\u2018tidyr\u2019\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3042\u308a\u307e\u3059\u3002 v4.2\u3067\u306f\u3001\u3053\u306e\u30d5\u30a1\u30f3\u30af\u30b7\u30e7\u30f3\u306b\u5217\u30d8\u30c3\u30c0\u30fc\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306bUI\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u904e\u53bb\u306e\u7c73\u56fd\u306e\u30d3\u30fc\u30eb\u7a0e\u7387\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002\u2019RATE'\u306e\u5217\u306e\u6b20\u640d\u5024\u3092\u524d\u306e\u5024\u3092\u30b3\u30d4\u30fc\u3057\u3066\u3046\u3081\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002", "\u5217\u30d8\u30c3\u30c0\u30fc\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u3001\u30c7\u30fc\u30bf\u3092\u7f6e\u63db/\u5909\u63db\u3001\u6b20\u640d\u5024\u3092\u524d\u5f8c\u306e\u5024\u3067\u57cb\u3081\u308b\u3092\u9078\u629e\u3057\u307e\u3059\u3002", "\u305d\u3046\u3059\u308b\u3068\u3001NA\u306e\u5024\u304c\u4ee5\u524d\u306e\u30c7\u30fc\u30bf\u304c\u5165\u529b\u3055\u308c\u3066\u3044\u308b\u30bb\u30eb\u306e\u30c7\u30fc\u30bf\u306b\u7f6e\u304d\u63db\u308f\u308a\u307e\u3057\u305f\u3002", "\u6570\u5024\u5217\u306e\u30d3\u30f3\uff08\u30ab\u30c6\u30b4\u30ea\uff09\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306f\u3001\u5217\u30d8\u30c3\u30c0\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30d3\u30f3\uff08\u30ab\u30c6\u30b4\u30ea\uff09\u306e\u4f5c\u6210\u300d\u30e1\u30cb\u30e5\u30fc\u3092\u9078\u629e\u3057\u307e\u3059\u3002 \u624b\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u30d3\u30f3\u3092\u4f5c\u6210\u3059\u308b\u3068\u304d\u306b\u3001\u5404\u30d3\u30f3\u306e\u30e9\u30d9\u30eb\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "\u300c\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\uff08\u884c\u305d\u306e\u307e\u307e-\u533a\u5207\u308a\u306a\u3057\uff09\u300d\u3068\u3044\u3046\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u30bd\u30fc\u30b9\u3092\u8ffd\u52a0\u3057\u3001\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u30921\u884c\u305a\u3064\u8aad\u307f\u8fbc\u3093\u3067\u30c7\u30fc\u30bf\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "Google BigQuery\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3092\u30c1\u30e3\u30fc\u30c8\u3001\u30ce\u30fc\u30c8\u3001\u30b9\u30e9\u30a4\u30c9\u3001\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30ea\u30f3\u30b0\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002", "\u4ee5\u524d\u306e\u30ea\u30ea\u30fc\u30b9\u3067\u306f\u3001\u30ce\u30fc\u30c8\uff08\u307e\u305f\u306f\u30b9\u30e9\u30a4\u30c9\uff09\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u9069\u5207\u306b\u4fdd\u5b58\u3067\u304d\u305a\u3001\u9593\u9055\u3063\u305f\u60c5\u5831\u3067\u65e2\u5b58\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u4e0a\u66f8\u304d\u3057\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u30d0\u30b0\u304c\u3042\u308a\u307e\u3057\u305f\u3002 \u3053\u306e\u3088\u3046\u306a\u3053\u3068\u304c\u3001\u6587\u7ae0\u4f5c\u6210\u3084\u7de8\u96c6\u306b\u6642\u9593\u3092\u8cbb\u3084\u3057\u305f\u5f8c\u306b\u767a\u751f\u3059\u308b\u3068\u3001\u975e\u5e38\u306b\u30a4\u30e9\u30a4\u30e9\u3057\u307e\u3059\u3002 \u3082\u3061\u308d\u3093\u3001\u79c1\u305f\u3061\u306fv4.1\u306e\u30d1\u30c3\u30c1\u3067\u30d0\u30b0\u3092\u4fee\u6b63\u3057\u3001\u305d\u308c\u4ee5\u6765\u3001\u3053\u306e\u30a8\u30ea\u30a2\u306e\u5468\u8fba\u306b\u3055\u3089\u306b\u591a\u304f\u306e\u30c6\u30b9\u30c8\u30b7\u30ca\u30ea\u30aa\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002 \u3057\u304b\u3057\u4eca\u5f8c\u3001\u540c\u3058\u3088\u3046\u306a\u554f\u984c\u3067\u30e6\u30fc\u30b6\u30fc\u304c\u6642\u9593\u3092\u7121\u99c4\u306b\u3057\u306a\u3044\u3053\u3068\u3092100\uff05\u7d04\u675f\u3059\u308b\u305f\u3081\u306b\u3001\u30ce\u30fc\u30c8\u3068\u30b9\u30e9\u30a4\u30c9\u306e\u5909\u66f4\u5c65\u6b74\u304b\u3089\u5fa9\u5143\u3067\u304d\u308b\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002", "\u5909\u66f4\u5c65\u6b74\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\u3002", "\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u306b\u623b\u3059\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067\u5fa9\u5143\u3067\u304d\u307e\u3059\u3002", "\u3053\u3053\u304b\u3089\u306f\u3001\u30ce\u30fc\u30c8\u3084\u30b9\u30e9\u30a4\u30c9\u306b\u7279\u306b\u95a2\u5fc3\u304c\u3042\u308b\u65b9\u3078\u306e\u6280\u8853\u7684\u306a\u60c5\u5831\u3067\u3059\u3002\u30ce\u30fc\u30c8\u3068\u30b9\u30e9\u30a4\u30c9\u306f\u3001\u6587\u5b57\u3092\u30bf\u30a4\u30d7\u3057\u305f\u308a\u30b0\u30e9\u30d5\u3092\u633f\u5165\u3057\u305f\u308a\u3059\u308b\u305f\u3073\u306b\u81ea\u52d5\u7684\u306b\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u3053\u306e\u81ea\u52d5\u7684\u306b\u4fdd\u5b58\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u5185\u306b\u4e00\u65e6\u4e00\u6642\u7684\u306b\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002\u30ce\u30fc\u30c8\u3084\u30b9\u30e9\u30a4\u30c9\u304b\u3089\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u79fb\u52d5\u3059\u308b\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u9589\u3058\u308b\u306a\u3069\u306e\u64cd\u4f5c\u3092\u3059\u308b\u3068\u3001Exploratory\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u304c\u3042\u308b\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u306b\u30b3\u30f3\u30c6\u30f3\u30c4\u304c\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u300c\u6c38\u7d9a\u7684\u300d\u4fdd\u5b58\u3068\u547c\u3070\u308c\u3001\u30ce\u30fc\u30c8\u3084\u30b9\u30e9\u30a4\u30c9\u3092\u958b\u3044\u305f\u72b6\u614b\u3067\u306f\u767a\u751f\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u3053\u306ev4.2\u3067\u306f\u3001\u300c\u6c38\u7d9a\u7684\u306a\u300d\u4fdd\u5b58\u306f10\u5206\u3054\u3068\u306b\u81ea\u52d5\u7684\u306b\u884c\u308f\u308c\u307e\u3059\u3002\u3053\u306e\u300c\u6c38\u7d9a\u7684\u306a\u300d\u4fdd\u5b58\u306f\u3001\u521d\u3081\u3066Exploratory Desktop\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3068\u304d\u306b\u5c0b\u306d\u3089\u308c\u308b\u30bd\u30fc\u30b9\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u30b7\u30b9\u30c6\u30e0\u3067\u3042\u308bGit\u3092\u4ecb\u3057\u3066\u884c\u308f\u308c\u307e\u3059\u3002\u3053\u308c\u304c\u3001\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u53e4\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5fa9\u5143\u3067\u304d\u308b\u7406\u7531\u3067\u3059\u3002 \u305d\u3057\u3066\u3001\u5c06\u6765\u306e\u30ea\u30ea\u30fc\u30b9\u3067\u306f\u3053\u306e\u6a5f\u80fd\u3092\u4ed6\u306e\u9818\u57df\u306b\u3082\u62e1\u5927\u3057\u3066\u3044\u304f\u4e88\u5b9a\u3067\u3059\uff01", "\u4ee5\u4e0a\u3067\u3059\uff01", "\u3059\u3079\u3066\u306e\u6a5f\u80fd\u3092\u4f7f\u3044\u3053\u306a\u3059\u306e\u306b\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u4eca\u56de\u306e\u30ea\u30ea\u30fc\u30b9\u3067\u3001\u307f\u306a\u3055\u3093\u306e\u63a2\u7d22\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u3068\u30ec\u30dd\u30fc\u30c8\u4f5c\u6210\u304c\u3055\u3089\u306b\u751f\u7523\u7684\u304b\u3064\u52b9\u679c\u7684\u306b\u306a\u308c\u3070\u3068\u601d\u3063\u3066\u304a\u308a\u307e\u3059\u3002 \u662f\u975e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u3066\u3001\u4f7f\u3063\u3066\u307f\u3066\u304f\u3060\u3055\u3044\uff01", "\u3059\u3079\u3066\u306e\u62e1\u5f35\u6a5f\u80fd\u3068\u30d0\u30b0\u4fee\u6b63\u306b\u95a2\u3057\u3066\u306f", "\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u3001v4.2\u306f\u3053\u3061\u3089\u306e", "\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u307e\u3059\uff01", "\u307e\u3060\u30a2\u30ab\u30a6\u30f3\u30c8\u3092\u304a\u6301\u3061\u3067\u306a\u3044\u5834\u5408\u306f\u300130\u65e5\u9593\u7121\u6599\u30c8\u30e9\u30a4\u30a2\u30eb\u306b\u30b5\u30a4\u30f3\u30a2\u30c3\u30d7\u3057\u3066\u8a66\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \u73fe\u5728\u5b66\u6821\u306e\u751f\u5f92\u3084\u6559\u5e2b\u3067\u3042\u308b\u5834\u5408\u306f\u3001\u7121\u6599\u3067\u3059\uff01", "Written by"], "postingTime": "2018-02-20T09:28:58.096Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Susan Li", "articleTile": "Multi-Class Text Classification with Scikit-Learn - Towards Data Science", "content": ["There are lots of applications of text classification in the commercial world. For example, news stories are typically organized by topics; content or products are often tagged by categories; users can be classified into cohorts based on how they talk about a product or brand online \u2026", "However, the vast majority of text classification articles and tutorials on the internet are binary text classification such as email spam filtering (spam vs. ham), sentiment analysis (positive vs. negative). In most cases, our real world problem are much more complicated than that. Therefore, this is what we are going to do today: Classifying Consumer Finance Complaints into 12 pre-defined classes. The data can be downloaded from ", ".", "We use ", " and ", " to develop our system, relying on ", " for the machine learning components. If you would like to see an implementation in ", ", read the ", ".", "The problem is supervised text classification problem, and our goal is to investigate which supervised machine learning methods are best suited to solve it.", "Given a new complaint comes in, we want to assign it to one of 12 categories. The classifier makes the assumption that each new complaint is assigned to one and only one category. This is multi-class text classification problem. I can\u2019t wait to see what we can achieve!", "Before diving into training machine learning models, we should look at some examples first and the number of complaints in each class:", "For this project, we need only two columns \u2014 \u201cProduct\u201d and \u201cConsumer complaint narrative\u201d.", "Example: \u201c I have outdated information on my credit report that I have previously disputed that has yet to be removed this information is more then seven years old and does not meet credit reporting requirements\u201d", "Example: Credit reporting", "We will remove missing values in \u201cConsumer complaints narrative\u201d column, and add a column encoding the product as an integer because categorical variables are often better represented by integers than strings.", "We also create a couple of dictionaries for future use.", "After cleaning up, this is the first five rows of the data we will be working on:", "We see that the number of complaints per product is imbalanced. Consumers\u2019 complaints are more biased towards Debt collection, Credit reporting and Mortgage.", "When we encounter such problems, we are bound to have difficulties solving them with standard algorithms. Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration. In the worst case, minority classes are treated as outliers and ignored. For some cases, such as fraud detection or cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for example by ", " each class.", "However, in our case of learning imbalanced data, the majority classes might be of our great interest. It is desirable to have a classifier that gives high prediction accuracy over the majority class, while maintaining reasonable accuracy for the minority classes. Therefore, we will leave it as it is.", "The classifiers and learning algorithms can not directly process the text documents in their original form, as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. Therefore, during the preprocessing step, the texts are converted to a more manageable representation.", "One common approach for extracting features from text is to use the bag of words model: a model where for each document, a complaint narrative in our case, the presence (and often the frequency) of words is taken into consideration, but the order in which they occur is ignored.", "Specifically, for each term in our dataset, we will calculate a measure called Term Frequency, Inverse Document Frequency, abbreviated to tf-idf. We will use ", "to calculate a ", " vector for each of consumer complaint narratives:", "(4569, 12633)", "Now, each of 4569 consumer complaint narratives is represented by 12633 features, representing the tf-idf score for different unigrams and bigrams.", "We can use ", " to find the terms that are the most correlated with each of the products:", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . bank", " . overdraft", " . Most correlated bigrams:", " . overdraft fees", " . checking account", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . car", " . vehicle", " . Most correlated bigrams:", " . vehicle xxxx", " . toyota financial", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . citi", " . card", " . Most correlated bigrams:", " . annual fee", " . credit card", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . experian", " . equifax", " . Most correlated bigrams:", " . trans union", " . credit report", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . collection", " . debt", " . Most correlated bigrams:", " . collect debt", " . collection agency", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . wu", " . paypal", " . Most correlated bigrams:", " . western union", " . money transfer", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . modification", " . mortgage", " . Most correlated bigrams:", " . mortgage company", " . loan modification", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . dental", " . passport", " . Most correlated bigrams:", " . help pay", " . stated pay", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . borrowed", " . payday", " . Most correlated bigrams:", " . big picture", " . payday loan", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . serve", " . prepaid", " . Most correlated bigrams:", " . access money", " . prepaid card", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . student", " . navient", " . Most correlated bigrams:", " . student loans", " . student loan", "# \u2018", "\u2019:", " . Most correlated unigrams:", " . handles", " . https", " . Most correlated bigrams:", " . xxxx provider", " . money want", "They all make sense, don\u2019t you think so?", "After all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms we can use for this type of problem.", "After fitting the training set, let\u2019s make some predictions.", "[\u2018Debt collection\u2019]", "[\u2018Credit reporting\u2019]", "Not too shabby!", "We are now ready to experiment with different machine learning models, evaluate their accuracy and find the source of any potential issues.", "We will benchmark the following four models:", "model_name", "0.822890", " 0.792927", " 0.688519", " 0.443826", "Name: accuracy, dtype: float64", "LinearSVC and Logistic Regression perform better than the other two classifiers, with LinearSVC having a slight advantage with a median accuracy of around 82%.", "Continue with our best model (LinearSVC), we are going to look at the confusion matrix, and show the discrepancies between predicted and actual labels.", "The vast majority of the predictions end up on the diagonal (predicted label = actual label), where we want them to be. However, there are a number of misclassifications, and it might be interesting to see what those are caused by:", "As you can see, some of the misclassified complaints are complaints that touch on more than one subjects (for example, complaints involving both credit card and credit report). This sort of errors will always happen.", "Again, we use the ", " to find the terms that are the most correlated with each of the categories:", "# \u2018", "\u2019:", " . Top unigrams:", " . bank", " . account", " . Top bigrams:", " . debit card", " . overdraft fees", "# \u2018", "\u2019:", " . Top unigrams:", " . vehicle", " . car", " . Top bigrams:", " . personal loan", " . history xxxx", "# \u2018", "\u2019:", " . Top unigrams:", " . card", " . discover", " . Top bigrams:", " . credit card", " . discover card", "# \u2018", "\u2019:", " . Top unigrams:", " . equifax", " . transunion", " . Top bigrams:", " . xxxx account", " . trans union", "# \u2018", "\u2019:", " . Top unigrams:", " . debt", " . collection", " . Top bigrams:", " . account credit", " . time provided", "# \u2018", "\u2019:", " . Top unigrams:", " . paypal", " . transfer", " . Top bigrams:", " . money transfer", " . send money", "# \u2018", "\u2019:", " . Top unigrams:", " . mortgage", " . escrow", " . Top bigrams:", " . loan modification", " . mortgage company", "# \u2018", "\u2019:", " . Top unigrams:", " . passport", " . dental", " . Top bigrams:", " . stated pay", " . help pay", "# \u2018", "\u2019:", " . Top unigrams:", " . payday", " . loan", " . Top bigrams:", " . payday loan", " . pay day", "# \u2018", "\u2019:", " . Top unigrams:", " . prepaid", " . serve", " . Top bigrams:", " . prepaid card", " . use card", "# \u2018", "\u2019:", " . Top unigrams:", " . navient", " . loans", " . Top bigrams:", " . student loan", " . sallie mae", "# \u2018", "\u2019:", " . Top unigrams:", " . https", " . tx", " . Top bigrams:", " . money want", " . xxxx provider", "They are consistent within our expectation.", "Finally, we print out the classification report for each class:", "Source code can be found on ", ". I look forward to hear any feedback or questions.", "Written by"], "postingTime": "2018-02-20T16:54:33.630Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "George Seif", "articleTile": "The 5 Clustering Algorithms Data Scientists Need to Know", "content": ["Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.", "In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we\u2019re going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!", "K-Means is probably the most well-known clustering algorithm. It\u2019s taught in a lot of introductory data science and machine learning classes. It\u2019s easy to understand and implement in code! Check out the graphic below for an illustration.", "K-Means has the advantage that it\u2019s pretty fast, as all we\u2019re really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity ", "(", ").", "On the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn\u2019t always trivial and ideally with a clustering algorithm we\u2019d want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.", "K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.", "Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.", "An illustration of the entire process from end-to-end with all of the sliding windows is shown below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.", "In contrast to K-means clustering, there is no need to select the number of clusters as mean-shift automatically discovers this. That\u2019s a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius \u201cr\u201d can be non-trivial.", "DBSCAN is a density-based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let\u2019s get started!", "DBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises, unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it can find arbitrarily sized and arbitrarily shaped clusters quite well.", "The main drawback of DBSCAN is that it doesn\u2019t perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold \u03b5 and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold \u03b5 becomes challenging to estimate.", "One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn\u2019t the best way of doing things by looking at the image below. On the left-hand side, it looks quite obvious to the human eye that there are two circular clusters with different radius\u2019 centered at the same mean. K-Means can\u2019t handle this because the mean values of the clusters are very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.", "Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have a standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.", "To find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation), we will use an optimization algorithm called Expectation\u2013Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed with the process of Expectation\u2013Maximization clustering using GMMs.", "There are 2 key advantages to using GMMs. Firstly GMMs are a lot more ", " in terms of ", " than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster\u2019s covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support ", " ", ".", "Hierarchical clustering algorithms fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or ", ") pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called ", " or ", ". This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm steps", "Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can\u2019t do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of ", ", unlike the linear complexity of K-Means and GMM.", "There are your top 5 clustering algorithms that a data scientist should know! We\u2019ll end off with an awesome visualization of how well these algorithms and a few others perform, courtesy of Scikit Learn! Very cool to see how the different algorithms compare and contrast with different data!", "Follow me on", " where I post all about the latest and greatest AI, Technology, and Science! Connect with me on ", " too!", "Written by"], "postingTime": "2019-09-14T00:40:07.243Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Irhum Shafkat", "articleTile": "Intuitively Understanding Variational Autoencoders - Towards Data Science", "content": ["In contrast to the more standard uses of neural networks as regressors or classifiers, Variational Autoencoders (VAEs) are powerful ", " models, now having applications as diverse as from generating fake human faces, to producing purely synthetic music.", "This post will explore what a VAE is, the intuition behind why it works so well, and its uses as a powerful generative tool for all kinds of media.", "When using generative models, you could simply want to generate a random, new output, that looks similar to the training data, and you can certainly do that too with VAEs. But more often, you\u2019d like to alter, or explore variations on ", ", and not just in a random way either, but in a desired, ", " direction. This is where VAEs work better than any other method currently available.", "An autoencoder network is actually a pair of two connected networks, an encoder and a decoder. An encoder network takes in an input, and converts it into a smaller, dense representation, which the decoder network can use to convert it back to the original input.", "If you\u2019re unfamiliar with encoder networks, but familiar with Convolutional Neural Networks (CNNs), chances are, you already know what an encoder does.", "The convolutional layers of any CNN take in a large image (eg. rank 3 tensor of size 299x299x3), and convert it to a much more compact, dense representation (eg. rank 1 tensor of size 1000). This dense representation is then used by the fully connected classifier network to classify the image.", "The encoder is similar, it is simply is a network that takes in an input and produces a much smaller representation (the ", ", that contains enough information for the next part of the network to process it into the desired output format. Typically, the encoder is trained together with the other parts of the network, optimized via back-propagation, to produce encodings specifically useful for the task at hand. In CNNs, the 1000-dimensional encodings produced are such that they\u2019re specifically useful for classification.", "Autoencoders take this idea, and slightly flip it on its head, by making the encoder generate encodings specifically useful for ", "The entire network is usually trained as a whole. The loss function is usually either the mean-squared error or cross-entropy between the output and the input, known as the", ", which penalizes the network for creating outputs different from the input.", "As the encoding (which is simply the output of the hidden layer in the middle) has far less units than the input, the encoder must choose to discard information. The encoder learns to preserve as much of the relevant information as possible in the limited encoding, and intelligently discard irrelevant parts. The decoder learns to take the encoding and properly reconstruct it into a full image. Together, they form an autoencoder.", "Standard autoencoders learn to generate compact representations and reconstruct their inputs well, but asides from a few applications like denoising autoencoders, they are fairly limited.", "The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous, or allow easy interpolation.", "For example, training an autoencoder on the MNIST dataset, and visualizing the encodings from a 2D latent space reveals the formation of distinct clusters. This makes sense, as distinct encodings for each image type makes it far easier for the decoder to decode them. This is fine if you\u2019re just ", " the same images.", "But when you\u2019re building a ", "model, you ", " want to prepare to ", " the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space.", "If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has ", "how to deal with that region of the latent space. During training, it ", " encoded vectors coming from that region of latent space.", "Variational Autoencoders (VAEs) have one fundamentally unique property that separates them from vanilla autoencoders, and it is this property that makes them so useful for generative modeling: their latent spaces are, ", "continuous, allowing easy random sampling and interpolation.", "It achieves this by doing something that seems rather surprising at first: making its encoder not output an encoding vector of size", "rather, outputting two vectors of size n: a vector of means", ", and another vector of standard deviations, ", ".", "They form the parameters of a vector of random variables of length n, with the ", "th element of ", " and ", "being the mean and standard deviation of the", "th random variable, ", "i, from which we sample, to obtain the sampled encoding which we pass onward to the decoder:", "This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.", "Intuitively, the mean vector controls where the encoding of an input should be centered around, while the standard deviation controls the \u201carea\u201d, how much from the mean the encoding can vary. As encodings are generated at random from anywhere inside the \u201ccircle\u201d (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training. In code:", "The model is now exposed to a certain degree of local variation by varying the encoding of one sample, resulting in smooth latent spaces on a local scale, that is, for similar samples. Ideally, we want overlap between samples that are not very similar too, in order to interpolate ", " classes. However, since there are", "on what values", "vectors ", " and ", " can take on, the encoder can learn to generate very different ", " for different classes, clustering them apart, and minimize ", ", making sure the encodings themselves don\u2019t vary much for the same sample (that is, less uncertainty for the decoder). This allows the decoder to efficiently reconstruct the ", " data.", "What we ideally want are encodings, ", " of which are as close as possible to each other while still being distinct, allowing smooth interpolation, and enabling the construction of ", "samples.", "In order to force this, we introduce the Kullback\u2013Leibler divergence (KL divergence", ") into the loss function. The KL divergence between two probability distributions simply measures how much they ", " from each other. Minimizing the KL divergence here means optimizing the probability distribution parameters ", " and ", "to closely resemble that of the target distribution.", "For VAEs, the KL loss is equivalent to the ", " of all the KL divergences between the ", "X", "~", "(\u03bc", ", \u03c3", "\u00b2) in ", ", and the standard normal", ". It\u2019s minimized when \u03bc", " = 0, \u03c3", " = 1.", "Intuitively, this loss encourages the encoder to distribute all encodings (for all types of inputs, eg. all MNIST numbers), evenly around the center of the latent space. If it tries to \u201ccheat\u201d by clustering them apart into specific regions, away from the origin, it will be penalized.", "Now, using purely KL loss results in a latent space results in encodings densely placed randomly, near the center of the latent space, with little regard for similarity among nearby encodings. The decoder finds it impossible to decode anything meaningful from this space, simply because there really isn\u2019t any meaning.", "Optimizing the two together, however, results in the generation of a latent space which maintains the similarity of nearby encodings on the ", "via clustering, yet ", " is very densely packed near the latent space origin (compare the axes with the original).", "Intuitively, this is the equilibrium reached by the ", "nature of the reconstruction loss, and the", " nature of the KL loss, forming distinct clusters the decoder can decode. This is great, as it means when randomly generating, if you sample a vector from the same prior distribution of the encoded vectors, ", "(", ", ", "), the decoder will successfully decode it. And if you\u2019re interpolating, there are no sudden gaps between clusters, but a", "a decoder can understand.", "So how do we actually produce these smooth interpolations we speak of? From here on out, it\u2019s simple vector arithmetic in the latent space.", "For example, if you wish to generate a new sample halfway between two samples, just find the difference between their mean (", ") vectors, and add half the difference to the original, and then simply decode it.", "What about generating", ", such as generating glasses on a face? Find two samples, one with glasses, one without, obtain their encoded vectors from the encoder, and save the difference. Add this new \u201cglasses\u201d vector to any other face image, and decode it.", "There are plenty of further improvements that can be made over the variational autoencoder. You could indeed, replace the standard fully-connected dense encoder-decoder with a convolutional-deconvolutional encoder-decoder pair, such as this project", ", to produce great synthetic human face photos.", "You could even train an autoencoder using LSTM encoder-decoder pairs (using a modified version of the seq2seq architecture) for ", " data (something not possible with methods such as GANs), to produce synthetic text, or even interpolate between MIDI samples such as Google Brain\u2019s Magenta\u2019s MusicVAE", ":", "VAEs work with remarkably diverse types of data, sequential or non-sequential, continuous or discrete, even labelled or completely unlabelled, making them highly powerful generative tools. I hope you now understand how VAEs work, and that you will be able to use them on your own generative endeavors as well.", "[1]", "[2]", "[3]", "[4]", "[5]", "Further reading:", "Implementations:", "Written by"], "postingTime": "2018-04-05T14:44:46.839Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Francesco Zuppichini", "articleTile": "How to use Dataset in TensorFlow - Towards Data Science", "content": ["The built-in Input Pipeline. Never use \u2018feed-dict\u2019 anymore", "As you should know, ", " is the slowest possible way to pass information to TensorFlow and it must be avoided. The correct way to feed data into your models is to use an input pipeline to ensure that the GPU has never to wait for new stuff to come in.", "Fortunately, TensorFlow has a built-in API, called ", " to make it easier to accomplish this task. In this tutorial, we are going to see how we can create an input pipeline and how to feed the data into the model efficiently.", "This article will explain the basic mechanics of the Dataset, covering the most common use cases.", "You can found all the code as a jupyter notebook here :", "In order to use a Dataset we need three steps:", "We first need some data to put inside our dataset", "This is the common case, we have a numpy array and we want to pass it to tensorflow.", "We can also pass more than one numpy array, one classic example is when we have a couple of data divided into features and labels", "We can, of course, initialise our dataset with some tensor", "This is useful when we want to dynamically change the data inside the Dataset, we will see later how.", "We can also initialise a Dataset from a generator, this is useful when we have an array of different elements length (e.g a sequence):", "Ouputs:", "In this case, you also need to specify the types and the shapes of your data that will be used to create the correct tensors.", "You can directly read a csv file into a dataset. For example, I have a csv file with tweets and their sentiment.", "I can now easily create a ", " from it by calling ", " . Be aware that the iterator will create a dictionary with key as the column names and values as Tensor with the correct row value.", "Where ", " is", "We have seen how to create a dataset, but how to get our data back? We have to use an ", ", that will give us the ability to iterate through the dataset and retrieve the real values of the data. There exist four types of iterators.", "This is the easiest iterator. Using the first example", "Then you need to call ", " to get the tensor that will contain your data", "We can run ", " in order to see its value", "In case we want to build a dynamic dataset in which we can change the data source at runtime, we can create a dataset with a placeholder. Then we can initialize the placeholder using the common ", " mechanism. This is done with an ", ". Using example three from last section", "This time we call ", " . Then, inside the", " scope, we run the ", " operation in order to pass our data, in this case a random numpy array. .", "Imagine that now we have a train set and a test set, a real common scenario:", "Then we would like to train the model and then evaluate it on the test dataset, this can be done by initialising the iterator again after training", "The concept is similar to before, we want to dynamic switch between data. But instead of feed new data to the same dataset, we switch dataset. As before, we want to have a train dataset and a test dataset", "We can create two Datasets", "Now, this is the trick, we create a generic Iterator", "and then two initialization operations:", "We get the next element as before", "Now, we can directly run the two initialisation operation using our session. Putting all together we get:", "This is very similar to the ", " iterator, but instead of switch between datasets, it switch between iterators. After we created two datasets", "One for training and one for testing. Then, we can create our iterator, in this case we use the ", " iterator, but you can also use a ", " iterator", "Now, we need to defined and ", " , that will be out placeholder that can be dynamically changed.", "Then, similar to before, we define a generic iterator using the shape of the dataset", "Then, we get the next elements", "In order to switch between the iterators we just have to call the ", " operation passing the correct ", " in the feed_dict. For example, to get one element from the train set:", "If you are using ", " iterators, as we are doing, just remember to initialize them before starting", "Putting all together we get:", "In the previous example we have used the session to print the value of the ", " element in the Dataset.", "In order to pass the data to a model we have to just pass the tensors generated from ", "In the following snippet we have a Dataset that contains two numpy arrays, using the same example from the first section. Notice that we need to wrap the ", " in another numpy array to add a dimension that we is needed to batch the data", "Then as always, we create an iterator", "We make a model, a simple neural network", "We ", " use the Tensors from ", " as input to the first layer and as labels for the loss function. Wrapping all together:", "Output:", "Usually batching data is a pain in the ass, with the ", "API we can use the method ", " that automatically batches the dataset with the provided size. The default value is one. In the following example, we use a batch size of 4", "Output:", "Using ", " we can specify the number of times we want the dataset to be iterated. If no parameter is passed it will loop forever, usually is good to just loop forever and directly control the number of epochs with a standard loop.", "We can shuffle the Dataset by using the method ", " that shuffles the dataset by default every epoch.", "We can also set the parameter ", " , a fixed size buffer from which the next element will be uniformly chosen from. Example:", "First run output:", "Second run output:", "Yep. It was shuffled. If you want, you can also set the ", " parameter.", "You can apply a custom function to each member of a dataset using the ", " method. In the following example we multiply each element by two:", "Output:", "In the example below we train a simple model using batching and we switch between train and test dataset using a ", "Output", "In the example below we train a simple model using batching and we switch between train and test dataset using a ", "TensorFlow dataset tutorial: ", "Dataset docs:", "The ", " API gives us a fast and robust way to create optimized input pipeline to train, evaluate and test our models. In this article, we have seen most of the common operation we can do with them.", "You can use the ", " that I\u2019ve made for this article as a reference.", "Thank you for reading,", "Francesco Saverio", "Written by"], "postingTime": "2019-05-27T21:26:57.335Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Will Koehrsen", "articleTile": "Statistical Significance Explained - Towards Data Science", "content": ["As the dean at a major university, you receive a concerning report showing your students get an average of 6.80 hours of sleep per night compared to the national college average of 7.02 hours. The student body president is worried about the health of students and points to this study as proof that homework must be reduced. The university president on the other hand dismisses the study as nonsense: \u201cBack in my day we got four hours of sleep a night and considered ourselves lucky.\u201d You have to decide if this is a serious issue. Fortunately, you\u2019re well-versed in statistics and finally see a chance to put your education to use!", "Statistical significance is one of those terms we often hear without really understanding. When someone claims data proves their point, we nod and accept it, assuming statisticians have done complex operations that yielded a result which cannot be questioned. In fact, statistical significance is not a complicated phenomenon requiring years of study to master, but a straightforward idea that everyone can \u2014 and should \u2014 understand. Like with most technical concepts, statistical significance is built on a few simple ideas: hypothesis testing, the normal distribution, and p values. In this article, we will briefly touch on all of these concepts (with further resources provided) as we work up to solving the conundrum presented above.", " An earlier edition of this post oversimplified the definition of the p-value. I would like to thank ", " for correcting my mistake. This was a great example of the type of collaborative learning possible online and I encourage any feedback, corrections, or discussion!", "The first idea we have to discuss is ", ". The \u201chypothesis\u201d refers to the researcher\u2019s initial belief about the situation before the study. This initial theory is known as the", " In our example these are:", "Notice how careful we have to be about the wording: we are looking for a very specific effect, which needs to be formalized in the hypotheses so after the fact we cannot claim to have been testing something else! (This is an example of a one-sided hypothesis test because we are concerned with a change in only one direction.) Hypothesis tests are one of the foundations of statistics and are used to assess the results of most studies. These studies can be anything from a medical trial to assess drug effectiveness to an observational study evaluating an exercise plan. What all studies have in common is that they are concerned with making comparisons, either between two groups or between one group and the entire population. In the medical example, we might compare the average time to recover between groups taking two different drugs, or, in our problem as dean, we want to compare sleep between our students and all the students in the country.", "The testing part of hypothesis tests allows us to determine which theory, the null or alternative, is better supported by the evidence. There are many hypothesis tests and we will use one called the z-test. However, before we can get to testing our data, we need to talk about two more crucial ideas.", "The second building block of statistical significance is the ", ", also called the Gaussian or bell curve. The normal distribution is used to represent how data from a process is distributed and is defined by the mean, given the Greek letter \u03bc (mu), and the standard deviation, given the letter \u03c3 (sigma). The mean shows the location of the center of the data and the standard deviation is the spread in the data.", "The application of the normal distribution comes from assessing data points in terms of the standard deviation. We can determine how anomalous a data point is based on how many standard deviations it is from the mean. The normal distribution has the following helpful properties:", "If we have a normal distribution for a statistic, we can characterize any point in terms of standard deviations from the mean. For example, average female height in the US is 65 inches (5' 5\") with a standard deviation of 4 inches. If we meet a new acquaintance who is 73 inches tall, we can say she is two standard deviations above the mean and is in the tallest 2.5% of females. (2.5% of females will be shorter than \u03bc \u2014 2\u03c3 (57 in) and 2.5% will be taller than \u03bc+2\u03c3).", "In statistics, instead of saying our data is two standard deviations from the mean, we assess it in terms of a z-score, which just represents the number of standard deviations a point is from the mean. Conversion to a z-score is done by subtracting the mean of the distribution from the data point and dividing by the standard deviation. In the height example, you can check that our friend would have a z-score of 2. If we do this to all the data points the new distribution is called the standard normal with a mean of 0 and a standard deviation of 1 as shown below.", "Every time we do a hypothesis test, we need to assume a distribution for the test statistic, which in our case is the average (mean) hours of sleep for our students. For a z-test, the normal curve is used as an approximation for the distribution of the test statistic. Generally, according to the ", ", as we take more averages from a data distribution, the averages will tend towards a normal distribution. However, this will always be an estimate because real-world data never perfectly follows a normal distribution. Assuming a normal distribution lets us determine how meaningful the result we observe in a study is. The higher or lower the z-score, the more unlikely the result is to happen by chance and the more likely the result is meaningful. To quantify just how meaningful the results are, we use one more concept.", "The final core idea is that of p-values. A ", ". That might seem a little convoluted, so let\u2019s look at an example.", "Say we are measuring average IQ in the US states of Florida and Washington. Our null hypothesis is that average IQs in Washington are not higher than average IQs in Florida. We perform the study and find IQs in Washington are higher by 2.2 points with a p-value of 0.346. This means, in a world where the null hypothesis \u2014 average IQs in Washington are not higher than average IQs in Florida \u2014 is true, there is a 34.6% chance we would measure IQs at least 2.2 points higher in Washington. So, if IQs in Washington ", " actually higher, we would still measure they ", " higher by at least 2.2 points about 1/3 of the time due to random noise. Subsequently, the lower the p-value, the more meaningful the result because it is less likely to be caused by noise.", "Whether or not the result can be called statistically significant depends on the p-value (known as alpha) we establish for significance ", "we begin the experiment . If the observed p-value is less than alpha, then the results are statistically significant. We need to choose alpha before the experiment because if we waited until after, we could just select a number that proves our results are significant no matter what the data shows!", "The choice of alpha depends on the situation and the field of study, but the most commonly used value is 0.05, corresponding to a 5% chance the results occurred at random. In my lab, I see values from 0.1 to 0.001 commonly in use. As an extreme example, the ", ", or a 1 in 3.5 million chance the discovery occurred because of noise. (Statisticians are loathe to admit that a p-value of 0.05 is arbitrary. ", "for indeterminate reasons and it stuck)!", "To get from a z-score on the normal distribution to a p-value, we can use a table or statistical software like R. The result will show us the probability of a z-score lower than the calculated value. For example, with a z-score of 2, the p-value is 0.977, which means there is only a 2.3% probability we observe a z-score higher than 2 at random.", "As a summary so far, we have covered three ideas:", "Now, let\u2019s put the pieces together in our example. Here are the basics:", "First, we need to convert our measurement into a z-score, or the number of standard deviations it is away from the mean. We do this by subtracting the population mean (the national average) from our measured value and dividing by the standard deviation over the square root of the number of samples. (", ". We account for this by dividing the standard deviation by the square root of the number of samples.)", "The z-score is called our ", ". Once we have a test-statistic, we can use a table or a programming language such as R to calculate the p-value. I use code here not to intimidate but to show how easy it is to implement our solution with free tools! (# are comments and ", "is output)", "Based on the p-value of 0.02116, we can reject the null hypothesis. (", ") There is ", "evidence our students get less sleep on average than college students in the US at a significance level of 0.05. The p-value shows there is a 2.12% chance that our results occurred because of random noise. In this battle of the presidents, the student was right.", "Before we ban all homework, we need to be careful not to assign too much to this result. Notice that our p-value, 0.02116, would not be significant if we had used a threshold of 0.01. Someone who wants to prove the opposite point in our study can simply manipulate the p-value. Anytime we examine a study, we should think about the p-value and the sample size in addition to the conclusion. With a relatively small sample size of 202, our study might have ", ". Further, this was an observational study, which means there is only evidence for ", "We showed there is a ", " between students at our school and less average sleep, but not that going to our school ", " a decrease in sleep. There could be other factors at play that affect sleep and only a ", "As with most technical concepts, statistical significance is not that complex and is just a combination of many small ideas. Most of the trouble comes with learning the vocabulary! Once you put the pieces together, you can start applying these statistical concepts. As you learn the basics of stats, you become better prepared to view studies and the news with a healthy skepticism. You can see what the data actually says rather than what someone tells you it means. The best tactic against dishonest politicians and corporations is a skeptical, well-educated public!", "As always, I welcome constructive criticism and feedback. I can be reached on Twitter ", ".", "Written by"], "postingTime": "2018-02-05T19:41:21.230Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "Exploratory\u2019s Weekly Update \u2014 1/29/2018 - learn data science", "content": ["Here\u2019s the latest ", "\u2019s weekly update. A list of things we thought interesting and wanted to share with you.", "It\u2019s been only 29 days since 2018 started, but I feel like there has been a lot happened already. The biggest thing so far is, of course, the release of Exploratory v4.2. If you haven\u2019t heard yet, here\u2019s ", ".", "The next big thing is our first online ", " we provided two weeks ago. It was a lot of fun and amazing experience to have met a such wonderful group of people who were from many different parts of the world but all shared their passion towards data!", "If you are interested in learning Data Science with R without programming or taking more values out of Exploratory, we have scheduled our next live online ", " in March. The early bird discount will end in a few days, so ", " today!", "Now, let\u2019s start this week\u2019s updates!", " \u2014 ", "Most of the SaaS products end up using Machine Learning / AI for reducing costs by automating existing work flows, rather than increasing revenues, which would require not only AI as a smart brain but also human as a creative brain.", " \u2014 ", "This is one of the most asked questions at our data science trainings. Still today, many people including data scientists are trying to answer this question in many different ways. A data scientist hero David Robinson at Stack Overflow is clarifying this by separating them based on the motivation for each category rather than talking about the technical differences.", " \u2014 ", "Being a heavy Slack user, I can only imagine they have a lot of business communication data to sit on and there are a lot you can do with it if you are data scientists. The question is when they will become evil? ;)", " \u2014 ", "Ah ok. Beginning of the end of Chat Bot hype.", "Exploratory v4.2 Released! \u2014 ", "If you are Exploratory user and haven\u2019t seen yet, I\u2019d strongly recommend you give it a shot. There are a lot of useful new features like Dashboard, Logistic Regression and PCA with Analytics View, Pie chart, Bubble chart, Show Details on Chart, Reference Line, Mutate for Multiple Columns, just to name a few.", " \u2014 ", "The data about projected country and regional changes in grain crop yields such as wheat, rice, coarse grains (barley and maize), and protein feed (soybean), due to global climate change is provided by NASA. (Introduced by ", ".)", " \u2014 ", "Some fans has built API about the movies from Studio Ghibli, known as a producer for movies like Princess Mononoke, Spirited Away. The source code for the API is published at this Github repository and ", " can be directly downloaded as JSON file, which can be imported into Exploratory Desktop. You should try it! (Introduced by \u2018", "\u2019.)", "We have started working on the development of Exploratory\u2019s next release v4.3. The main theme is Statistics and we will be making this part of the product more robust and easier to use. And of course, we\u2019ll be enhancing other areas, too. Let us know if you have any feedback or suggestion, now is the good time!", "As mentioned at the beginning, we have scheduled ", " to get your data analysis skills to the next level by exposing you to the powerful Data Science methods ranging from Machine Learning, Statistics, Data Visualization, and Data Wrangling. Visit our ", " for more details.", "That\u2019s it for this week.", "Have a wonderful week!", "Kan, CEO/Exploratory", "This is a weekly update of what I have seen in Data Science / AI and thought were interesting, plus what Team Exploratory is working on. If you like to receive this by email, subscribe from ", "!", "Written by"], "postingTime": "2018-02-01T02:14:09.434Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Saishruthi Swaminathan", "articleTile": "Linear Regression \u2014 Detailed View - Towards Data Science", "content": ["Linear regression is used for finding linear relationship between target and one or more predictors. There are two types of linear regression- Simple and Multiple.", "Simple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. For example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. Statistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight.", "The core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line.", "(Full code \u2014 ", ")", "We have a dataset which contains information about relationship between \u2018number of hours studied\u2019 and \u2018marks obtained\u2019. Many students have been observed and their hours of study and grade are recorded. This will be our training data. Goal is to design a model that can predict marks if given the number of hours studied. Using the training data, a regression line is obtained which will give minimum error. This linear equation is then used for any new data. That is, if we give number of hours studied by a student as an input, our model should predict their mark with minimum error.", "Y(pred) = b0 + b1*x", "The values b0 and b1 must be chosen so that they minimize the error. If sum of squared error is taken as a metric to evaluate the model, then goal to obtain a line that best reduces the error.", "If we don\u2019t square the error, then positive and negative point will cancel out each other.", "For model with one predictor,", "Apart from above equation co-efficient of the model can also be calculated from normal equation.", "Theta contains co-efficient of all predictors including constant term \u2018b0\u2019. Normal equation performs computation by taking inverse of input matrix. Complexity of the computation will increase as the number of features increase. It gets very slow when number of features grow large.", "Below is the python implementation of the equation.", "Complexity of the normal equation makes it difficult to use, this is where gradient descent method comes into picture. Partial derivative of the cost function with respect to the parameter can give optimal co-efficient value.", "(Complete details of gradient descent is in ", ")", "Python code for gradient descent", "Randomness and unpredictability are the two main components of a regression model.", "Prediction = Deterministic + Statistic", "Deterministic part is covered by the predictor variable in the model. Stochastic part reveals the fact that the expected and observed value is unpredictable. There will always be some information that are missed to cover. This information can be obtained from the residual information.", "Let\u2019s explain the concept of residue through an example. Consider, we have a dataset which predicts sales of juice when given a temperature of place. Value predicted from regression equation will always have some difference with the actual value. Sales will not match exactly with the true output value. This difference is called as residue.", "Residual plot helps in analyzing the model using the values of residues. It is plotted between predicted values and residue. Their values are standardized. The distance of the point from 0 specifies how bad the prediction was for that value. If the value is positive, then the prediction is low. If the value is negative, then the prediction is high. 0 value indicates prefect prediction. Detecting residual pattern can improve the model.", "Non-random pattern of the residual plot indicates that the model is,", "Characteristics of a residue", "Residual implementation and plot", ") &(", "This value ranges from 0 to 1. Value \u20181\u2019 indicates predictor perfectly accounts for all the variation in Y. Value \u20180\u2019 indicates that predictor \u2018x\u2019 accounts for no variation in \u2018y\u2019.", "1. Regression sum of squares (SSR)", "This gives information about how far estimated regression line is from the horizontal \u2018no relationship\u2019 line (average of actual output).", "2. Sum of Squared error (SSE)", "How much the target value varies around the regression line (predicted value).", "3. Total sum of squares (SSTO)", "This tells how much the data point move around the mean.", "Python implementation", "Value of R2 may end up being negative if the regression line is made to pass through a point forcefully. This will lead to forcefully making regression line to pass through the origin (no intercept) giving an error higher than the error produced by the horizontal line. This will happen if the data is far away from the origin.", "(For mode details \u2014 ", ")", "This is related to value of \u2018r-squared\u2019 which can be observed from the notation itself. It ranges from -1 to 1.", "r = (+/-) sqrt(r\u00b2)", "If the value of b1 is negative, then \u2018r\u2019 is negative whereas if the value of \u2018b1\u2019 is positive then, \u2018r\u2019 is positive. It is unitless.", "Null hypothesis is the initial claim that researcher specify using previous research or knowledge.", "Low P-value: Rejects null hypothesis indicating that the predictor value is related to the response", "High P-value: Changes in predictor are not associated with change in target", "Obtained Regression line", "Full code: https://github.com/SSaishruthi/Linear_Regression_Detailed_Implementation", "This is an educational post made by complilation of materials (like Prof.Andrew Ng course, Siraj Raval\u2019s videos, etc. ) that helped me in my journey. Other references are stated near the content.", "\u2014 \u2014 To be continued", "Written by"], "postingTime": "2019-01-18T19:51:04.755Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "\u9ec3\u529f\u8a73 Steeve Huang", "articleTile": "Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)", "content": ["Reinforcement Learning (RL) refers to a kind of Machine Learning method in which the agent receives a delayed reward in the next time step to evaluate its previous action. It was mostly used in games (e.g. Atari, Mario), with performance on par with or even exceeding humans. Recently, as the algorithm evolves with the combination of Neural Networks, it is capable of solving more complex tasks, such as the pendulum problem:", "Although there are a great number of RL algorithms, there does not seem to be a comprehensive comparison between each of them. It gave me a hard time when deciding which algorithms to be applied to a specific task. This article aims to solve this problem by briefly discussing the RL setup, and providing an introduction for some of the well-known algorithms.", "Typically, a RL setup is composed of two components, an agent and an environment.", "Then environment refers to the object that the agent is acting on (e.g. the game itself in the Atari game), while the agent represents the RL algorithm. The environment starts by sending a state to the agent, which then based on its knowledge to take an action in response to that state. After that, the environment send a pair of next state and reward back to the agent. The agent will update its knowledge with the reward returned by the environment to evaluate its last action. The loop keeps going on until the environment sends a terminal state, which ends to episode.", "Most of the RL algorithms follow this pattern. In the following paragraphs, I will briefly talk about some terms used in RL to facilitate our discussion in the next section.", "The model stands for the simulation of the dynamics of the environment. That is, the model learns the transition probability ", " from the pair of current state s", "and action a to the next state s", ". If the transition probability is successfully learned, the agent will know how likely to enter a specific state given current state and action. However, model-based algorithms become impractical as the state space and action space grows (S * S * A, for a tabular setup).", "On the other hand, model-free algorithms rely on trial-and-error to update its knowledge. As a result, it does not require space to store all the combination of states and actions. All the algorithms discussed in the next section fall into this category.", "An on-policy agent learns the value based on its current action a derived from the current policy, whereas its off-policy counter part learns it based on the action a* obtained from another policy. In Q-learning, such policy is the greedy policy. (We will talk more on that in Q-learning and SARSA)", "Q-Learning is an off-policy, model-free RL algorithm based on the well-known Bellman Equation:", "E in the above equation refers to the expectation, while \u019b refers to the discount factor. We can re-write it in the form of Q-value:", "The optimal Q-value, denoted as Q* can be expressed as:", "The goal is to maximize the Q-value. Before diving into the method to optimize Q-value, I would like to discuss two value update methods that are closely related to Q-learning.", "Policy iteration runs an loop between policy evaluation and policy improvement.", "Policy evaluation estimates the value function V with the greedy policy obtained from the last policy improvement. Policy improvement, on the other hand, updates the policy with the action that maximizes V for each of the state. The update equations are based on Bellman Equation. It keeps iterating till convergence.", "Value Iteration only contains one component. It updates the value function V based on the Optimal Bellman Equation.", "After the iteration converges, the optimal policy is straight-forwardly derived by applying an argument-max function for all of the states.", "Note that these two methods require the knowledge of the transition probability ", ", indicating that it is a model-based algorithm. However, as I mentioned earlier, model-based algorithm suffers from scalability problem. So how does Q-learning solves this problem?", "\u03b1 refers to the learning rate (i.e. how fast are we approaching the goal). The idea behind Q-learning is highly relied on value iteration. However, the update equation is replaced with the above formula. As a result, we do not need to worry about the transition probability anymore.", "Note that the next action ", " is chosen to maximize the next state\u2019s Q-value instead of following the current policy. As a result, Q-learning belongs to the off-policy category.", "SARSA very much resembles Q-learning. The key difference between SARSA and Q-learning is that SARSA is an on-policy algorithm. It implies that SARSA learns the Q-value based on the action performed by the current policy instead of the greedy policy.", "The action a_(t+1) is the action performed in the next state s_(t+1) under current policy.", "From the pseudo code above you may notice two action selection are performed, which always follows the current policy. By contrast, Q-learning has no constraint over the next action, as long as it maximizes the Q-value for the next state. Therefore, SARSA is an on-policy algorithm.", "Although Q-learning is a very powerful algorithm, its main weakness is lack of generality. If you view Q-learning as updating numbers in a two-dimensional array (Action Space * State Space), it, in fact, resembles dynamic programming. This indicates that for states that the Q-learning agent has not seen before, it has no clue which action to take. In other words, Q-learning agent does not have the ability to estimate value for unseen states. To deal with this problem, DQN get rid of the two-dimensional array by introducing Neural Network.", "DQN leverages a Neural Network to estimate the Q-value function. The input for the network is the current, while the output is the corresponding Q-value for each of the action.", "In 2013, DeepMind applied DQN to ", ", as illustrated in the above figure. The input is the raw image of the current game situation. It went through several layers including convolutional layer as well as fully connected layer. The output is the Q-value for each of the actions that the agent can take.", "The question boils down to: ", "The answer is that we train the network based on the Q-learning update equation. Recall that the target Q-value for Q-learning is:", "The \u03d5 is equivalent to the state s, while the \ud835\udf3d stands for the parameters in the Neural Network, which is not in the domain of our discussion. Thus, the loss function for the network is defined as the Squared Error between target Q-value and the Q-value output from the network.", "Another two techniques are also essential for training DQN:", "Although DQN achieved huge success in higher dimensional problem, such as the Atari game, the action space is still discrete. However, many tasks of interest, especially physical control tasks, the action space is continuous. If you discretize the action space too finely, you wind up having an action space that is too large. For instance, assume the degree of free random system is 10. For each of the degree, you divide the space into 4 parts. You wind up having 4\u00b9\u2070 =1048576 actions. It is also extremely hard to converge for such a large action space.", "DDPG relies on the actor-critic architecture with two eponymous elements, actor and critic. An actor is used to tune the parameter \ud835\udf3d for the policy function, i.e. decide the best action for a specific state.", "A critic is used for evaluating the policy function estimated by the actor according to the temporal difference (TD) error.", "Here, the lower-case ", " denotes the policy that the actor has decided. Does it look familiar? Yes! It looks just like the Q-learning update equation! TD learning is a way to learn how to predict a value depending on future values of a given state. Q-learning is a specific type of TD learning for learning Q-value.", "DDPG also borrows the ideas of ", " and ", "from DQN ", "Another issue for DDPG is that it seldom performs exploration for actions. A solution for this is adding noise on the parameter space or the action space.", "It is claimed that adding on parameter space is better than on action space, according to this ", " written by OpenAI. One commonly used noise is ", ".", "I have discussed some basic concepts of Q-learning, SARSA, DQN , and DDPG. In the next article, I will continue to discuss other state-of-the-art Reinforcement Learning algorithms, including NAF, A3C\u2026 etc. In the end, I will briefly compare each of the algorithms that I have discussed. Should you have any problem or question regarding to this article, please do not hesitate to leave a comment below or follow me on ", ".", "Written by"], "postingTime": "2018-09-16T16:14:55.881Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Will Koehrsen", "articleTile": "Time Series Analysis in Python: An Introduction - Towards Data Science", "content": ["Time series are one of the most common data types encountered in daily life. Financial prices, weather, home energy usage, and even weight are all examples of data that can be collected at regular intervals. Almost every data scientist will encounter time series in their daily work and learning how to model them is an important skill in the data science toolbox. One powerful yet simple method for analyzing and predicting periodic data is the ", ". The idea is straightforward: represent a time-series as a combination of patterns at different scales such as daily, weekly, seasonally, and yearly, along with an overall trend. Your energy use might rise in the summer and decrease in the winter, but have an overall decreasing trend as you increase the energy efficiency of your home. An additive model can show us both patterns/trends and make predictions based on these observations.", "The following image shows an additive model decomposition of a time-series into an overall trend, yearly trend, and weekly trend.", "This post will walk through an introductory example of creating an additive model for financial time-series data using Python and the ", ". Along the way, we will cover some data manipulation using pandas, accessing ", ", and ", ". I have included code where it is instructive, and I encourage anyone to check out the ", " on GitHub for the full analysis. This introduction will show you all the steps needed to start modeling time-series on your own!", "Disclaimer: Now comes the boring part when I have to mention that when it comes to financial data, ", " and you cannot use the methods here to get rich. I chose to use stock data because it is easily available on a daily frequency and fun to play around with. If you really want to become wealthy, learning data science is a better choice than playing the stock market!", "Usually, about 80% of the time spent on a data science project is getting and cleaning data. Thanks to the quandl financial library, that was reduced to about 5% for this project. Quandl can be installed with pip from the command line, lets you access thousands of financial indicators with a single line of Python, and allows up to 50 requests a day without signing up. If you sign up for a free account, you get an api key that allows unlimited requests.", "First, we import the required libraries and get some data. Quandl automatically puts our data into a pandas dataframe, the data structure of choice for data science. (For other companies, just replace the \u2018TSLA\u2019 or \u2018GM\u2019 with the stock ticker. You can also specify a date range).", "There is an almost unlimited amount of data on quandl, but I wanted to focus on comparing two companies within the same industry, namely Tesla and General Motors. Tesla is a fascinating company not only because it is the first ", ", but also because at times in ", " despite only selling 4 different cars. The other contender for the title of most valuable car company is General Motors which recently has shown signs of embracing the future of cars by building some pretty cool (but not cool-looking) all-electric vehicles.", "We could easily have spent hours searching for this data and downloading it as csv spreadsheet files, but instead, thanks to quandl, we have all the data we need in a few seconds!", "Before we can jump into modeling, it\u2019s best to get an idea of the structure and ranges by making a few exploratory plots. This will also allows us to look for outliers or missing values that need to be corrected.", "Pandas dataframes can be easily plotted with matplotlib. If any of the graphing code looks intimidating, don\u2019t worry. I also find matplotlib to be unintuitive and often copy and paste examples from Stack Overflow or documentation to get the graph I want. One of the rules of programming is don\u2019t reinvent a solution that already exists!", "Comparing the two companies on stock prices alone does not show which is more valuable because the total value of a company (market capitalization) also depends on the number of shares (Market cap= share price * number of shares). Quandl does not have number of shares data, but I was able to find average yearly stock shares for both companies with a quick Google search. Is is not exact, but will be accurate enough for our analysis. Sometimes we have to make do with imperfect data!", "To create a column of market cap in our dataframe,we use a few tricks with pandas, such as moving the index to a column (reset_index) and simultaneously indexing and altering values in the dataframe ", ".", "This creates a \u2018cap\u2019 column for Tesla. We do the same process with the GM data and then merge the two. ", " is an essential part of a data science workflow because it allows us to join datasets on a shared column. In this case, we have stock prices for two different companies on the same dates and we therefore want to join the data on the date column. We perform an \u2018inner\u2019 merge to save only Date entries that are present in both dataframes. After merging, we rename the columns so we know which one goes with which car company.", "The market cap is in billions of dollars. We can see General Motors started off our period of analysis with a market cap about 30 times that of Tesla! Do things stay that way over the entire timeline?", "We observe a meteoric rise for Tesla and a minor increase for General Motors over the course of the data. Tesla even surpasses GM in value during 2017!", "During that period, Tesla sold about ", "while ", ". GM was valued less than Tesla during a period in which it sold 30 times more cars! This definitely displays the power of a persuasive executive and a high-quality \u2014 if extremely low-quantity \u2014 product. Although the value of Tesla is now lower than GM, a good question might be, can we expect Tesla to again surpass GM? When will this happen? For that we turn to additive models for forecasting, or in other words, predicting the future.", "was released in 2017 for Python and R, and data scientists around the world rejoiced. Prophet is designed for analyzing time series with daily observations that display patterns on different time scales. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints, but we will stick to the basic functions to get a model up and running. Prophet, like quandl, can be installed with pip from the command line.", "We first import prophet and rename the columns in our data to the correct format. The Date column must be called \u2018ds\u2019 and the value column we want to predict \u2018y\u2019. We then create prophet models and fit them to the data, much like a Scikit-Learn machine learning model:", "When creating the prophet models, I set the changepoint prior to 0.15, up from the default value of 0.05. This hyperparameter is used to control ", ", with a higher value being more sensitive and a lower value less sensitive. This value is used to combat one of the most fundamental trade-offs in machine learning: ", ".", "If we fit too closely to our training data, called", ", we have too much variance and our model will not be able to generalize well to new data. On the other hand, if our model does not capture the trends in our training data it is underfitting and has too much bias. When a model is underfitting, increasing the changepoint prior allows more flexibility for the model to fit the data, and if the model is overfitting, decreasing the prior limits the amount of flexibility. The effect of the changepoint prior scale can be illustrated by graphing predictions made with a range of values:", "The higher the changepoint prior scale, the more flexible the model and the closer it fits to the training data. This may seem like exactly what we want, but learning the training data too well can lead to overfitting and an inability to accurately make predictions on new data. We therefore need to find the right balance of fitting the training data and being able to generalize to new data. As stocks vary from day-to-day, and we want our model to capture this, I increased the flexibility after experimenting with a range of values.", "In the call to create a prophet model, we can also specify changepoints, which occur when a time-series goes from increasing to decreasing, or from increasing slowly to increasing rapidly (they are located ", "). Changepoints can correspond to significant events such as product launches or macroeconomic swings in the market. If we do not specify changepoints, prophet will calculate them for us.", "To make forecasts, we need to create what is called a future dataframe. We specify the number of future periods to predict (two years) and the frequency of predictions (daily). We then make predictions with the prophet model we created and the future dataframe:", "Our future dataframes contain the estimated market cap of Tesla and GM for the next two years. We can visualize predictions with the prophet plot function.", "The black dots represent the actual values (notice how they stop at the beginning of 2018), the blue line indicates the forecasted values, and the light blue shaded region is the uncertainty (", "). The region of uncertainty increases the further out in the future the prediction is made because initial uncertainty propagates and grows over time. This is observed in ", ".", "We can also inspect changepoints identified by the model. Again, changepoints represent when the time series growth rate significantly changes (goes from increasing to decreasing for example).", "For comparison, we can look at the ", " over this time range to see if the changes line up. We plot the changepoints (vertical lines) and search trends on the same graph:", "Some of the changepoints in the market value of Tesla align with changes in frequency of Tesla searches, but not all of them. From this, I would say that relative Google search frequency is not a great indicator of stock changes.", "We still need to figure out when the market capitalization of Tesla will surpass that of General Motors. Since we have both predictions for the next two years we can plot both companies on the same graph after merging the dataframes. Before merging, we rename the columns to keep track of the data.", "First we will plot just the estimate. The estimate (called \u2018yhat\u2019 in the prophet package) smooths out some of the noise in the data so it looks a little different than the raw plots. The level of smoothness will depend on the changepoint prior scale \u2014 higher priors mean a more flexible model and more ups and downs.", "Our model thinks the brief surpassing of GM by Tesla in 2017 was just noise, and it is not until early 2018 that Tesla beats out GM for good in the forecast. The exact date is January 27, 2018, so if that happens, I will gladly take credit for predicting the future!", "When making the above graph, we left out the most important part of a forecast: the uncertainty! We can use matplotlib (see ", ") to show the regions of doubt:", "This is a better representation of the prediction. It shows the value of both companies is expected to increase, but Tesla will increase more rapidly than General Motors. Again, the uncertainty increases over time as expected for a prediction and the lower bound of Tesla is below the upper bound of GM in 2020 meaning GM might retain the lead.", "The last step of the market capitalization analysis is looking at the overall trend and patterns. Prophet allows us to easily visualize the overall trend and the component patterns:", "The trend is pretty clear: GM stock is rising and going to keep rising. The yearly pattern is interesting because it seems to suggest GM increases in value at the end of the year with a long slow decline into the summer. We can try to determine if there is a correlation between the yearly market cap and the average monthly sales of GM over the time period. I first gathered the monthly vehicle sales from Google and then averaged over the months using groupby. This is another critical data science operation, because often we want to compare stats between categories, such as users of a specific age group, or vehicles from one manufacturer. In this case, we want to calculate average sales in each month, so we group the months together and then average the sales.", "It does not look like monthly sales are correlated with the market cap. The monthly sales are second highest in August, which is right at the lowest point for the market cap!", "Looking at the weekly trend, there does not appear to be any meaningful signal (there are no stock prices recorded on the weekends so we look at the change during the week).This is to be expected as the ", " states there is no predictable pattern in stock prices on a daily basis. As evidenced by our analysis, in the long run, stocks tend to increase, but on a day-to-day scale, there is almost no pattern that we can take advantage of even with the best models.", "A simple look at the ", " (a market index of the 30 largest companies on the stock exchange) nicely illustrates this point:", "Clearly, the message is to go back to 1900 and invest your money! Or in reality, when the market drops, don\u2019t withdraw because it will go back up according to history. On the overall scale, the day-to-day fluctuations are too small to even be seen and if we are thinking like data scientists, we realize that playing daily stocks is foolish compared to investing in the entire market and holding for long periods of time.", "Prophet can also be applied to larger-scale data measures, such as Gross Domestic Product, a measure of the overall size of a country\u2019s economy. I made the following forecast by creating prophet models based on the historical GDP of the US and China.", "The exact date China will surpass the US in GDP is 2036! This model is limited because of the low frequency of the observations (GDP is measured once per quarter but prophet works best with daily data), but it provides a basic prediction with no macroeconomic knowledge required.", "There are many ways to model time-series, from ", " to ", ". Additive Models are useful because they are quick to develop, fast to train, provide interpretable patterns, and make predictions with uncertainties. The capabilities of Prophet are impressive and we have only scratched the surface here. I encourage you to use this article and the notebook to explore some of the ", " or your own time series. Stay tuned for future work on time series analysis, and for an application of prophet to my daily life, see my post on ", " As a first step in exploring time-series, additive models in Python are the way to go!", "As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu.", "Written by"], "postingTime": "2018-01-19T02:39:00.590Z"}
{"nameOfPublication": "learn data science", "nameOfAuthor": "Kan Nishida", "articleTile": "Exploratory\u2019s Weekly Update \u2014 Vol.9 \u2014 Amazon AI Transformation, & more.", "content": ["Hi there!", "It\u2019s Kan from Exploratory. Hope this email finds you well.", "It\u2019s been super warm here in Mill Valley (just a bit north of San Francisco). There were a lot of people at the beach last weekend, and Cherry Blossom trees in our yard start blossoming! It\u2019s weird to think, but definitely the feeling of Spring is here now.", "Anyway, before going to this weeks\u2019 update, just one thing.", "For our ", " in this coming March, we have added a student discount plan (50% off) so if you happen to be a student or know students who would be interested in learning Data Science, please directly reply to me, I\u2019ll send you a follow up instruction to enroll.", "Now, let\u2019s start this week\u2019s updates!", "\u2014 ", "The AI transformation Amazon has made to itself can be a great case for companies outside of Silicon Valley to survive and thrive in this AI era. Just a few years ago, nobody thought Amazon could pull off something like Alexa and beat other tech titans like Google, Facebook, and Apple. Steve Levi at Wired has written a detail story behind the transformation.", "\u2014 ", "Glassdoor publishes the top 50 jobs in U.S. every year and they just announced this year\u2019s. And of course, Data Scientist is listed as \u21161 again. This makes it 3 years in a row. Their criteria for selecting the best jobs are salary, number of the job openings, and job satisfaction. It\u2019s not surprising to see Data Scientist as the best job in the world, but what makes it interesting this year is its separation from Data Engineer which is down to the 33rd from the 3rd last year. Big Data hype might have settled after the last 10 years run.", " \u2014 ", " \u2014 ", "Starting a Data Science project is easy, but making it a successful project is hard. And we have started hearing more and more Data Science projects failure stories. The main problem comes from Over-expectation and Under-expectation. If you are not familiar with Data Science, you would either expect some magics that would take care of your problems by themselves OR you don\u2019t even realize you have problems that could have been solved by Data Science methods relatively easily. Without having the right understanding of what Data Science can and can\u2019t do, hiring Data Scientists or starting Data Science projects would be up for failures at companies.", " \u2014 ", "I\u2019m a big fan of Ramen. That\u2019s one reason I go to Japan periodically other than teaching Data Science! :) Anyway, someone has reviewed more than 2600 instant cup noodles and created a database. You can download the data in excel format from ", ".", " \u2014 ", " at Harvard University has gathered the global trading data from United Nation\u2019s Comtrade database and transformed it to make it easier to analyze. The data file comes in STATA format, which is a common format among people in Social Science field. You can import it into Exploratory easily though.", "(Both introduced by \u2018", "\u2019).", "For the next release, v4.3, we are adding a support for Column Name Filter for Viz View, Analytic View, and all other Data Wrangling UIs. When you have a lot of columns to choose from, this will make it much less painful.", "As mentioned at the beginning, we have scheduled March Booster Training class, visit our ", " for more details. Also, we have added a student discount plan (50% off)! If you happen to be currently a student, directly reply to me, I\u2019ll send you a follow up instruction to enroll.", "That\u2019s it for this week.", "Have a wonderful week!", "Kan, CEO/Exploratory", "This is a weekly update of what I have seen in Data Science / AI and thought were interesting, plus what Team Exploratory is working on. If you like to receive this by email, subscribe from ", "!", "Written by"], "postingTime": "2018-02-06T14:35:04.579Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Will Koehrsen", "articleTile": "Markov Chain Monte Carlo in Python - Towards Data Science", "content": ["The past few months, I encountered one term again and again in the data science world: Markov Chain Monte Carlo. In my research lab, in podcasts, in articles, every time I heard the phrase I would nod and think that sounds pretty cool with only a vague idea of what anyone was talking about. Several times I tried to learn MCMC and Bayesian inference, but every time I started reading the books, I soon gave up. Exasperated, I turned to the best method to learn any new skill: apply it to a problem.", "Using some of my sleep data I had been meaning to explore and a hands-on application-based book (", ", ", "), I finally learned Markov Chain Monte Carlo through a real-world project. As usual, it was much easier (and more enjoyable) to understand the technical concepts when I applied them to a problem rather than reading them as abstract ideas on a page. This article walks through the introductory implementation of Markov Chain Monte Carlo in Python that finally taught me this powerful modeling and analysis tool.", "The ", " I encourage anyone to take a look and use it on their own data. This article focuses on applications and results, so there are a lot of topics covered at a high level, but I have tried to provide links for those wanting to learn more!", "My Garmin Vivosmart watch tracks when I fall asleep and wake up based on heart rate and motion. It\u2019s not 100% accurate, but real-world data is never perfect, and we can still extract useful knowledge from noisy data with the right model!", "The objective of this project was to use the sleep data to create a model that specifies the posterior probability of sleep as a function of time. As time is a continuous variable, specifying the entire posterior distribution is intractable, and we turn to methods to approximate a distribution, such as Markov Chain Monte Carlo (MCMC).", "Before we can start with MCMC, we need to determine an appropriate function for modeling the posterior probability distribution of sleep. One simple way to do this is to visually inspect the data. The observations for when I fall asleep as a function of time are shown below.", "Every data point is represented as a dot, with the intensity of the dot showing the number of observations at the specific time. My watch records only the minute at which I fall asleep, so to expand the data, I added points to every minute on both sides of the precise time. If my watch says I fell asleep at 10:05 PM, then every minute before is represented as a 0 (awake) and every minute after gets a 1 (asleep). This expanded the roughly 60 nights of observations into 11340 data points.", "We can see that I tend to fall asleep a little after 10:00 PM but we want to create a model that captures the transition from awake to asleep in terms of a probability. We could use a simple step function for our model that changes from awake (0) to asleep (1) at one precise time, but this would not represent the uncertainty in the data. I do not go to sleep at the same time every night, and we need a function to that models the transition as a gradual process to show the variability. The best choice given the data is a logistic function which is smoothly transitions between the bounds of 0 and 1. Following is a logistic equation for the probability of sleep as a function of time", "Here, \u03b2 (beta) and \u03b1 (alpha) are the parameters of the model that we must learn during MCMC. A logistic function with varying parameters is shown below.", "A logistic function fits the data because the probability of being asleep transitions gradually, capturing the variability in my sleep patterns. We want to be able to plug in a time t to the function and get out the probability of sleep, which must be between 0 and 1. Rather than a straight yes or no answer to the question am I asleep at 10:00 PM, we can get a ", "To create this model, we use the data to find the best alpha and beta parameters through one of the techniques classified as Markov Chain Monte Carlo.", " refers to a class of methods for sampling from a probability distribution in order to construct the ", "distribution. We cannot directly calculate the logistic distribution, so instead we generate thousands of values \u2014 called samples \u2014 for the parameters of the function (alpha and beta) to create an approximation of the distribution. The idea behind MCMC is that as we generate more samples, our approximation gets closer and closer to the actual true distribution.", "There are two parts to a Markov Chain Monte Carlo method. ", "refers to a general technique of using repeated random samples to obtain a numerical answer. Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response. By choosing random values, we can explore a large portion of the parameter space, the range of possible values for the variables. A parameter space for our problem using normal priors for the variables (more on this in a moment) is shown below.", "Clearly we cannot try every single point in these plots, but by randomly sampling from regions of higher probability (red) we can create the most likely model for our problem.", "A ", " is a process where the next state depends only on the current state. (A state in this context refers to the assignment of values to the parameters). A Markov Chain is memoryless because only the current state matters and not how it arrived in that state. If that\u2019s a little difficult to understand, consider an everyday phenomenon, the weather. If we want to predict the weather tomorrow we can get a reasonable estimate using only the weather today. If it snowed today, we look at historical data showing the distribution of weather on the day after it snows to estimate probabilities of the weather tomorrow. The concept of a Markov Chain is that we do not need to know the entire history of a process to predict the next output, an approximation that works well in many real-world situations.", "Putting together the ideas of Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters. MCMC can be considered as a random walk that gradually converges to the true distribution.", "In order to draw random values of alpha and beta, we need to assume a prior distribution for these values. As we have no assumptions about the parameters ahead of time, we can use a normal distribution. The normal, or Gaussian distribution, is defined by the mean, showing the location of the data, and the variance, showing the spread. Several normal distributions with different means and spreads are below:", "The specific MCMC algorithm we are using is called ", ". In order to connect our observed data to the model, every time a set of random values are drawn, the algorithm evaluates them against the data. If they do not agree with the data (I\u2019m simplifying a little here), the values are rejected and the model remains in the current state. If the random values are in agreement with the data, the values are assigned to the parameters and become the current state. This process continues for a specified number of steps, with the accuracy of the model improving with the number of steps.", "Putting it all together, the basic procedure for Markov Chain Monte Carlo in our problem is as follows:", "The algorithm returns all of the values it generates for alpha and beta. We can then use the average of these values as the most likely final values for alpha and beta in the logistic function. MCMC cannot return the \u201cTrue\u201d value but rather an approximation for the distribution. The final model for the probability of sleep given the data will be the logistic function with the average values of alpha and beta.", "The above details went over my head many times until I applied them in Python! Seeing the results first-hand is a lot more helpful than reading someone else describe. To implement MCMC in Python, we will use the ", " It abstracts away most of the details, allowing us to create models without getting lost in the theory.", "The following code creates the full model with the parameters, ", " and ", ", the probability, ", ", and the observations, ", " The ", " variable refers to the specific algorithm, and the ", " holds all of the values of the parameters generated by the model.", "(Check out the notebook for the full code)", "To get a sense of what occurs when we run this code, we can look at all the value of alpha and beta generated during the model run.", "These are called trace plots. We can see that each state is correlated to the previous \u2014 the Markov Chain \u2014 but the values oscillate significantly \u2014 the Monte Carlo sampling.", "In MCMC, it is common practice to discard up to 90% of the trace. The algorithm does not immediately converge to the true distribution and the initial values are often inaccurate. The later values for the parameters are generally better which means they are what we should use for building our model. We used 10000 samples and discarded the first 50%, but an industry application would likely use hundreds of thousands or millions of samples.", "MCMC converges to the true value given enough steps, but assessing convergence can be difficult. I will leave that topic out of this post (one way is by measuring the ", " of the traces) but it is an important consideration if we want the most accurate results. PyMC3 has built in functions for assessing the quality of models, including trace and autocorrelation plots.", "After finally building and running the model, it\u2019s time to use the results. We will the the average of the last 5000 alpha and beta samples as the most likely values for the parameters which allows us to create a single curve modeling the posterior sleep probability:", "The model represents the data well. Moreover, it captures the inherent variability in my sleep patterns. Rather than a single yes or no answer, the model gives us a probability. For example, we can query the model to find out the probability I am asleep at a given time and find the time at which the probability of being asleep passes 50%:", "Although I try to go to bed at 10:00 PM, that clearly does not happen most nights! We can see that the average time I go to bed is around 10:14 PM.", "These values are the ", "estimates given the data. However, there is uncertainty associated with these probabilities because the model is approximate. To represent this uncertainty, we can make predictions of the sleep probability at a given time using all of the alpha and beta samples instead of the average and then plot a histogram of the results.", "These results give a better indicator of what an MCMC model really does. The method does not find a single answer, but rather a sample of possible values. Bayesian Inference is useful in the real-world because it expresses predictions in terms of probabilities. We can say there is one most likely answer, but the more accurate response is that there are a range of values for any prediction.", "I can use the waking data to find a similar model for when I wake up in the morning. I try to always be up at 6:00 AM with my alarm, but we can see that does not always happen! The following image shows the final model for the transition from sleeping to waking along with the observations.", "We can query the model to find the probability I\u2019m asleep at a given time and the most likely time for me to wake up.", "Looks like I have some work to do with that alarm!", "A final model I wanted to create \u2014 both out of curiosity and for the practice \u2014 was my duration of sleep. First, we need to find a function to model the distribution of the data. Ahead of time, I think it would be normal, but we can only find out by examining the data!", "A normal distribution would work, but it would not capture the outlying points on the right side (times when I severely slept in). We could use two separate normal distributions to represent the two modes, but instead, I will use a skewed normal. The skewed normal has three parameters, the mean, the variance, and alpha, the skew. All three of these must be learned from the MCMC algorithm. The following code creates the model and implements the Metropolis Hastings sampling.", "Now, we can use the average values of the three parameters to construct the most likely distribution. Following is the final skewed normal distribution on top of the data.", "It looks like a nice fit! We can query the model to find the likelihood I get at least a certain amount of sleep and the most likely duration of sleep:", "I\u2019m not entirely pleased with those results, but what can you expect as a graduate student?", "Once again, completing this project showed me the ", "! Along the way to building an end-to-end implementation of Bayesian Inference using Markov Chain Monte Carlo, I picked up many of the fundamentals and enjoyed myself in the process. Not only did I learn a little bit about my habits (and what I need to improve), but now I can finally understand what everyone is talking about when they say MCMC and Bayesian Inference. Data science is about constantly adding tools to your repertoire and the most effective way to do that is to find a problem and get started!", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter ", ".", "Written by"], "postingTime": "2018-04-05T21:00:55.531Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Hussnain Fareed", "articleTile": "Setup a Python Environment for Machine Learning and Deep Learning", "content": ["Interest in ", "has exploded over the past decade. You see machine learning in computer science programs, industry conferences, and in many applications in daily life.", "I am assuming you already know about Machine Learning, therefore I will not be explaining What and Why.", "So, I find many beginners facing problems while installing libraries and setting up environment. As i have faced first time when i was trying. So this guide is totally for beginners .", "In this story I will tell you how you can easily setup a python environment on your system. I am using Windows but this guide is also suitable for Ubuntu & Linux users.", "After completing this tutorial, you will have a working Python environment to begin learning, and developing machine learning and deep learning software.", "Firs of all to perform machine learning and deep learning on any dataset, the software/program requires a computer system powerful enough to handle the computing power necessary. So the following is required:", "Note: In the case of laptops, the ideal option would be to purchase a gaming laptop from any vendor deemed suitable such as Alienware, ASUS, Lenovo Legion, Acer Predator etc.", "Let\u2019s just get straight to the installation process. we are gonna hit the rock \ud83d\ude09", "In this tutorial, we will cover the following steps:", "In this step, we will download the Anaconda Python package for your platform.", "Anaconda is a free and easy-to-use environment for scientific Python.", "I am using Windows you can choose according to your OS.", "In this step, we will install the Anaconda Python software on your system.", "Installation is very easy and quick once you download the setup. Open the setup and follow the wizard instructions.", " ", "It might take 5 to 10 minutes or some more time according to your system.", "Open Anaconda Prompt to type the following command(s). Don\u2019t worry Anaconda Prompt is just works same as cmd.", "Choose your version depending on your Operating System and GPU.", "Here is a guide to check that if your ", " your Nvidia Graphic Card", "For downloading other versions you can follow this link: ", " ", " People with version 9.0 ", " can also install the given patch in any case of error while proceeding.", "2. Download cuDNN ", "Download the latest version of cuDNN. Choose your version depending on your Operating System and CUDA. Membership registration is required. Don\u2019t worry you can easily create an account using your email.", "Put your unzipped folder in C drive as follows:", "Here we will create a new anaconda environment for our specific usage so that it will not affect the root of Anaconda. Amazing!! isn\u2019t it? \ud83d\ude1b", "Open Anaconda Prompt to type the following commands.", "2. Activate the conda environment by issuing the following command:", "In this step, we will install Python libraries used for deep learning, specifically: TensorFlow, and Keras.", "TensorFlow is a tool for machine learning. While it contains a wide range of functionality, TensorFlow is mainly designed for deep neural network models.", "=> For installing TensorFlow, Open Anaconda Prompt to type the following commands.", "To install the GPU version of TensorFlow:", "To install the CPU-only version of TensorFlow:", "If your machine or system is the only CPU supported you can install CPU version for basic learning and practice.", "=> You can test the installation by running this program on shell:", "For getting started and documentation you can visit ", " website.", "2. ", "Keras is a high-level neural networks API, written in Python and capable of running on top of ", ", ", ", or ", ".", "=> For installing Keras Open Anaconda Prompt to type the following commands.", "=> Let\u2019s try running ", " in your prompt. you can use other ", " as well.", "Open Anaconda Prompt to type the following commands.", "For getting started and documentation you can visit ", " website.", "Here is an implementation of ", " I have done.", "There are some other famous libraries like ", "you can use as per on your choice and use.", "Congratulations! \ud83d\ude09 You have successfully created the environment using TensorFlow, Keras (with Tensorflow backend) over GPU on Windows!", "Written by"], "postingTime": "2019-01-29T15:18:51.655Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Alvira Swalin", "articleTile": "How to Handle Missing Data - Towards Data Science", "content": ["One of the most common problems I have faced in Data Cleaning/Exploratory Analysis is handling the missing values. Firstly, understand that there is NO good way to deal with missing data. I have come across different solutions for data imputation depending on the kind of problem \u2014 Time series Analysis, ML, Regression etc. and it is difficult to provide a general solution. In this blog, I am attempting to summarize the most commonly used methods and trying to find a structural solution.", "Before jumping to the methods of data imputation, we have to understand the reason why data goes missing.", "In the first two cases, it is safe to remove the data with missing values depending upon their occurrences, while in the third case removing observations with missing values can produce a bias in the model. So we have to be really careful before removing observations. Note that imputation does not necessarily give better results.", "Computing the overall mean, median or mode is a very basic imputation method, it is the only tested function that takes no advantage of the time series characteristics or relationship between the variables. It is very fast, but has clear disadvantages. One disadvantage is that mean imputation reduces variance in the dataset.", "To begin, several predictors of the variable with missing values are identified using a correlation matrix. The best predictors are selected and used as independent variables in a regression equation. The variable with missing data is used as the dependent variable. Cases with complete data for the predictor variables are used to generate the regression equation; the equation is then used to predict missing values for incomplete cases. In an iterative process, values for the missing variable are inserted and then all cases are used to predict the dependent variable. These steps are repeated until there is little difference between the predicted values from one step to the next, that is they converge. ", "It \u201ctheoretically\u201d provides good estimates for missing values. However, there are several disadvantages of this model which tend to outweigh the advantages. First, because the replaced values were predicted from other variables they tend to fit together \u201ctoo well\u201d and so standard error is deflated. One must also assume that there is a linear relationship between the variables used in the regression equation when there may not be one.", "This is by far the most preferred method for imputation for the following reasons:", "- Easy to use", "- No biases (if imputation model is correct)", "There are other machine learning techniques like XGBoost and Random Forest for data imputation but we will be discussing KNN as it is widely used. In this method, k neighbors are chosen based on some distance measure and their average is used as an imputation estimate. The method requires the selection of the number of nearest neighbors, and a distance metric. KNN can predict both discrete attributes (the most frequent value among the k nearest neighbors) and continuous attributes (the mean among the k nearest neighbors)", "The distance metric varies according to the type of data:", "1. Continuous Data: The commonly used distance metrics for continuous data are Euclidean, Manhattan and Cosine", "2. Categorical Data: Hamming distance is generally used in this case. It takes all the categorical attributes and for each, count one if the value is not the same between two points. The Hamming distance is then equal to the number of attributes for which the value was different.", "One of the most attractive features of the KNN algorithm is that it is simple to understand and easy to implement. The non-parametric nature of KNN gives it an edge in certain settings where the data may be highly \u201cunusual\u201d.", "One of the obvious drawbacks of the KNN algorithm is that it becomes time-consuming when analyzing large datasets because it searches for similar instances through the entire dataset. Furthermore, the accuracy of KNN can be severely degraded with high-dimensional data because there is little difference between the nearest and farthest neighbor.", "Among all the methods discussed above, multiple imputation and KNN are widely used, and multiple imputation being simpler is generally preferred.", "If you have any questions about this post, please ask in the comments and I will do my best to answer.", "Written by"], "postingTime": "2018-03-19T22:21:13.372Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Nimesh Sinha", "articleTile": "Understanding LSTM and its quick implementation in keras for sentiment analysis.", "content": ["Long Short Term Memory networks, usually called \u201cLSTMs\u201d , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN.", "Being human, when we watch a movie, we don\u2019t think from scratch every time while understanding any event. We rely on the recent experiences happening in the movie and learn from them. But, a conventional neural network is unable to learn from the previous events because the information does not pass from one step to the next. On contrary, RNN learns information from immediate previous step.", "For example, there is a scene in a movie where a person is in a basketball court. We will improvise the basketball activities in the future frames: an image of someone running and jumping probably be labeled as ", ", and an image of someone sitting and watching is probably ", "A typical RNN looks like above-where X(t) is input, h(t) is output and A is the neural network which gains information from the previous step in a loop. The output of one unit goes into the next one and the information is passed.", "But, sometimes we don\u2019t need our network to learn only from immediate past information. Suppose we want to predict the blank word in the text \u2018 David, a 36-year old man lives in San Francisco. He has a female friend Maria. Maria works as a cook in a famous restaurant in New York whom he met recently in a school alumni meet. Maria told him that she always had a passion for _________ . Here, we want our network to learn from dependency \u2018cook\u2019 to predict \u2018cooking. There is a gap between the information what we want to predict and from where we want it to get predicted . This is called long-term dependency. We can say that anything larger than trigram as a long term dependency. Unfortunately, RNN does not work practically in this situation.", "During the training of RNN, as the information goes in loop again and again which results in very large updates to neural network model weights. This is due to the accumulation of error gradients during an update and hence, results in an unstable network. At an extreme, the values of weights can become so large as to overflow and result in NaN values.The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than 1 or vanishing occurs if the values are less than 1.", "The above drawback of RNN pushed the scientists to develop and invent a new variant of the RNN model, called Long Short Term Memory. LSTM can solve this problem, because it uses gates to control the memorizing process.", "Let\u2019s understand the architecture of LSTM and compare it with that of RNN:", "The symbols used here have following meaning:", "a) X : Scaling of information", "b)+ : Adding information", "c) \u03c3 : Sigmoid layer", "d) tanh: tanh layer", "e) h(t-1) : Output of last LSTM unit", "f) c(t-1) : Memory from last LSTM unit", "g) X(t) : Current input", "h) c(t) : New updated memory", "i) h(t) : Current output", "To overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero. ", " is a suitable function with the above property.", "As Sigmoid can output 0 or 1, it can be used to forget or remember the information.", "Information passes through many such LSTM units.There are three main components of an LSTM unit which are labeled in the diagram:", "We just saw that there is a big difference in the architecture of a typical RNN and a LSTM. In LSTM, our model learns what information to store in long term memory and what to get rid of.", "Here, I used LSTM on the reviews data from ", " for sentiment analysis using keras.", "This is what my data looks like.", "I used Tokenizer to vectorize the text and convert it into sequence of integers after restricting the tokenizer to use only top most common 2500 words. I used pad_sequences to convert the sequences into 2-D numpy array.", "Then, I built my LSTM network.There are a few hyper parameters:", "The other hyper parameters like dropout, batch_size are similar to that of CNN.", "I used softmax as activation function.", "Now, I fit my model on training set and check the accuracy on validation set.", "I got a validation accuracy of 86% in just one epoch while running on a small dataset which includes all the businesses.", "LSTM outperforms the other models when we want our model to learn from long term dependencies. LSTM\u2019s ability to forget, remember and update the information pushes it one step ahead of RNNs.", "4. ", "5. ", "Written by"], "postingTime": "2018-03-03T04:41:05.571Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Aditya Mishra", "articleTile": "Metrics to Evaluate your Machine Learning Algorithm", "content": ["Evaluating your machine learning algorithm is an essential part of any project. Your model may give you satisfying results when evaluated using a metric ", "but may give poor results when evaluated against other metrics such as ", " or any other such metric. Most of the times we use classification accuracy to measure the performance of our model, however it is not enough to truly judge our model. In this post, we will cover different types of evaluation metrics available.", "Classification Accuracy", "Logarithmic Loss", "Confusion Matrix", "Area under Curve", "F1 Score", "Mean Absolute Error", "Mean Squared Error", "Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.", "It works well only if there are equal number of samples belonging to each class.", "For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get ", " by simply predicting every training sample belonging to class A.", "When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the ", "Classification Accuracy is great, but gives us the false sense of achieving high accuracy.", "The real problem arises, when the cost of misclassification of the minor class samples are very high. If we deal with a rare but fatal disease, the cost of failing to diagnose the disease of a sick person is much higher than the cost of sending a healthy person to more tests.", "Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. Suppose, there are N samples belonging to M classes, then the Log Loss is calculated as below :", "where,", "y_ij, indicates whether sample i belongs to class j or not", "p_ij, indicates the probability of sample i belonging to class j", "Log Loss has no upper bound and it exists on the range [0, \u221e). Log Loss nearer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy.", "In general, minimising Log Loss gives greater accuracy for the classifier.", "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.", "Lets assume we have a binary classification problem. We have some samples belonging to two classes : YES or NO. Also, we have our own classifier which predicts a class for a given input sample. On testing our model on 165 samples ,we get the following result.", "There are 4 important terms :", "Accuracy for the matrix can be calculated by taking average of the values lying across the", "i.e", "Confusion Matrix forms the basis for the other types of metrics.", "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :", "False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR bot hare computed at threshold values such as (0.00, 0.02, 0.04, \u2026., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1].", "As evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.", "F1 Score is used to measure a test\u2019s accuracy", "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).", "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :", "F1 Score tries to find the balance between precision and recall.", "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don\u2019t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :", "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the ", "of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.", "That\u2019s it.", "Thanks for reading. For any suggestion or queries, leave your comments below.", "If you liked the article, please hit the \ud83d\udc4f icon to support it. This will help other Medium users find it. Share it, so that others can read it.", "Written by"], "postingTime": "2018-11-01T18:23:56.171Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Will Koehrsen", "articleTile": "Hyperparameter Tuning the Random Forest in Python - Towards Data Science", "content": ["So we\u2019ve built a random forest model to solve our machine learning problem (perhaps by following this ", ") but we\u2019re not too impressed by the results. What are our options? As we saw in the ", ", our first step should be to gather more data and perform feature engineering. Gathering more data and feature engineering usually has the greatest payoff in terms of time invested versus improved performance, but when we have exhausted all data sources, it\u2019s time to move on to model hyperparameter tuning. This post will focus on optimizing the random forest model in Python using Scikit-Learn tools. Although this article builds on part one, it fully stands on its own, and we will cover many widely-applicable machine learning concepts.", "I have included Python code in this article where it is most instructive. Full code and data to follow along can be found on the project ", ".", "The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of ", " (or your parents might have!). While model ", "are learned during training \u2014 such as the slope and intercept in a linear regression \u2014 ", " must be set by the data scientist before", "training. In the case of a random forest, hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node. (The parameters of a random forest are the variables and thresholds used to split each node learned during training). Scikit-Learn implements a set of ", "for all models, but these are not guaranteed to be optimal for a problem. The best hyperparameters are usually impossible to determine ahead of time, and tuning a model is where machine learning turns from a science into trial-and-error based engineering.", "Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations evaluate the performance of each model. However, evaluating each model only on the training set can lead to one of the most fundamental problems in machine learning: ", ".", "If we optimize the model for the training data, then our model will score very well on the training set, but will not be able to generalize to new data, such as in a test set. When a model performs highly on the training set but poorly on the test set, this is known as overfitting, or essentially creating a model that knows the training set very well but cannot be applied to new problems. It\u2019s like a student who has memorized the simple problems in the textbook but has no idea how to apply concepts in the messy real world.", "An overfit model may look impressive on the training set, but will be useless in a real application. Therefore, the standard procedure for hyperparameter optimization accounts for overfitting through ", ".", "The technique of cross validation (CV) is best explained by example using the most common method, ", " When we approach a machine learning problem, we make sure to split our data into a training and a testing set. In K-Fold CV, we further split our training set into K number of subsets, called folds. We then iteratively fit the model K times, each time training the data on K-1 of the folds and evaluating on the Kth fold (called the validation data). As an example, consider fitting a model with K = 5. The first iteration we train on the first four folds and evaluate on the fifth. The second time we train on the first, second, third, and fifth fold and evaluate on the fourth. We repeat this procedure 3 more times, each time evaluating on a different fold. At the very end of training, we average the performance on each of the folds to come up with final validation metrics for the model.", "For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one, train it on the full training set, and then evaluate on the testing set. This sounds like an awfully tedious process! Each time we want to assess a different set of hyperparameters, we have to split our training data into K fold and train and evaluate K times. If we have 10 sets of hyperparameters and are using 5-Fold CV, that represents 50 training loops. Fortunately, as with most problems in machine learning, someone has solved our problem and model tuning with K-Fold CV can be automatically implemented in Scikit-Learn.", "Usually, we only have a vague idea of the best hyperparameters and thus the best approach to narrow our search is to evaluate a wide range of values for each hyperparameter. Using Scikit-Learn\u2019s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values.", "As a brief recap before we get into model tuning, we are dealing with a supervised regression machine learning problem. We are trying to predict the temperature tomorrow in our city (Seattle, WA) using past historical weather data. We have 4.5 years of training data, 1.5 years of test data, and are using 6 different features (variables) to make our predictions. (To see the full code for data preparation, see the ", ").", "Let\u2019s examine the features quickly.", "In previous posts, we checked the data to check for anomalies and we know our data is clean. Therefore, we can skip the data cleaning and jump straight into hyperparameter tuning.", "To look at the available hyperparameters, we can create a random forest and examine the default values.", "Wow, that is quite an overwhelming list! How do we know where to start? A good place is the ", ". This tells us the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features). We could go read the ", "and try to theorize the best hyperparameters, but a more efficient use of our time is just to try out a wide range of values and see what works! We will try adjusting the following set of hyperparameters:", "To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:", "On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.", "Now, we instantiate the random search and fit it like any Scikit-Learn model:", "The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.", "We can view the best parameters from fitting the random search:", "From these results, we should be able to narrow the range of values for each hyperparameter.", "To determine if random search yielded a better model, we compare the base model with the best random search model.", "We achieved an unspectacular improvement in accuracy of 0.4%. Depending on the application though, this could be a significant benefit. We can further improve our results by using grid search to focus on the most promising hyperparameters ranges found in the random search.", "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:", "This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings. We can fit the model, display the best hyperparameters, and evaluate performance:", "It seems we have about maxed out performance, but we can give it one more try with a grid further refined from our previous results. The code is the same as before just with a different grid so I only present the results:", "A small decrease in performance indicates we have reached diminishing returns for hyperparameter tuning. We could continue, but the returns would be minimal at best.", "We can make some quick comparisons between the different approaches used to improve performance showing the returns on each. The following table shows the final results from all the improvements we made (including those from the first part):", "Model is the (very unimaginative) names for the models, accuracy is the percentage accuracy, error is the average absolute error in degrees, n_features is the number of features in the dataset, n_trees is the number of decision trees in the forest, and time is the training and predicting time in seconds.", "The models are as follows:", "In terms of programmer-hours, gathering data took about 6 hours while hyperparameter tuning took about 3 hours. As with any pursuit in life, there is a point at which pursuing further optimization is not worth the effort and knowing when to stop can be just as important as being able to keep going (sorry for getting all philosophical). Moreover, in any data problem, there is what is called the ", ", which is the absolute minimum possible error in a problem. Bayes error, also called reproducible error, is a combination of latent variables, the factors affecting a problem which we cannot measure, and inherent noise in any physical process. Creating a perfect model is therefore not possible. Nonetheless, in this example, we were able to significantly improve our model with hyperparameter tuning and we covered numerous machine learning topics which are broadly applicable.", "To further analyze the process of hyperparameter optimization, we can change one setting at a time and see the effect on the model performance (essentially conducting a controlled experiment). For example, we can create a grid with a range of number of trees, perform grid search CV, and then plot the results. Plotting the training and testing error and the training time will allow us to inspect how changing one hyperparameter impacts the model.", "First we can look at the effect of changing the number of trees in the forest. (see notebook for training and plotting code)", "As the number of trees increases, our error decreases up to a point. There is not much benefit in accuracy to increasing the number of trees beyond 20 (our final model had 100) and the training time rises consistently.", "We can also examine curves for the number of features to split a node:", "As we increase the number of features retained, the model accuracy increases as expected. The training time also increases although not significantly.", "Together with the quantitative stats, these visuals can give us a good idea of the trade-offs we make with different combinations of hyperparameters. Although there is usually no way to know ahead of time what settings will work the best, this example has demonstrated the simple tools in Python that allow us to optimize our machine learning model.", "As always, I welcome feedback and constructive criticism. I can be reached at wjk68@case.edu", "Written by"], "postingTime": "2018-01-10T15:14:58.001Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Jonny Brooks-Bartlett", "articleTile": "Probability concepts explained: Maximum likelihood estimation", "content": ["In this post I\u2019ll explain what the maximum likelihood method for parameter estimation is and go through a simple example to demonstrate the method. Some of the content requires knowledge of fundamental probability concepts such as the definition of joint probability and independence of events. ", " so feel free to read this if you think you need a refresher.", "Often in machine learning we use a model to describe the process that results in the data that are observed. For example, we may use a random forest model to classify whether customers may cancel a subscription from a service (known as ", ") or we may use a linear model to predict the revenue that will be generated for a company depending on how much they may spend on advertising (this would be an example of ", "). Each model contains its own set of parameters that ultimately defines what the model looks like.", "For a linear model we can write this as", ". In this example ", "could represent the advertising spend and ", "might be the revenue generated. ", " and ", " are parameters for this model. Different values for these parameters will give different lines (see figure below).", "So parameters define a blueprint for the model. It is only when specific values are chosen for the parameters that we get an instantiation for the model that describes a given phenomenon.", "Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.", "The above definition may still sound a little cryptic so let\u2019s go through an example to help understand this.", "Let\u2019s suppose we have observed 10 data points from some process. For example, each data point could represent the length of time in seconds that it takes a student to answer a specific exam question. These 10 data points are shown in the figure below", "We first have to decide which model we think best describes the process of generating the data. This part is very important. At the very least, we should have a good idea about which model to use. This usually comes from having some domain expertise but we wont discuss this here.", "For these data we\u2019ll assume that the data generation process can be adequately described by a Gaussian (normal) distribution. Visual inspection of the figure above suggests that a Gaussian distribution is plausible because most of the 10 points are clustered in the middle with few points scattered to the left and the right. (Making this sort of decision on the fly with only 10 data points is ill-advised but given that I generated these data points we\u2019ll go with it).", "Recall that the Gaussian distribution has 2 parameters. The mean, \u03bc, and the standard deviation, \u03c3. Different values of these parameters result in different curves (just like with the straight lines above). We want to know", "(See figure below). Maximum likelihood estimation is a method that will find the values of \u03bc and \u03c3 that result in the curve that best fits the data.", "The true distribution from which the data were generated was f1 ~ N(10, 2.25), which is the blue curve in the figure above.", "Now that we have an intuitive understanding of what maximum likelihood estimation is we can move on to learning how to calculate the parameter values. The values that we find are called the maximum likelihood estimates (MLE).", "Again we\u2019ll demonstrate this with an example. Suppose we have three data points this time and we assume that they have been generated from a process that is adequately described by a Gaussian distribution. These points are 9, 9.5 and 11. ", "What we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we\u2019ll make our first assumption. ", ". This assumption makes the maths much easier. If the events (i.e. the process that generates the data) are independent, then the total probability of observing all of data is the product of observing each data point individually (i.e. the product of the marginal probabilities).", "The probability density of observing a single data point ", "that is generated from a Gaussian distribution is given by:", "The semi colon used in the notation ", "is there to emphasise that the symbols that appear after it are parameters of the probability distribution. So it shouldn\u2019t be confused with a conditional probability (which is typically represented with a vertical line e.g. ", "In our example the total (joint) probability density of observing the three data points is given by:", "We just have to figure out the values of ", "and", "that results in giving the maximum value of the above expression.", "If you\u2019ve covered calculus in your maths classes then you\u2019ll probably be aware that there is a technique that can help us find maxima (and minima) of functions. It\u2019s called ", " All we have to do is find the derivative of the function, set the derivative function to zero and then rearrange the equation to make the parameter of interest the subject of the equation. And voil\u00e0, we\u2019ll have our MLE values for our parameters. I\u2019ll go through these steps now but I\u2019ll assume that the reader knows how to perform differentiation on common functions. If you would like a more detailed explanation then just let me know in the comments.", "The above expression for the total probability is actually quite a pain to differentiate, so it is almost always simplified by taking the natural logarithm of the expression. This is absolutely fine because the natural logarithm is a ", ". This means that if the value on the x-axis\u00a0increases, the value on the y-axis also increases (see figure below). This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.", "Taking logs of the original expression gives us:", "This expression can be simplified again using the laws of logarithms to obtain:", "This expression can be differentiated to find the maximum. In this example we\u2019ll find the MLE of the mean, \u03bc. To do this we take the partial derivative of the function with respect to \u03bc, giving", "Finally, setting the left hand side of the equation to zero and then rearranging for \u03bc gives:", "And there we have our maximum likelihood estimate for \u03bc. We can do the same thing with \u03c3 too but I\u2019ll leave that as an exercise for the keen reader.", " is the short answer. It\u2019s more likely that in a real world scenario the derivative of the log-likelihood function is still analytically intractable (i.e. it\u2019s way too hard/impossible to differentiate the function by hand). Therefore, iterative methods like ", " are used to find numerical solutions for the parameter estimates. The overall idea is still the same though.", "Well this is just statisticians being pedantic (but for good reason). Most people tend to use probability and likelihood interchangeably but statisticians and probability theorists distinguish between the two. The reason for the confusion is best highlighted by looking at the equation.", "These expressions are equal! So what does this mean? Let\u2019s first define ", "? It means ", ". It\u2019s worth noting that we can generalise this to any number of parameters and any distribution.", "On the other hand ", " means ", "The equation above says that the probability density of the data given the parameters is equal to the likelihood of the parameters given the data. But despite these two things being equal, the likelihood and the probability density are fundamentally asking different questions \u2014 one is asking about the data and the other is asking about the parameter values. This is why the method is called maximum likelihood and not maximum probability.", "Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the least squares method. For a more in-depth mathematical derivation check out ", ".", "Intuitively we can interpret the connection between the two methods by understanding their objectives. For least squares parameter estimation we want to find the line that minimises the total squared distance between the data points and the regression line (see the figure below). In maximum likelihood estimation we want to maximise the total probability of the data. When a Gaussian distribution is assumed, the maximum probability is found when the data points get closer to the mean value. Since the Gaussian distribution is symmetric, this is equivalent to minimising the distance between the data points and the mean value.", "If there is anything that is unclear or I\u2019ve made some mistakes in the above feel free to leave a comment. In the next post I plan to cover ", ".", "Thank you for reading.", "Written by"], "postingTime": "2018-01-31T09:22:44.396Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Dipanjan (DJ) Sarkar", "articleTile": "Continuous Numeric Data - Towards Data Science", "content": ["is something which you cannot ignore whether to choose to agree or disagree with it. A more apt saying in today\u2019s digital revolutionary age would be ", ". Indeed data has become a first class asset for businesses, corporations and organizations irrespective of their size and scale. Any intelligent system regardless of their complexity needs to be powered by data. At the heart of any intelligent system, we have one or more algorithms based on machine learning, deep learning or statistical methods which consume this data to gather knowledge and provide intelligent insights over a period of time. Algorithms are pretty naive by themselves and cannot work out of the box on raw data. Hence the need for engineering meaningful features from raw data is of utmost importance which can be understood and consumed by these algorithms.", "Any intelligent system basically consists of an end-to-end pipeline starting from ingesting raw data, leveraging data processing techniques to wrangle, process and engineer meaningful features and attributes from this data. Then we usually leverage techniques like statistical models or machine learning models to model on these features and then deploy this model if necessary for future usage based on the problem to be solved at hand. A typical standard machine learning pipeline based on the ", " industry standard process model is depicted below.", "Ingesting raw data and building models on top of this data directly would be foolhardy since we wouldn\u2019t get desired results or performance and also algorithms are not intelligent enough to automatically extract meaningful features from raw data (there are automated feature extraction techniques which are enabled nowadays with deep learning methodologies to some extent, but more on that later!).", "Our main area of focus falls under the data preparation aspect as pointed out in the figure above, where we deal with various methodologies to extract meaningful attributes or features from the raw data after it has gone through necessary wrangling and pre-processing.", "Feature engineering is an essential part of building any intelligent system. Even though you have a lot of newer methodologies coming in like deep learning and meta-heuristics which aid in automated machine learning, each problem is domain specific and better features (suited to the problem) is often the deciding factor of the performance of your system. Feature Engineering is an art as well as a science and this is the reason Data Scientists often spend 70% of their time in the data preparation phase before modeling. Let\u2019s look at a few quotes relevant to feature engineering from several renowned people in the world of Data Science.", "\u201cComing up with features is difficult, time-consuming, requires expert knowledge. \u2018Applied machine learning\u2019 is basically feature engineering.\u201d", "\u2014 Prof. Andrew Ng.", "This basically reinforces what we mentioned earlier about data scientists spending close to 80% of their time in engineering features which is a difficult and time-consuming process, requiring both domain knowledge and mathematical computations.", "\u201cFeature engineering is the process of transforming ", " into ", " that better represent ", " to ", ", resulting in improved ", " on ", ".\u201d", "\u2014 Dr. Jason Brownlee", "This gives us an idea about feature engineering being the process of transforming data into features to act as inputs for machine learning models such that ", " help in ", "the overall ", ". Features are also very much dependent on the underlying problem. Thus, even though the machine learning task might be same in different scenarios, like classification of emails into spam and non-spam or classifying handwritten digits, the features extracted in each scenario will be very different from the other.", "Prof. Pedro Domingos from the University of Washington, in his paper titled, ", "tells us the following.", "\u201cAt the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\u201d", "\u2014 Prof. Pedro Domingos", "The final quote which should motivate you about feature engineering is from renowned Kaggler, Xavier Conort. Most of you already know that tough real-world machine learning problems are often posted on Kaggle regularly which is usually open to everyone.", "\u201cThe algorithms we used are very standard for Kagglers. \u2026We spent most of our efforts in feature engineering. \u2026 We were also very careful to discard features likely to expose us to the risk of over-fitting our model.\u201d", "\u2014 Xavier Conort", "A ", " is typically a specific representation on top of ", ", which is an individual, measurable attribute, typically depicted by a column in a dataset. Considering a generic two-dimensional dataset, each ", " is depicted by a ", " and each ", " by a ", ", which will have a specific value for an observation.", "Thus like in the example in the figure above, each row typically indicates a feature vector and the entire set of features across all the observations forms a two-dimensional feature matrix also known as a feature-set. This is akin to data frames or spreadsheets representing two-dimensional data. Typically machine learning algorithms work with these numeric matrices or tensors and hence most feature engineering techniques deal with converting raw data into some numeric representations which can be easily understood by these algorithms.", "Features can be of two major types based on the dataset. Inherent ", "are obtained directly from the dataset with no extra data manipulation or engineering. ", " are usually obtained from feature engineering, where we extract features from existing data attributes. A simple example would be creating a new feature ", " from an employee dataset containing ", " by just subtracting their birth date from the current date.", "There are diverse types and formats of data including structured and unstructured data. In this article, we will discuss various feature engineering strategies for dealing with structured continuous numeric data. All these examples are a part of one of my recent books ", "and you can access relevant datasets and code used in this article on ", "A big shout out also goes to ", " who helped me by providing some excellent pointers on feature engineering techniques.", "Numeric data typically represents data in the form of scalar values depicting observations, recordings or measurements. Here, by numeric data, we mean ", " and not discrete data which is typically represented as categorical data. Numeric data can also be represented as a vector of values where each value or entity in the vector can represent a specific feature. Integers and floats are the most common and widely used numeric data types for continuous numeric data. Even though numeric data can be directly fed into machine learning models, you would still need to engineer features which are relevant to the scenario, problem and domain before building a model. Hence the need for feature engineering still remains. Let\u2019s leverage python and look at some strategies for feature engineering on numeric data. We load up the following necessary dependencies first (typically in a ", " notebook).", "Like we mentioned earlier, raw numeric data can often be fed directly to machine learning models based on the context and data format. Raw measures are typically indicated using numeric variables directly as features without any form of transformation or engineering. Typically these features can indicate values or counts. Let\u2019s load up one of our datasets, the ", " also available on ", ".", "Pok\u00e9mon is a huge media franchise surrounding fictional characters called Pok\u00e9mon which stands for pocket monsters. In short, you can think of them as fictional animals with superpowers! This dataset consists of these characters with various statistics for each character.", "If you closely observe the data frame snapshot in the above figure, you can see that several attributes represent numeric raw values which can be used directly. The following snippet depicts some of these features with more emphasis.", "Thus, you can directly use these attributes as features which are depicted in the above data frame. These include each Pok\u00e9mon\u2019s HP (Hit Points), Attack and Defense stats. In fact, we can also compute some basic statistical measures on these fields.", "With this you can get a good idea about statistical measures in these features like count, average, standard deviation and quartiles.", "Another form of raw measures include features which represent frequencies, counts or occurrences of specific attributes. Let\u2019s look at a sample of data from the ", " which depicts counts or frequencies of songs which have been heard by various users.", "It is quite evident from the above snapshot that the ", " field can be used directly as a frequency\\count based numeric feature.", "Often raw frequencies or counts may not be relevant for building a model based on the problem which is being solved. For instance if I\u2019m building a recommendation system for song recommendations, I would just want to know if a person is interested or has listened to a particular song. This doesn\u2019t require the number of times a song has been listened to since I am more concerned about the various songs he\\she has listened to. In this case, a binary feature is preferred as opposed to a count based feature. We can binarize our ", " field as follows.", "You can also use ", " ", " class here from its ", " module to perform the same task instead of ", " arrays.", "You can clearly see from the above snapshot that both the methods have produced the same result. Thus we get a binarized feature indicating if the song was listened to or not by each user which can be then further used in a relevant model.", "Often when dealing with continuous numeric attributes like proportions or percentages, we may not need the raw values having a high amount of precision. Hence it often makes sense to round off these high precision percentages into numeric integers. These integers can then be directly used as raw values or even as categorical (discrete-class based) features. Let\u2019s try applying this concept in a dummy dataset depicting store items and their popularity percentages.", "Based on the above ouputs, you can guess that we tried two forms of rounding. The features depict the item popularities now both on a scale of ", " and on a scale of ", ". You can use these values both as numerical or categorical features based on the scenario and problem.", "Supervised machine learning models usually try to model the output responses (discrete classes or continuous values) as a function of the input feature variables. For example, a simple linear regression equation can be depicted as", "where the input features are depicted by variables", "having weights or coefficients denoted by", "respectively and the goal is to predict the response ", "In this case, this simple linear model depicts the relationship between the output and inputs, purely based on the individual, separate input features.", "However, often in several real-world scenarios, it makes sense to also try and capture the interactions between these feature variables as a part of the input feature set. A simple depiction of the extension of the above linear regression formulation with interaction features would be", "where the features represented by", "denote the interaction features. Let\u2019s try engineering some interaction features on our Pok\u00e9mon dataset now.", "From the output data frame, we can see that we have two numeric (continuous) features, ", " and ", ". We will now build features up to the 2nd degree by leveraging ", ".", "The above feature matrix depicts a total of five features including the new interaction features. We can see the degree of each feature in the above matrix as follows.", "Looking at this output, we now know what each feature actually represents from the degrees depicted here. Armed with this knowledge, we can assign a name to each feature now as follows. This is just for ease of understanding and you should name your features with better, easy to access and simple names.", "Thus the above data frame represents our original features along with their interaction features.", "The problem of working with raw, continuous numeric features is that often the distribution of values in these features will be skewed. This signifies that some values will occur quite frequently while some will be quite rare. Besides this, there is also another problem of the varying range of values in any of these features. For instance view counts of specific music videos could be abnormally large (", "we\u2019re looking at you!) and some could be really small. Directly using these features can cause a lot of issues and adversely affect the model. Hence there are strategies to deal with this, which include binning and transformations.", "Binning, also known as quantization is used for transforming continuous numeric features into discrete ones (categories). These discrete values or numbers can be thought of as categories or bins into which the raw, continuous numeric values are binned or grouped into. Each bin represents a specific degree of intensity and hence a specific range of continuous numeric values fall into it. Specific strategies of binning data include fixed-width and adaptive binning. Let\u2019s use a subset of data from a dataset extracted from the ", " which talks about various attributes pertaining to coders and software developers.", "The ", " variable is basically a unique identifier for each coder\\developer who took the survey and the other fields are pretty self-explanatory.", "Just like the name indicates, in fixed-width binning, we have specific fixed widths for each of the bins which are usually pre-defined by the user analyzing the data. Each bin has a pre-fixed range of values which should be assigned to that bin on the basis of some domain knowledge, rules or constraints. Binning based on rounding is one of the ways, where you can use the rounding operation which we discussed earlier to bin raw values.", "Let\u2019s now consider the ", " feature from the coder survey dataset and look at its distribution.", "The above histogram depicting developer ages is slightly right skewed as expected (lesser aged developers). We will now assign these raw age values into specific bins based on the following scheme", "We can easily do this using what we learnt in the ", " section earlier where we round off these raw age values by taking the floor value after dividing it by 10.", "You can see the corresponding bins for each age have been assigned based on rounding. But what if we need more flexibility? What if we want to decide and fix the bin widths based on our own rules\\logic? Binning based on custom ranges will help us achieve this. Let\u2019s define some custom age ranges for binning developer ages using the following scheme.", "Based on this custom binning scheme, we will now label the bins for each developer age value and we will store both the bin range as well as the corresponding label.", "The drawback in using fixed-width binning is that due to us manually deciding the bin ranges, we can end up with irregular bins which are not uniform based on the number of data points or values which fall in each bin. Some of the bins might be densely populated and some of them might be sparsely populated or even empty! Adaptive binning is a safer strategy in these scenarios where we let the data speak for itself! That\u2019s right, we use the data distribution itself to decide our bin ranges.", "Quantile based binning is a good strategy to use for adaptive binning. Quantiles are specific values or cut-points which help in partitioning the continuous valued distribution of a specific numeric field into discrete contiguous bins or intervals. Thus, ", " help in partitioning a numeric attribute into ", " equal partitions. Popular examples of quantiles include the ", " known as the ", "which divides the data distribution into two equal bins, ", " known as the ", "which divide the data into 4 equal bins and ", " also known as the ", "which create 10 equal width bins. Let\u2019s now look at the data distribution for the developer ", " field.", "The above distribution depicts a right skew in the income with lesser developers earning more money and vice versa. Let\u2019s take a ", " or a ", "based adaptive binning scheme. We can obtain the quartiles easily as follows.", "Let\u2019s now visualize these quantiles in the original distribution histogram!", "The red lines in the distribution above depict the quartile values and our potential bins. Let\u2019s now leverage this knowledge to build our quartile based binning scheme.", "This should give you a good idea of how quantile based adaptive binning works. An important point to remember here is that the resultant outcome of binning leads to discrete valued categorical features and you might need an additional step of feature engineering on the categorical data before using it in any model. We will cover feature engineering strategies for categorical data shortly in the next part!", "We talked about the adverse effects of skewed data distributions briefly earlier. Let\u2019s look at a different strategy of feature engineering now by making use of statistical or mathematical transformations.We will look at the Log transform as well as the Box-Cox transform. Both of these transform functions belong to the Power Transform family of functions, typically used to create monotonic data transformations. Their main significance is that they help in stabilizing variance, adhering closely to the normal distribution and making the data independent of the mean based on its distribution", "The log transform belongs to the power transform family of functions. This function can be mathematically represented as", "which reads as ", "of ", "to the base ", "is equal to ", "This can then be translated into", "which indicates as to what power must the base ", "be raised to in order to get ", ". The natural logarithm uses ", " where ", "= 2.71828 popularly known as Euler\u2019s number. You can also use base ", "=10 used popularly in the decimal system.", "Log transforms are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. This tends to make the skewed distribution as normal-like as possible. Let\u2019s use log transform on our developer ", " feature which we used earlier.", "The ", " field depicts the transformed feature after log transformation. Let\u2019s look at the data distribution on this transformed field now.", "Based on the above plot, we can clearly see that the distribution is more normal-like or gaussian as compared to the skewed distribution on the original data.", "The Box-Cox transform is another popular function belonging to the power transform family of functions. This function has a pre-requisite that the numeric values to be transformed must be positive (similar to what ", " transform expects). In case they are negative, shifting using a constant value helps. Mathematically, the Box-Cox transform function can be denoted as follows.", "Such that the resulted transformed output ", "is a function of input ", "and the transformation parameter \u03bb such that when \u03bb = 0, the resultant transform is the natural ", "transform which we discussed earlier. The optimal value of \u03bb is usually determined using a maximum likelihood or log-likelihood estimation. Let\u2019s now apply the Box-Cox transform on our developer income feature. First we get the optimal lambda value from the data distribution by removing the non-null values as follows.", "Now that we have obtained the optimal \u03bb value, let us use the Box-Cox transform for two values of \u03bb such that \u03bb = 0 and \u03bb = \u03bb(optimal) and transform the developer ", " feature.", "The transformed features are depicted in the above data frame. Just like we expected, ", " and ", " have the same values. Let\u2019s look at the distribution of the transformed ", " feature after transforming with the optimal \u03bb.", "The distribution looks more normal-like similar to what we obtained after the ", "transform.", "Feature engineering is a very important aspect of machine learning and data science and should never be ignored. While we have automated feature engineering methodologies like deep learning as well as automated machine learning frameworks like ", " (which still stresses that it requires good features to work well!). Feature engineering is here to stay and even some of these automated methodologies often require specific engineered features based on the data type, domain and the problem to be solved.", "We looked at popular strategies for feature engineering on continuous numeric data in this article. In the next part, we will look at popular strategies for dealing with discrete, categorical data and then move on to unstructured data types in future articles. Stay tuned!", "All the code and datasets used in this article can be accessed from my ", "The code is also available as a ", "Written by"], "postingTime": "2019-03-27T13:43:20.095Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Hafidz Zulkifli", "articleTile": "Understanding Learning Rates and How It Improves Performance in Deep Learning", "content": ["This post is an attempt to document my understanding on the following topic:", "Much of this post are based on the stuff written by past fast.ai fellows [1], [2], [5] and [3] . This is a concise version of it, arranged in a way for one to quickly get to the meat of the material. Do go over the references for more details.", "Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we\u2019ll be taking a long time to converge \u2014 especially if we get stuck on a plateau region.", "The following formula shows the relationship.", "Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.", "As such, it\u2019s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate.", "Furthermore, the learning rate affects how quickly our model can converge to a local minima (aka arrive at the best accuracy). Thus getting it right from the get go would mean lesser time for us to train the model.", "In Section 3.3 of \u201c", ".\u201d [4], Leslie N. Smith argued that you could estimate a good learning rate by training the model initially with a very low learning rate and increasing it (either linearly or exponentially) at each iteration.", "If we record the learning at each iteration and plot the learning rate (log) against loss; we will see that as the learning rate increase, there will be a point where the loss stops decreasing and starts to increase. In practice, our learning rate should ideally be somewhere to the left to the lowest point of the graph (as demonstrated in below graph). In this case, 0.001 to 0.01.", "At the moment it is supported as a function in the fast.ai package, developed by Jeremy Howard as a way to abstract the pytorch package (much like how Keras is an abstraction for Tensorflow).", "One only needs to type in the following command to start finding the most optimal learning rate to use before training a neural network.", "At this juncture we\u2019ve covered what learning rate is all about, it\u2019s importance, and how can we systematically come to an optimal value to use when we start training our model.", "Next we would go through how learning rates can still be used to improve our model\u2019s performance.", "Typically when one sets their learning rate and trains the model, one would only wait for the learning rate to decrease over time and for the model to eventually converge.", "However, as the gradient reaches a plateau, the training loss becomes harder to improve. In [3], ", "There are a few options that we could consider. In general, taking a quote from [1],", "\u2026 instead of using a fixed value for learning rate and decreasing it over time, if the training doesn\u2019t improve our loss anymore, we\u2019re going to be changing the learning rate every iteration according to some cyclic function ", ". Each cycle has a fixed length in terms of number of iterations. This method lets the learning rate cyclically vary between reasonable boundary values.", "In [2], Leslie proposes a \u2018triangular\u2019 method where the learning rates are restarted after every few iterations.", "Another method that is also popular is called ", "[6]. This method basically uses the cosine function as the cyclic function and restarts the learning rate at the maximum at each cycle. The \u201cwarm\u201d bit comes from the fact that when the learning rate is restarted, it does not start from scratch; but rather from the parameters to which the model converged during the last step [7].", "While there are variations of this, the below diagram demonstrates one of its implementation, where each cycle is set to the same time period.", "Thus we now have a way to reduce the training time, by basically periodically jumping around \u201c", "Aside from saving time, research also shows that using these method tend to improve classification accuracy without tuning and within fewer iteration.", "In the fast.ai course, much emphasis is given in leveraging pretrained model when solving AI problems. For example, in solving an image classification problem, students are taught how to use pretrained models such VGG or Resnet50 and connecting it to whatever image dataset that you want to predict.", "To summarize how model building is done in fast.ai (the program, not to be confused with the fast.ai package), below are the few steps [8] that we\u2019d normally take:", "1. Enable data augmentation, and precompute=True", "3. Train last layer from precomputed activations for 1\u20132 epochs", "4. Train last layer with data augmentation (i.e. precompute=False) for 2\u20133 epochs with cycle_len=1", "5. Unfreeze all layers", "8. Train full network with cycle_mult=2 until over-fitting", "From the steps above, we notice that step 2, 5, and 7 are concerned about learning rate. In the earlier part of this post, we\u2019ve basically covered item no.2 of the steps mentioned \u2014 where we touched on how to derive the best learning rate prior to training the model.", "In the subsequent section we went over how by using SGDR we are able to reduce the training time and increase accuracy by restarting the learning rate every now and then to avoid regions where gradient close to zero.", "In this last section, we\u2019ll go over differential learning, and how it\u2019s being used to determine the learning rate when training models attached with a pretrained model.", "It is a method where you set different learning rates to different layers in the network during training. This is in contrast to how people normally configure the learning rate, which is to use the same rate throughout the network during training.", "To illustrate the concept a bit clearer, we can refer to the below diagram, where a pretrained model is split into 3 groups, where each group would be configured with an increasing learning rate value.", "The intuition behind this method of configuration is that the first few layers would typically contain very granular details of the data, such as the lines and the edges \u2014 of which we normally wouldn\u2019t want to change much and like to retain it\u2019s information. As such, there\u2019s not much need to change their weights by a big amount.", "In contrast, in later layers such as the ones in green above \u2014 where we get detailed features of the data such as eyeballs or mouth or nose; we might not necessarily need to keep them.", "In [9], it is argued that fine-tuning an entire model would be too costly as some could have more than 100 layers. As such, what people usually do is to fine-tune the model one layer at a time.", "However, this introduces a sequential requirement, hindering parallelism, and requires multiple passes through the dataset, resulting in overfitting for small datasets.", "It has also been demonstrated that the methods introduced in [9] are able to improve both accuracy and reduce error rates in various NLP classification tasks (below)", "Written by"], "postingTime": "2018-01-27T17:18:00.194Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Daniel McNichol", "articleTile": "On Average, You\u2019re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis", "content": ["You have a bunch of numbers. You want to summarize them with fewer numbers, preferably a single number. So you add up all the numbers then divide the sum by the total number of numbers. Boom: behold the \u201c", "\u201d, right?", "Maybe.", "Contrary to popular belief, ", " isn\u2019t actually a thing, mathematically speaking. Meaning: ", ". What we ", " is \u201carithmetic mean\u201d, the well-known operation described above. We call this \u201caverage\u201d because we expect it to conform to the colloquial definition of \u201caverage\u201d: a typical, \u2018normal\u2019 or middle value. Often we\u2019re correct, but less often than we think.", "The arithmetic mean is just one among many ways of arriving at an \u201caverage\u201d value. More technically, these are known as \u201c", "\u201d, \u201c", "\u201d or \u201c", "\u201d.", "Probably the 2nd most famous ", " is the ", ", the literal middle value of a dataset (which, as such, is often more \u201caverage\u201d than the mean). I won\u2019t discuss this here, but suffice to say that the ", " is overused in many cases when the ", " is more appropriate. Further reading ", ", ", " & ", " (last one overlaps a bit with the rest of this article, and is very good).", "This article will focus on two lesser known measures: ", ".", " develops a conceptual, intuitive & practical understanding of how they work & when to use them.", " is a separate post & gets a bit deeper & more technical, demonstrating their respective dynamics with R code, real & simulated data & plots.", "The ", " is just 1 of 3 \u2018", "\u2019 (named after Pythagoras & his ilk, who studied their proportions). As foretold, the ", " & ", " ", " round out the trio.", "To understand the basics of how they function, let\u2019s work forward from the familiar arithmetic mean.", "The ", " mean is appropriately named: we find it by ", "all of the numbers in the dataset, then dividing by however many numbers are in the dataset (in order to bring the sum back down to the scale of the original numbers).", "Notice, what we are essentially saying here is: ", "But there\u2019s nothing particularly special about addition. It\u2019s just one rather simple mathematical operation. The arithmetic mean works well to produce an \u201caverage\u201d number of a dataset when there is an ", " relationship between the numbers. Such a relationship is often called \u201c", "\u201d, because when graphed in ascending or descending order the numbers tend fall on or around a straight line. A simple idealized example would be a dataset where each number is produced by adding 3 to the previous number:", "The ", " thus gives us a perfectly reasonable middle value:", "But not all datasets are best described by this relationship. Some have a ", " or ", " relationship, for instance if we ", " each consecutive number by 3 rather than ", " by 3 as we did above:", "This produces what is known as a ", " (hint hint). When plotted in order, these numbers resemble more of a curve than a straight line.", "In this situation, the ", " is ill-suited to produce an \u201caverage\u201d number to summarize this data.", " isn\u2019t particularly close to most of the numbers in our dataset. In fact it\u2019s more than ", " the ", " (middle number), which is ", ".", "This skew is more apparent when the data is plotted on a flat number line:", "So what to do?", "Introducing\u2026", "Since the relationship is ", ", to find the ", " we multiply rather than add all the numbers. Then to rescale the product back down to the range of the dataset, we have to take the ", ", rather than simply dividing. You remember the ", ": the number that needs to be squared to arrive at our number of interest.", "This is the same idea, but rather than raising to the second power (aka \u2018squaring\u2019), we need to find the number that would be raised to the 7th power to produce our product, because there are 7 numbers in our dataset, which we multiplied together. This is generally known as the ", ", where ", " is the size of the dataset. Thus, we need to find the ", ".", "Notice, what we are saying here is: ", "So, the geometric mean of our dataset is:", "And on the number line:", "In this case, our geometric mean ", " resembles the middle value of our dataset. In fact, it is equivalent to the ", ".", ": the geometric mean will not ", " equal the median, only in cases where there is an exact consistent multiplicative relationship between all numbers (e.g. multiplying each previous number by 3, as we did). Real world datasets rarely contain such exact relationships, but for those that approximate this sort of multiplicative relationship, the ", " will give a closer \u2018middle number\u2019 than the ", ".", "It turns out that there are many practical uses for the ", ", as ", " relationships abound in the real world.", "One ", " is:", "Assume we have ", " that accrues a varying rate of interest each year for ", ":", "We\u2019d like to take a shortcut to find our average annual interest rate, & thus our total amount of money after 5 years, so we try to \u201caverage\u201d these rates:", "Then we insert this average % into a ", ":", "Just to be sure we\u2019re not fooling ourselves, let\u2019s do this the long way & compare results:", "What happened? Our shortcut overestimated our actual earnings by nearly ", ".", "We made a common error: ", "Now let\u2019s try again with the ", ":", "(", ": we have to use ", " as inputs in the geometric mean calculation because those are the actual factors that are multiplied with the principal values to produce the amount of interest accrued at each period, and ", ". This has the added benefit of avoiding negative numbers even when there is a negative rate, which the geometric mean equation can\u2019t handle [it also can\u2019t handle ", "]. The arithmetic mean doesn\u2019t have this issue. It\u2019s the same whether we use the interest rates themselves or ", " as input [then subtract ", " from the result], because it is additive rather than multiplicative. But the geometric mean will be different, and wrong, if we don\u2019t add ", ".)", "Plugging the ", "of the interest rates into our compound interest formula:", "That\u2019s more like it.", "We used the right mean for the right job & got the right result.", "A fancy feature of the ", " is that you can actually average across numbers on ", ".", "For instance, we want to compare online ratings for two coffeeshops using two different sources. The problem is that ", " uses a ", " scale & ", " uses a ", " scale:", "If we naively take the arithmetic mean of raw ratings for each coffeeshop:", " ", "We\u2019d conclude that ", "was the winner.", "If we were a bit more number-savvy, we\u2019d know that we have to ", " our values onto the same scale before averaging them with the arithmetic mean, to get an accurate result. So we multiply the ", " ratings by ", " to bring them from a ", " scale to the ", " scale of ", ":", "So we find that ", " is the true winner, contrary to the naive application of arithmetic mean above.", "The geometric mean, however, allows us to reach the same conclusion without having to fuss over the scale or units of measure:", "Et voil\u00e0!", "The ", " is dominated by numbers on the larger scale, which makes us think ", " is the higher rated shop. This is because the arithmetic mean expects an additive relationship between numbers & doesn\u2019t account for scales & proportions. Hence the need to bring numbers onto the same scale before applying the arithmetic mean.", "The ", ", on the other hand, can handle varying proportions with ease, due to it\u2019s multiplicative nature. This is a tremendously useful property, but ", ": We no longer have any interpretable ", " at all. The geometric mean is effectively ", " in such situations.", "I.e. the geometric means above are not ", "nor ", ". They are just unitless numbers, in relative proportion to each other. (Technically, their scale is the geometric mean of the original scales, ", ", which is ", "). This can be a problem if we actually want to interpret the results relative to some scale that is meaningful to us, such as the original ", " or ", " systems. But if we just want to know the relationship between ratings of the two coffeeshops, we\u2019re good to go.", ": As ", " by ", ", there is no guarantee that the ", " will always preserve the ordering of the ", " on scaled or normalized values, much less be proportionate to it, as I originally indicated. Rather, it is simply a different way to summarize the relationship between different sets of numbers (albeit one that will often produce more \u2018credible\u2019 summaries of values on different scales). So again, care & critical thought are necessary to its application.", "To ", ":", "As with most things in life, there are few ironclad rules for applying the geometric mean (outside of compound interest & such things). There are some heuristics & rules of thumb, but ultimately judgement & scientific skepticism are required, as ever, for ", ".", "More on this in the conclusion below, but for now let\u2019s introduce our final ", "\u2026", "The 3rd & final Pythagorean mean.", "This section will be shorter than the last as the harmonic mean is yet more esoteric than the geometric mean, but still worth understanding.", "Whereas the ", " requires addition & the ", " employs multiplication, the ", " utilizes ", ".", "As you may remember, the ", " of a number ", " is simply ", ". (e.g. the ", " of ", " is ", "). For numbers that are already fractions, this means that you can simply \u201cflip\u201d the numerator & denominator: ", ". This is true because ", " yields that fraction\u2019s ", ", e.g. ", ".", "Another way to think about reciprocals is: ", ". So when finding the reciprocal of a number ", ", we are simply asking: ", ". (This is why the ", " is also sometimes called the ", ".)", "So then, the ", " can be described in words as: ", ".", "Thats a lot of reciprocal flips there, but it\u2019s actually just a few simple steps:", "In math notation, this looks like:", "A simple example from ", ": the harmonic mean of ", ", ", ", and ", " is ", ":", "Notice, what we are saying here is: ", "(", " due to the fact that ", " has no ", " (nothing can be multiplied by ", " to ", "), the ", " also cannot handle datasets containing ", "\u2019s, similar to the ", ".)", "So that\u2019s how the plumbing works. ", " ", "To answer this, we have to answer: ", "Since ", ", like all division, are just multiplication in disguise (which is just addition in disguise), we realize: ", ".", "For instance, what is ", "? If you remember elementary school maths, you\u2019ll probably just multiply ", " by ", " (the ", " of ", ") to solve this:", "But an equivalent method would be to scale the numbers ", " & ", " to a common denominator, then divide in the normal way:", "So again, similar to using the ", " as a shortcut to finding the relationship between additive ", " of scores on different scales without normalizing to a common denominator ", ", ", ".", "As such, the ", " ", " ", "over the geometric mean", " Thus ", ".", "The canonical example of using ", " in the real world involves traveling over physical space at different rates, i.e. speeds:", "Consider a trip to the grocery store & back:", "Again, we might naively apply the ", " to ", " & ", ", and proudly declare \u201c", "!\u201d", "But consider again: because you travelled faster in one direction, you covered those ", " quicker & spent less time overall traveling at that speed, so your average rate of travel across your entire trip\u2019s duration is not the middle point between ", " & ", ", it should be closer to ", "because you spent longer traveling at that speed.", "In order to apply the ", " correctly here, we\u2019d have to determine the amount of time spent traveling at each rate, then weight our arithmetic mean calculation appropriately:", "So we see that our true average rate of travel was ", ", which is ", " (or ", ") lower than our naive declaration of ", " using an unweighted ", ".", "You can probably guess where this is headed\u2026", "Let\u2019s try it again using the ", ".", "Our true average rate of travel, ", " adjusted for time spent traveling in each direction = ", "!", "Note a few things:", "Like the case of compound interest and the ", ", this is an example of a precise, objectively correct application of the ", ". But again, things aren\u2019t always so clear. There are other precise, mathematically justifiable ", " in physics, finance, hydrology & even (by convention) in ", ". More germane to data science: it is often applied to ", " in the evaluation of machine learning models.", "But more often than this, it\u2019s a judgement call, dependent on a nimble understanding of your data & the task at hand.", "I\u2019ll try to clarify & summarize the finer points below.", "To recap & make explicit what we\u2019ve already demonstrated:", "For instance, we saw that:", "In ", " (a separate post to follow), we\u2019ll see what should be clear to those already familiar with multiplicative transformations: the ", " of a dataset is equivalent to the ", " of the ", " of each number in that dataset. So just as the ", " is simply the ", " with a few reciprocal transformations, the ", " is just the ", " with a ", ".", "If each", "mean", "is just a transformation or reformulation of the other, how do these transformations interact & affect your results?", "Due to their respective equations: ", ".", "The three means are closer together or farther apart depending on the spread of the underlying data. The ", " occurs in the extreme case when all numbers in the dataset are the same exact number, in which case all 3 means are also equivalent. Thus, ", ":", "These proportions can be observed in the geometric depiction of the ", " (+ quadratic) ", " at the beginning of this section.", "Recognizing this relationship helps immensely in understanding when to apply each mean, & what the impact to your results will be.", "To make this more concrete, lets revisit our original additive & multiplicative datasets, with all three means depicted in each:", "Clearly, the ", " & ", " ", " seem to substantially understate the \u2018middle\u2019 of this linear, additive dataset. This is because those means are more sensitive to smaller numbers than larger numbers (making them also relatively ", " to large outliers).", "Here, the ", " sits precisely in the ordinal middle of the dataset, while the ", " still skews to the low side & the ", " skews hard to the high side, pulled by large outliers.", "It\u2019s not trivial to depict a dataset where the central tendency is well-described by the ", ", so I\u2019m just going to move on\u2026", "Please comment below with your own use cases & experience with the lesser ", " (as well as any errata you might catch in this piece!).", "\u2014 ", "Follow on twitter: ", "LinkedIn: ", "Github: ", " A previous version of this piece stated \u201cThe ", " of ", " is proportionate to the ", " when those values are normalized to a common scale\u201d. ", " ", " that this is not true. Thanks for the correction!", "Written by"], "postingTime": "2018-09-24T01:59:05.685Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Dipanjan (DJ) Sarkar", "articleTile": "The Art of Effective Visualization of Multi-dimensional Data", "content": [" is one of the core components of any analysis life-cycle pertaining to a data science project or even specific research. Data aggregation, summarization and ", "are some of the main pillars supporting this area of data analysis. Since the days of traditional ", " to even in this age of ", ", ", " has been a powerful tool and has been widely adopted by organizations owing to its effectiveness in abstracting out the right information, understanding and interpreting results clearly and easily. However, dealing with multi-dimensional datasets with typically more than two attributes start causing problems, since our medium of data analysis and communication is typically restricted to two dimensions. In this article, we will explore some effective strategies of visualizing data in multiple dimensions (ranging from ", " up to ", ").", "This is a very popular English idiom we are all familiar with and should serve as enough inspiration and motivation for us to understand and leverage data visualization as as effective tool in our analysis. Always remember that ", "Before we begin, I would also like to mention the following quote which is really relevant and reinforces the necessity of data visualization.", "\u2014 John Tukey", "I am assuming the average reader knows about the essential graphs and charts which are used for plotting and visualizing data hence I will not go into detailed explanations but we will be covering most of them during our hands-on experiments here. Data visualization should be leveraged on top of data to communicate patterns and insights with ", " as mentioned by notable visualization pioneer and statistician, ", ".", "Structured data typically consists of data observations represented by rows and features or data attributes represented by columns. Each column can also be called as a specific dimension of the dataset. Most common data types include continuous, numeric data and discrete, categorical data. Hence any data visualization will basically depict one or more data attributes in an easy to understand visual like a scatter plot, histogram, box-plot and so on. I will cover both ", " (one-dimension) and ", "(multi-dimensional) data visualization strategies. We will be using the Python machine learning eco-system here and we recommend you to check out frameworks for data analysis and visualization including ", ", ", ", ", ", ", " and ", ". Besides this, knowing about ", " is also a must if you are interested in crafting beautiful and meaningful visualizations with data. Interested readers are recommended to read ", "Edward Tufte.", "Let\u2019s get cracking instead of me droning on about theory and concepts. We will use the ", "available from the ", ". This data actually consists of two datasets depicting various attributes of red and white variants of the Portuguese ", " wine. All the analyses in this article is available in my ", " as a ", " for those of you itching to try it out yourself!", "We\u2019ll start by loading up the following necessary dependencies for our analyses.", "We will mainly be using ", "and ", "as our visualization frameworks here but you are free to check out and try the same with any other framework of your choice. Let\u2019s take a look at the data after some basic data pre-processing steps.", "We create a single data frame wines by merging both the datasets pertaining to red and white wine samples. We also create a new categorical variable ", "based on the ", " attribute of wine samples. Let\u2019s take a peek at the data now.", "It is quite evident that we have several numeric and categorical attributes for wine samples. Each observation belongs to a red or white wine sample and the attributes are specific attributes or properties measured and obtained from physicochemical tests. You can check out the ", " if you want to understand the detailed explanation of each attribute but the names are pretty self-explanatory. Let\u2019s do a quick basic descriptive summary statistics on some of these attributes of interest.", "It\u2019s quite easy to contrast and compare these statistical measures for the different types of wine samples. Notice the stark difference in some of the attributes. We will emphasize those in some of our visualizations later on.", "Univariate analysis is basically the simplest form of data analysis or visualization where we are only concerned with analyzing one data attribute or variable and visualizing the same (one dimension).", "One of the quickest and most effective ways to visualize all numeric data and their distributions, is to leverage ", "using ", "The plots above give a good idea about the basic data distribution of any of the attributes.", "Let\u2019s drill down to ", " Essentially a ", "or a ", " works quite well in understanding how the data is distributed for that attribute.", "It is quite evident from the above plot that there is a definite right skew in the distribution for wine ", " .", " is slightly different and ", "are one of the most effective ways to do the same. You can use ", " also but in general try avoiding them altogether, especially when the number of distinct categories is more than three.", "Let\u2019s move on to looking at higher dimensional data now.", "Multivariate analysis is where the fun as well as the complexity begins. Here we analyze multiple data dimensions or attributes (2 or more). Multivariate analysis not only involves just checking out distributions but also potential relationships, patterns and correlations amongst these attributes. You can also leverage inferential statistics and hypothesis testing if necessary based on the problem to be solved at hand to check out statistical significance for different attributes, groups and so on.", "One of the best ways to check out potential relationships or correlations amongst the different data attributes is to leverage a ", "and depict it as a ", ".", "The gradients in the heatmap vary based on the strength of the correlation and you can clearly see it is very easy to spot potential attributes having strong correlations amongst themselves. Another way to visualize the same is to use ", " amongst attributes of interest.", "Based on the above plot, you can see that scatter plots are also a decent way of observing potential relationships or patterns in two-dimensions for data attributes.", "An important point to note about pairwise scatter plots is that the plots are actually symmetric. The scatterplot for any pair of attributes ", " looks different from the same attributes in ", " only because the vertical and horizontal scales are different. It does not contain any new information.", "Another way of visualizing multivariate data for multiple attributes together is to use ", ".", "Basically, in this visualization as depicted above, points are represented as connected line segments. Each vertical line represents one data attribute. One complete set of connected line segments across all the attributes represents one data point. Hence points that tend to cluster will appear closer together. Just by looking at it, we can clearly see that ", "is slightly more for ", " as compared to ", ". Also ", " and ", " is higher for ", " as compared to ", " and ", " is higher for ", " as compared to ", ". Check out the statistics from the statistic table we derived earlier to validate this assumption!", "Let\u2019s look at some ways in which we can ", ". ", " and ", " in particular are good ways to not only check for patterns, relationships but also see the individual distributions for the attributes.", "The ", " is depicted on the left side and the ", " on the right in the above figure. Like we mentioned, you can check out correlations, relationships as well as individual distributions in the joint plot.", "How about ", "One way is to leverage separate plots (subplots) or ", "for one of the categorical dimensions.", "While this is a good way to visualize categorical data, as you can see, leveraging ", "has resulted in writing a lot of code. Another good way is to use ", " or ", " for the different attributes in a single plot. We can leverage ", "for the same easily.", "This definitely looks cleaner and you can also effectively compare the different categories easily from this single plot.", "Let\u2019s look at ", " (essentially numeric and categorical together). One way is to use ", " along with generic ", "or ", ".", "While this is good, once again we have a lot of boilerplate code which we can avoid by leveraging ", "and even depict the plots in one single chart.", "You can see the plot generated above is clear and concise and we can easily compare across the distributions easily. Besides this, ", " are another way of effectively depicting groups of numeric data based on the different values in the categorical attribute. ", " are a good way to know the quartile values in the data and also potential outliers.", "Another similar visualization is ", ", which are another effective way to visualize grouped numeric data using kernel density plots (depicts probability density of the data at different values).", "You can clearly see the density plots above for the different wine ", " categories for wine ", " .", "Visualizing data till two-dimensions is pretty straightforward but starts becoming complex as the number of dimensions (attributes) start increasing. The reason is because we are bound by the two-dimensions of our display mediums and our environment.", "For three-dimensional data, we can introduce a fake notion of ", "by taking a ", "in our chart or leveraging subplots and facets.", "However for data higher than three-dimensions, it becomes even more difficult to visualize the same. The best way to go higher than three dimensions is to use ", " and so on. You can also use ", "as a dimension by making an animated plot for other attributes over time (considering time is a dimension in the data). Check out ", " to get an idea of the same!", "Considering three attributes or dimensions in the data, we can visualize them by considering a ", " and introducing the notion of ", "or ", "to separate out values in a categorical dimension.", "The above plot enables you to check out correlations and patterns and also compare around wine groups. Like we can clearly see ", " and ", " is higher for ", " as compared to ", ".", "Let\u2019s look at strategies for ", ". One way would be to have two dimensions represented as the regular ", "(", "-axis)and ", "(", "-axis) and also take the notion of ", "(", "-axis) for the third dimension.", "We can also still leverage the regular 2-D axes and introduce the notion of ", "as the third dimension (essentially a", ") where the size of the dots indicate the quantity of the third dimension.", "Thus you can see how the chart above is not a conventional scatter plot but more of a bubble chart with varying point sizes (bubbles) based on the quantity of ", " . Of course its not always that you will find definite patterns in the data like in this case, we see varying sizes across the other two dimensions.", "For ", "while we can use the conventional ", ", we can leverage the notion of ", "as well as ", "or ", "to support the additional third dimension. The ", "framework helps us keep the code to a minimum and plot this effectively.", "The chart above clearly shows the frequency pertaining to each of the dimensions and you can see how easy and effective this can be in understanding relevant insights.", "Considering visualization for ", "we can use the notion of ", " for separating our groups in one of the categorical attributes while using conventional visualizations like ", " for visualizing two dimensions for numeric attributes.", "Thus hue acts as a good separator for the categories or groups and while there is no or very weak correlation as observed above, we can still understand from these plots that ", "are slightly higher for ", "as compared to ", ". Instead of a scatter plot, you can also use a ", " to understand the data in three dimensions.", "It is quite evident and expected that ", " samples have higher ", "levels as compared to ", ". You can also see the density concentrations based on the hue intensity.", "In case we are dealing with ", ", we can use ", " and ", " for visualizing data and use visualizations like ", " or ", " to visualize the different groups of data.", "In the figure above, we can see that in the 3-D visualization on the right hand plot, we have represented wine ", "on the x-axis and ", "as the ", ". We can clearly see some interesting insights like ", "is higher for ", " as compared to ", ".", "You can also consider using ", " for representing mixed attributes with more than one categorical variable in a similar way.", "We can see that both for ", "and ", "attributes, the wine ", "content increases with better quality. Also ", " tend to have a sightly higher median ", "content as compared to ", " based on the ", ". However if we check the ", ", we can see that for ", " (", "), the ", " median ", "content is greater than ", " samples. Otherwise ", "seem to have a slightly higher median ", "content in general as compared to ", ".", "Based on our discussion earlier, we leverage various components of the charts visualize multiple dimensions. One way to visualize data in four dimensions is to use ", "and ", "as specific data dimensions in a conventional plot like a ", ".", "The ", "attribute is denoted by the hue which is quite evident from the above plot. Also, while interpreting these visualizations start getting difficult due to the complex nature of the plots, you can still gather insights like ", " is higher for ", " and ", " is higher for ", ". Of course if there were some association between ", " and ", " we might have seen a gradually increasing or decreasing plane of data points showing some trend.", "Another strategy is to keep a 2-D plot but use ", "and data point ", "as data dimensions. Typically this would be a ", " similar to what we visualized earlier.", "We use ", "to represent ", "and the data point ", "to represent ", ". We do see similar patterns from what we observed in the previous chart and bubble sizes are larger for ", " in general indicate ", " values are higher for ", " as compared to ", ".", "If we have more that two categorical attributes to represent, we can reuse our concept of leveraging ", "and ", "to depict these attributes and regular plots like ", " to represent the numeric attributes. Let\u2019s look at a couple of examples.", "The effectiveness of this visualization is verified by the fact we can easily spot multiple patterns. The ", " levels for ", " are lower and also ", " have lower acidity levels. Also based on ", " samples, ", " have higher levels of ", "and ", " have the lowest levels of ", "!", "Let\u2019s take up a similar example with some other attributes and build a visualization in four dimensions.", "We clearly see that ", " have lower content of ", " which is quite relevant if you also have the necessary domain knowledge about wine composition. We also see that", " levels for ", " are lower than ", ". The ", " levels are however higher for ", " in several data points.", "Once again following a similar strategy as we followed in the previous section, to visualize data in five dimensions, we leverage various plotting components. Let\u2019s use ", ", ", "and ", " to represent three of the data dimensions besides ", " representing the other two dimensions. Since we use the notion of size, we will be basically plotting a three dimensional ", ".", "This chart depicts the same patterns and insights that we talked about in the previous section. However, we can also see that based on the point sizes which are represented by ", ", ", " have higher ", " levels as compared to ", ".", "Instead of ", ", we can also use ", "along with ", "to represent more than one categorical attribute in these five data dimensions. One of the attributes representing ", "can be ", " or even ", "(but we might need to represent it with numbers for data point sizes). While we don\u2019t depict that here due to the lack of categorical attributes, feel free to try it out on your own datasets.", "This is basically an alternative approach to visualizing the same plot which we plotted previously for five dimensions. While the additional dimension of ", " might confuse many when looking at the plot we plotted previously, this plot due to the advantage of ", ", still remains effectively on the 2-D plane and hence is often more effective and easy to interpret.", "We can already see that it\u2019s becoming complex handling so many data dimensions! If some of you are thinking, why not add more dimensions? Let\u2019s go ahead and give it a shot!", "Now that we are having fun (I hope!), let\u2019s add another data dimension in our visualizations. We will leverage ", ", ", ", ", "and ", "besides our ", " to depict all the six data dimensions.", "Wow that is six dimensions in one plot! We have wine ", " depicted by ", ", ", "(the squared pixel), ", " (the X marks) and ", "(the circles) quality wines. The ", " is represented by ", " by the ", "and data point ", " represents ", " content.", "Interpreting this might seem a bit taxing but consider a couple of components at a time when trying to understand what\u2019s going on.", "We can also build a 6-D visualization by removing the ", " component and use ", "instead for a categorical attribute.", "Thus in this scenario, we leverage ", "and ", "to represent three categorical attributes and the ", " and ", "to represent three numerical attributes for our 6-D data visualization.", "Data visualization is an art as well as a science. If you\u2019re reading this, I really commend your efforts in going through this extensive article. The intent is not to memorize anything nor to give a fixed set of rules for visualizing data. The main objective here is to understand and learn some effective strategies for visualizing data especially when the number of dimensions start to increase. I encourage you to leverage these snippets for visualizing your own datasets in the future. Feel free to leave your feedback in the comments and do share your own strategies of effective data visualization ", "All the code and datasets used in this article can be accessed from my ", "The code is also available as a ", "Written by"], "postingTime": "2018-12-11T15:32:45.250Z"}
{"nameOfPublication": "Towards Data Science", "nameOfAuthor": "Will Koehrsen", "articleTile": "Stock Prediction in Python - Towards Data Science", "content": ["Trying to predict the stock market is an enticing prospect to data scientists motivated not so much as a desire for material gain, but for the challenge.We see the daily up and downs of the market and imagine there must be patterns we, or our models, can learn in order to beat all those day traders with business degrees. Naturally, when I started using additive models for time series prediction, I had to test the method in the proving ground of the stock market with simulated funds. Inevitably, I joined the many others who have tried to beat the market on a day-to-day basis and failed. However, in the process, I learned a ton of Python including object-oriented programming, data manipulation, modeling, and visualization. I also found out why we should avoid playing the daily stock market without losing a single dollar (all I can say is play the long game)!", "When we don\u2019t experience immediate success \u2014 in any task, not just data science \u2014 we have three options:", "While option three is the best choice on an individual and community level, it takes the most courage to implement. I can selectively choose ranges when my model delivers a handsome profit, or I can throw it away and pretend I never spent hours working on it. That seems pretty naive! We advance by repeatedly failing and learning rather than by only promoting our success. Moreover, Python code written for a difficult task is not Python code written in vain!", "This post documents the prediction capabilities of Stocker, the \u201cstock explorer\u201d tool I developed in Python. ", ", I showed how to use Stocker for analysis, and the ", " for anyone wanting to use it themselves or contribute to the project.", "Stocker is a Python tool for stock exploration. Once we have the required libraries installed (check out the documentation) we can start a Jupyter Notebook in the same folder as the script and import the Stocker class:", "The class is now accessible in our session. We construct an object of the Stocker class by passing it any valid stock ticker (", "is output):", "Just like that we have 20 years of daily Amazon stock data to explore! Stocker is built on the Quandl financial library and with over 3000 stocks to use. We can make a simple plot of the stock history using the ", "method:", "The analysis capabilities of Stocker can be used to find the overall trends and patterns within the data, but we will focus on predicting the future price. Predictions in Stocker are ", " which considers a time series as a combination of an overall trend along with seasonalities on different time scales such as daily, weekly, and monthly. Stocker uses the prophet package developed by Facebook for additive modeling. Creating a model and making a prediction can be done with Stocker in a single line:", "Notice that the prediction, the green line, contains a confidence interval. This represents the model\u2019s uncertainty in the forecast. In this case, the confidence interval width is set at 80%, meaning we expect that this range will contain the actual value 80% of the time. The confidence interval grows wide further out in time because the estimate has more uncertainty as it gets further away from the data. Any time we make a prediction we must include a confidence interval. Although most people tend to want a simple answer about the future, our forecast must reflect that we live in an uncertain world!", "Anyone can make stock predictions: simply pick a number and that\u2019s your estimate (I might be wrong, but I\u2019m pretty sure this is all people on Wall Street do). For us to trust our model we need to evaluate it for accuracy.There are a number of methods in Stocker for assessing model accuracy.", "To calculate accuracy, we need a test set and a training set. We need to know the answers \u2014 the actual stock price \u2014 for the test set, so we will use the past one year of historical data (2017 in our case). When training, we do not let our model see the answers to the test set, so we use three years of data previous to the testing time frame (2014\u20132016). The basic idea of supervised learning is the model learns the patterns and relationships in the data from the training set and then is able to correctly reproduce them for the test data.", "We need to quantify our accuracy, so we using the predictions for the test set and the actual values, we calculate metrics including average dollar error on the testing and training set, the percentage of the time we correctly predicted the direction of a price change, and the percentage of the time the actual price fell within the predicted 80% confidence interval. All of these calculations are automatically done by Stocker with a nice visual:", "Those are abysmal stats! We might as well have flipped a coin. If we were using this to invest, we would probably be better off buying something sensible like lottery tickets. However, don\u2019t give up on the model just yet. We usually expect a first model to be rather bad because we are using the default settings (called hyperparameters). If our initial attempts are not successful, we can turn these knobs to make a better model. There are a number of different settings to adjust in a Prophet model, with the most important the changepoint prior scale which controls the amount of weight the model places on shifts in the trend of the data.", "Changepoints represent where a time series goes from increasing to decreasing or from increasing slowly to increasingly rapidly (or vice versa). They occur at the places with the ", ". The changepoint prior scale represents the amount of emphasis given to the changepoints in the model. This is used to control ", " (also known as the bias vs. variance tradeoff).", "A higher prior creates a model with more weight on the changepoints and a more flexible fit. This may lead to overfitting because the model will closely stick to the training data and not be able to generalize to new test data. Lowering the prior decreases the model flexibility which can cause the opposite problem: underfitting. This occurs when our model does not follow the training data closely enough and fails to learn the underlying patterns. Figuring out the proper settings to achieve the right balance is more a matter of engineering than of theory, and here we must rely on empirical results. The Stocker class contains two different ways to choose an appropriate prior: visually and quantitatively. We can start off with the graphical method:", "Here, we are training on three years of data and then showing predictions for six months. We do not quantify the predictions here because we are just trying to understand the role of the changepoint prior. This graph does a great job of illustrating under- vs overfitting! The lowest prior, the blue line, does not follow the training data, the black observations , very closely. It kind of does its own thing and picks a route through the general vicinity of the data. In contrast, the highest prior, the yellow line, sticks to the training observations as closely as possible. The default value for the changepoint prior is 0.05 which falls somewhere in between the two extremes.", "Notice also the difference in uncertainty (shaded intervals) for the priors. The lowest prior has the largest uncertainty on the ", " data, but the smallest uncertainty on the ", " data. In contrast, the highest prior has the smallest uncertainty on the ", "data but the greatest uncertainty on the ", "data. The higher the prior, the more confident it is on the training data because it closely follows each observation. When it comes to the test data however, an overfit model is lost without any data points to anchor it. As stocks have quite a bit of variability, we probably want a more flexible model than the default so the model can capture as many patterns as possible.", "Now that we have an idea of the effect of the prior, we can numerically evaluate different values using a training and validation set:", "Here, we have to be careful that our validation data is not the same as our testing data. If this was the case, we would create the best model for the test data, but then we would just be overfitting the test data and our model could not translate to real world data. In total, as is commonly done in data science, we are using three different sets of data: a training set (2013\u20132015), a validation set (2016), and a testing set (2017).", "We evaluated four priors with four metrics: training error, training range (confidence interval), testing error, and testing range (confidence interval) with all values in dollars. As we saw in the graph, the higher the prior, the lower the training error and the lower the uncertainty on the training data. We also see that a higher prior decreases our testing error, backing up our intuition that closely fitting to the data is a good idea with stocks. In exchange for greater accuracy on the test set, we get a greater range of uncertainty on the test data with the increased prior.", "The Stocker prior validation also displays two plots illustrating these points:", "Since the highest prior produced the lowest testing error, we should try to increase the prior even higher to see if we get better performance. We can refine our search by passing in additional values to the validation method:", "The test set error is minimized at a prior of 0.5. We will set the changepoint prior attribute of the Stocker object appropriately.", "There are other settings of the model we can adjust, such as the patterns we expect to see, or the number of training years of data the model uses. Finding the best combination simply requires repeating the above procedure with a number of different values. Feel free to try out any settings!", "Now that our model is optimized, we can again evaluate it:", "That looks better! This shows the importance of ", ". Using default values provides a reasonable first guess, but we need to be sure we are using the correct model \u201csettings,\u201d just like we try to optimize how a stereo sounds by adjusting balance and fade (sorry for the outdated reference).", "Making predictions is an interesting exercise, but the real fun is looking at how well these forecasts would play out in the actual market. Using the ", " method, we can \u201cplay\u201d the stock market using our model over the evaluation period. We will use a strategy informed by our model which we can then compare to the simple strategy of buying and holding the stock over the entire period.", "The rules of our strategy are straightforward:", "We play this each day for the entire evaluation period which in our case is 2017. To play, add the number of shares to the method call. Stocker will inform us how the strategy played out in numbers and graphs:", "This shows us a valuable lesson: buy and hold! While we would have made a considerable sum playing our strategy, the better bet would simply have been to invest for the long term.", "We can try other test periods to see if there are times when our model strategy beats the buy and hold method. Our strategy is rather conservative because we do not play when we predict a market decrease, so we might expect to do better than a holding strategy when the stock takes a downturn.", "I knew our model could do it! However, our model only beat the market when we were had the benefit of hindsight to choose the test period.", "Now that we are satisfied we have a decent model, we can make future predictions using the ", " method.", "The model is overall bullish on Amazon as are ", " Additionally, the uncertainty increases the further out in time we make estimates as expected. In reality, if we were using this model to actively trade, we would train a new model every day and would make predictions for a maximum of one day in the future.", "While we might not get rich from the Stocker tool, the benefit is in the development rather than the end results! We can\u2019t actually know if we can solve a problem until we try but ", " For anyone interested in checking out the code or using Stocker themselves, it is ", ".", "As always, I enjoy feedback and constructive criticism. I can be reached on Twitter ", ".", "Written by"], "postingTime": "2018-01-21T18:15:38.589Z"}
